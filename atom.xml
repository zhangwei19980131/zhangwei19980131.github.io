<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>夜忆香</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-08-11T09:18:58.357Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>AliceMerser</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Typora使用笔记</title>
    <link href="http://example.com/2021/12/09/typora%E4%B8%8B%E7%BC%96%E5%86%99Markdown%E6%96%87%E4%BB%B6/"/>
    <id>http://example.com/2021/12/09/typora%E4%B8%8B%E7%BC%96%E5%86%99Markdown%E6%96%87%E4%BB%B6/</id>
    <published>2021-12-09T04:38:03.802Z</published>
    <updated>2021-08-11T09:18:58.357Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>#typora入门使用</p><h1 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h1><h1 id="一级标题使用一个"><a href="#一级标题使用一个" class="headerlink" title="一级标题使用一个#"></a>一级标题使用一个#</h1><h2 id="两级标题使用两个"><a href="#两级标题使用两个" class="headerlink" title="两级标题使用两个#"></a>两级标题使用两个#</h2><h3 id="三级标题使用三个"><a href="#三级标题使用三个" class="headerlink" title="三级标题使用三个#"></a>三级标题使用三个#</h3><p>最多6个</p><h1 id="文字"><a href="#文字" class="headerlink" title="文字"></a>文字</h1><p>删除线使用~</p><p><del>文字</del></p><p>斜体与加粗</p><p><em>张维</em>    <strong>张维</strong>    <em><strong>张维</strong></em></p><p>下划线</p><p><u>张维</u>  这个<u>加上 </u></p><p>或者ctrl + u</p><p><u>张维</u></p><p>==张维==</p><p>水 H<del>2</del>O  面积 m^2^</p><p>:smile:  :  :100:  :arrow_lower_right:     蚌埠住了:sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: </p><h1 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h1><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">| name | price|</span><br><span class="line">---|---</span><br><span class="line">coffee | 19</span><br><span class="line">chicken | 25</span><br></pre></td></tr></table></figure><table><thead><tr><th align="center">name</th><th align="center">price</th></tr></thead><tbody><tr><td align="center">coffee</td><td align="center">19</td></tr><tr><td align="center">chicken</td><td align="center">25</td></tr></tbody></table><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><blockquote><p>啦啦啦啦啦啦</p></blockquote><h1 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h1><h2 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h2><ul><li><p>哈哈哈哈</p></li><li><p>啦啦啦啦</p></li></ul><h2 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h2><ol><li>后面要加空格</li><li>是这样的</li></ol><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>[<span class="number">10</span>]:</span><br><span class="line">    i++</span><br></pre></td></tr></table></figure><h1 id="分隔线"><a href="#分隔线" class="headerlink" title="分隔线"></a>分隔线</h1><hr><hr><p>*** 与 —都可以</p><h1 id="跳转"><a href="#跳转" class="headerlink" title="跳转"></a>跳转</h1><h2 id="外部跳转"><a href="#外部跳转" class="headerlink" title="外部跳转"></a>外部跳转</h2><p><a href="http://baidu.com/">百度一下</a></p><p><a href="http://bilibili.com/">bilibili</a></p><h2 id="内部跳转"><a href="#内部跳转" class="headerlink" title="内部跳转"></a>内部跳转</h2><p><a href="#%E6%A0%87%E9%A2%98">跳回标题</a></p><h2 id="自动链接"><a href="#自动链接" class="headerlink" title="自动链接"></a>自动链接</h2><p><a href="http://baidu.com/">http://baidu.com</a></p><h1 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h1><h2 id="网上的图片"><a href="#网上的图片" class="headerlink" title="网上的图片"></a>网上的图片</h2><p>目前hexo没弄出来网上链接图片，这个是本地的</p><h2 id="本地图片"><a href="#本地图片" class="headerlink" title="本地图片"></a>本地图片</h2><p>一起测试<br><a href="https://imgtu.com/i/fubehT"><img src="https://z3.ax1x.com/2021/08/07/fubehT.png" alt="fubehT.png"></a></p><p><a href="https://imgtu.com/i/fubZNV"><img src="https://z3.ax1x.com/2021/08/07/fubZNV.jpg" alt="fubZNV.jpg"></a></p><h1 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h1><p>利用Mermaid画流程图，状态图，甘特图。生成的不是一张图片，而是html代码</p><h2 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">A--&gt;B</span><br><span class="line">B--&gt;C</span><br><span class="line">C--&gt;A</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A--&gt;B</span><br><span class="line">B--&gt;C</span><br><span class="line">C--&gt;A</span><br></pre></td></tr></table></figure><h2 id="常用符号"><a href="#常用符号" class="headerlink" title="常用符号"></a>常用符号</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">A</span><br><span class="line">B[矩形节点]</span><br><span class="line">C(圆角矩形节点)</span><br><span class="line">D((圆形节点))</span><br><span class="line">E&#123;菱形节点&#125;</span><br><span class="line">F&gt;右向旗帜状节点]</span><br></pre></td></tr></table></figure><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">begin(出门)--&gt;buy(买炸鸡)</span><br><span class="line">buy--&gt;IsRemaining&#123;&quot;还有没有炸鸡？&quot;&#125;</span><br><span class="line">IsRemaining--&gt;|Yes|happy[买完炸鸡开心]--&gt;goback(回家)</span><br><span class="line">IsRemaining--&gt;|No|sak[伤心]--&gt;goback(回家)</span><br></pre></td></tr></table></figure><h2 id="连线"><a href="#连线" class="headerlink" title="连线"></a>连线</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">a--&gt;b</span><br><span class="line">c---d</span><br><span class="line">e-.-f</span><br><span class="line">g--test---h</span><br><span class="line">i--test--&gt;l</span><br><span class="line">m-.-&gt;n</span><br></pre></td></tr></table></figure><p>……</p><h2 id="子图表"><a href="#子图表" class="headerlink" title="子图表"></a>子图表</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">subgraph 买炸鸡前</span><br><span class="line">begin(出门)--&gt;buy(买炸鸡)</span><br><span class="line">end</span><br><span class="line">buy--&gt;IsRemaining&#123;&quot;还有没有炸鸡？&quot;&#125;</span><br><span class="line">IsRemaining--&gt;|Yes|happy[买完炸鸡开心]--&gt;goback(回家)</span><br><span class="line">IsRemaining--&gt;|No|sak[伤心]--&gt;goback(回家)</span><br></pre></td></tr></table></figure><h2 id="饼图"><a href="#饼图" class="headerlink" title="饼图"></a>饼图</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pie </span><br><span class="line">title piechart</span><br><span class="line">&quot;Dog&quot; : 256</span><br><span class="line">&quot;Cat&quot; : 412</span><br><span class="line">&quot;Rat&quot; : 888</span><br></pre></td></tr></table></figure><h2 id="甘特图"><a href="#甘特图" class="headerlink" title="甘特图"></a>甘特图</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">gantt</span><br><span class="line">dateFormat  YYYY-MM-DD</span><br><span class="line">title Shop项目交付计划</span><br><span class="line"></span><br><span class="line">section 里程碑 0.1 </span><br><span class="line">数据库设计          :active,    p1, 2021-08-15, 3d</span><br><span class="line">详细设计            :           p2, after p1, 2d</span><br><span class="line"></span><br><span class="line">section 里程碑 0.2</span><br><span class="line">后端开发            :           p3, 2021-08-22, 20d</span><br><span class="line">前端开发            :           p4, 2021-08-22, 15d</span><br><span class="line"></span><br><span class="line">section 里程碑 0.3</span><br><span class="line">功能测试            :       p6, after p3, 5d</span><br><span class="line">上线               :        p7, after p6, 2d</span><br><span class="line">交付               :        p8, afterp7, 2d</span><br></pre></td></tr></table></figure><h1 id="总结markdown"><a href="#总结markdown" class="headerlink" title="总结markdown:"></a>总结markdown:</h1><ol><li><p><strong>标签的形式组织排版</strong></p></li><li><p><strong>代码的形式存储信息</strong></p></li><li><p><strong>跨平台共享内容</strong></p></li><li><p><strong>专注于内容本身</strong></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Markdown" scheme="http://example.com/categories/Markdown/"/>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2021/12/09/hello-world/"/>
    <id>http://example.com/2021/12/09/hello-world/</id>
    <published>2021-12-09T03:19:47.934Z</published>
    <updated>2021-08-05T12:22:35.051Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="Gitalk" scheme="http://example.com/tags/Gitalk/"/>
    
  </entry>
  
  <entry>
    <title>图像分割</title>
    <link href="http://example.com/2021/10/04/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    <id>http://example.com/2021/10/04/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/</id>
    <published>2021-10-04T02:27:37.000Z</published>
    <updated>2021-10-04T02:34:56.845Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h1><p><img src="https://pic.imgdb.cn/item/615a67852ab3f51d91353f51.jpg"></p><p>检测任务（Detection）是检测出图片中的物体位置，一般需要进行画框。比如下图中把人、羊，还有狗都框出来了，具体来说，网络需要输出框的坐标。在上图的检测任务中，矩形框还是比较粗糙的，并不知道每个像素具体属于哪个物体。下图中语义分割任务输出的绿色像素是背景，蓝色像素属于羊，红色像素是狗，还有一个颜色的像素属于人。当然，可以再精细一些，比如不同的羊的像素点用不同的颜色标记出来，那就是实例分割。</p><p>语义分割就是把每个像素都打上标签（这个像素点是人，树，背景等）（语义分割只区分类别，不区分类别中的具体单位）</p><p>实例分割不光要区别类别，还要区分类别中的每一个个体</p><p><img src="https://pic.imgdb.cn/item/615a67ab2ab3f51d91357196.jpg"></p><p>在详细解释一下分割是要在原始图像中逐像素的找到你需要的家伙，有点像抠图，我们把一张图片上的人物或动物从背景中提取出来。</p><p><img src="https://pic.imgdb.cn/item/615a67ce2ab3f51d913599c6.jpg"></p><p>那深度学习下的图像分割过程也非常简单，下图中一个encoder,decoder，一个编码器，一个解码器，编码器通过把原来图像编码成特征图，它可能是一个几千维，几万维的特征，然后通过解码器把这个特征在解码成我们之前预先设定好的一个分割结果，比如说现在的一个分割结果是猪，天，草地，很明显这是一个语义分割。那基于这个架构的话我们有很多个尝试。比如说下面这个FCN(Fully Convolutional Networks)，encoder就用传统的分类网络，把分类器去掉，只做前面卷积也就是特征提取的部分，如VGG，从224一直卷到14，也就是我们的特征图了，然后再用反卷积的操作，比如上采样等等，相对称的decoder出骑自行车的人的效果图，我们可以发现整个网路是没有全连接层，这是一个全卷积网络。但是这个存在一个问题，我们原来这么大一张图卷积到这么小，然后我们在放成这么大，会存在图片失真也就是图像质量下降的现象，所以后来又出了Unet，Unet呢，卷积的过程是有下采样的，每次下采样的过程中保留一些特征，在下采样的过程中不断保留这些特征，等到上采样的时候decoder的时候，我在把它还回去，把缺少的东西再给补回来，所以说这个在各种情况下的应用效果都还不错。</p><p><img src="https://pic.imgdb.cn/item/615a67f82ab3f51d9135cbd5.jpg"></p><p>假设我们输入是一个224×224×3的一个图像，上面有一个人和背景，那我们输出的图片大小应该也是224×224，不应该是变大变小，而维度，图像分割是做一个逐像素点的分割，是一个二分类的问题，比如说这一个点是人还是不是人，所以输出的维度应该是2，所有输出是224×224×2。</p><p>然后就是进行卷积+池化进行一个特征提取，我们的图片越来越小，这是一个下采样的操作，然后进行上采样，上采样呢，一般来说我们叫做插值，例如现在有一个像素值是5还有一个像素值是20，我们往其中插入两个值比如10和15。</p><p>整体来说就是一个编码解码的过程，计算机把我们能看懂的图片数据转换成它能识别的特征或者说是矩阵，这是编码，解码就是计算机用我们这个特征或者说矩阵帮我们输出出最终我们想要的一个结果。</p><p>最早在医学领域做细胞分割，为什么说在医学领域做的比较好或者说做的比较多？医学当中很多待分割的目标，比如说是一个小细胞，都是比较小的东西。而网络结构有什么特点呢？越深层的网络，看到东西越多，更适合做一些大目标的分割，网络结构浅的话适合做一些小目标的分割，U-net比较浅层的原因，或者说设计最初的思想就是想在医学当中解决一些小目标的问题。</p><p>U-net里面前面这些层视为浅层，后面为深层，在这里箭头代表的是一个拼接操作，既融进来浅层特征，也融进来深层特征，拼接要注意尺寸大小要对应上。</p><p><img src="https://pic.imgdb.cn/item/615a681f2ab3f51d9135f9a1.jpg"></p><p>这是一个简图，下采样上采样拼接，但是有一个小问题，这两个拼接还好，第一层和最后一层，一个只是浅层的一个是非常深的，他俩拼接就显得并不是很合适，有一些跨度，跨度有些地方有点太大了。怎么解决这个问题呢？</p><p><img src="https://pic.imgdb.cn/item/615a683e2ab3f51d91361f06.jpg"></p><p>提出了U-Net++，我们标号1,2,3,4,1不能和2拼接，因为大小不一样，所以1和2的上采样拼接，2和3的上采样拼接..我们看这个X0,2。这是由3个进行拼接的，X0,0 X0,1 X1,1，这回呢我们拼接的方法更多了，距离也更近一些，论文当中也通过实验证明这样拼接呢效果会更好一些。这是第一个改进的地方，第二是关于损失函数的改进，论文当中在这里，这里，这里都连上了loss，为什么这么做呢，举个例子是我们网络费了好大劲最后输出一个结果，就好比我们读书上学，上小学、初中、高中、大学，最终的损失函数就是考大学，高考，我们考的怎么样，这是一个损失函数，它是通过最终的结果来评判我们怎么去优化，但是呢我们想，要是最终想要高考考的好，前提呢需要是考上好的小学，好的初中，好的高中，最后才能上好的大学，就是我们的损失函数不仅仅是在最后一步，在中间的部分也加上损失函数，相当于让网络关注的点更多了，不仅仅要最终输出的好，每一步都要输出的好。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="图像分割" scheme="http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>域名备案</title>
    <link href="http://example.com/2021/09/26/%E5%9F%9F%E5%90%8D%E5%A4%87%E6%A1%88/"/>
    <id>http://example.com/2021/09/26/%E5%9F%9F%E5%90%8D%E5%A4%87%E6%A1%88/</id>
    <published>2021-09-26T02:21:47.000Z</published>
    <updated>2021-09-26T02:37:39.499Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="关于hexo博客想要使用cdn加速又要备案的问题"><a href="#关于hexo博客想要使用cdn加速又要备案的问题" class="headerlink" title="关于hexo博客想要使用cdn加速又要备案的问题"></a>关于hexo博客想要使用cdn加速又要备案的问题</h1><h2 id="马上放国庆假了-smile-smile-smile"><a href="#马上放国庆假了-smile-smile-smile" class="headerlink" title="马上放国庆假了 :smile: :smile: :smile:"></a>马上放国庆假了 :smile: :smile: :smile:</h2><h2 id="域名问题与实名认证的问题"><a href="#域名问题与实名认证的问题" class="headerlink" title="域名问题与实名认证的问题"></a>域名问题与实名认证的问题</h2><p>由于我的域名购买是在godday上购买，而godday国外域名服务商购买时似乎省略了实名认证的步骤，也没有提供域名备案方法，而hexo博客的服务器github page 是在国外，一般情况不需要备案，但是要是想要使用cdn加速，还是要备案。。。。。。</p><h2 id="这是一个很麻烦的问题"><a href="#这是一个很麻烦的问题" class="headerlink" title="这是一个很麻烦的问题"></a>这是一个很麻烦的问题</h2><h2 id="鸽了-等域名过期后，在阿里云或腾讯云购买域名再说（2021-9-26）"><a href="#鸽了-等域名过期后，在阿里云或腾讯云购买域名再说（2021-9-26）" class="headerlink" title="鸽了~~ 等域名过期后，在阿里云或腾讯云购买域名再说（2021.9.26）"></a>鸽了~~ 等域名过期后，在阿里云或腾讯云购买域名再说（2021.9.26）</h2><h2 id="可申请备案域名查询"><a href="#可申请备案域名查询" class="headerlink" title="可申请备案域名查询"></a>可申请备案域名查询</h2><p><a href="https://seo.juziseo.com/doc/tld/cnicp">https://seo.juziseo.com/doc/tld/cnicp</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="备案" scheme="http://example.com/tags/%E5%A4%87%E6%A1%88/"/>
    
  </entry>
  
  <entry>
    <title>深度相机</title>
    <link href="http://example.com/2021/09/14/%E6%B7%B1%E5%BA%A6%E7%9B%B8%E6%9C%BA/"/>
    <id>http://example.com/2021/09/14/%E6%B7%B1%E5%BA%A6%E7%9B%B8%E6%9C%BA/</id>
    <published>2021-09-14T07:50:28.000Z</published>
    <updated>2021-09-14T08:36:31.926Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="深度相机与RGB-D相机"><a href="#深度相机与RGB-D相机" class="headerlink" title="深度相机与RGB-D相机"></a>深度相机与RGB-D相机</h1><h2 id="什么是深度图"><a href="#什么是深度图" class="headerlink" title="什么是深度图"></a>什么是深度图</h2><p><img src="https://pic.imgdb.cn/item/614054b944eaada73903726b.jpg"></p><p>左侧是一个普通的RGB图像，右侧是一个Depth图像。如何获得呢？第一个是深度相机采集，第二个是立体匹配计算，就是我们用双目相机来计算出一张深度图，第三个是数据集，比如说我们右边这两幅图，它的来源就是德国TUM大学的一个比较知名的一个开源数据集</p><p><img src="https://pic.imgdb.cn/item/614054db44eaada73903ba27.jpg"></p><p><a href="https://imgtu.com/i/4kTq8s"><img src="https://z3.ax1x.com/2021/09/14/4kTq8s.gif" alt="4kTq8s.gif"></a></p><p>这个是使用工具ImageJ打开的一张深度图，我们可以看到这个光标在动，x,y的坐标也在发生变化，value值也在变化，值代表的是距离，是当前的被测物体到镜头之间的距离。比方说我们的这个value值是6000，除以尺度因子就是它的一个物理距离，不同的相机它的尺度因子是不一样的，TUM这个采集数据集的相机的尺度因子是5000</p><h2 id="什么是深度相机与RGB-D相机"><a href="#什么是深度相机与RGB-D相机" class="headerlink" title="什么是深度相机与RGB-D相机"></a>什么是深度相机与RGB-D相机</h2><p><img src="https://pic.imgdb.cn/item/6140562e44eaada739062b50.jpg"></p><p>深度相机是一种比较广泛的说法，能测出深度都可以叫深度相机，不管是通过软件算法间接计算还是物理方法直接测量，我们通常说的RGB-D相机是通过物理方法测试的比如说结构光、TOF(飞行时间) 我们看一下右边有这么多种深度相机，有单目结构光和双目结构光相机，这个是只有深度值的相机，下面这个普通双目是通过立体匹配来计算深度，还有tof等等，单目和双目有什么区别呢，单目就是说我们有一个红外发射器，然后有一个红外接收器，而双目是有一个红外发射器，两个红外接收器，就是得到两个红外图。</p><h2 id="RGB-D相机分类"><a href="#RGB-D相机分类" class="headerlink" title="RGB-D相机分类"></a>RGB-D相机分类</h2><p><img src="https://pic.imgdb.cn/item/6140564c44eaada73906680c.jpg"></p><p>Kinect 是最早做RGB-D相机的，是微软他们推出的设备，比如说Kinect v1(结构光法)后来iphone X是最早在手机上推出深度相机的，tof的呢，有Kinect v2，还有Phab 2 pro，这是2016年由谷歌和联想共同推出的一款手机，主要是在国外市场，国内大家可能不太清楚。</p><h2 id="RGB-D相机原理（结构光）"><a href="#RGB-D相机原理（结构光）" class="headerlink" title="RGB-D相机原理（结构光）"></a>RGB-D相机原理（结构光）</h2><p><img src="https://pic.imgdb.cn/item/6140566b44eaada73906a5c5.jpg"></p><p>关于RGB-D相机原理是比较复杂的，这里我就简单介绍带过一下，我们通常拿到设备就是直接用，不需要深入了解里面的硬件是怎么做的。</p><p>结构光呢，是为了解决双目匹配问题产生的，我们看到右边这两张图，这是RGB的一个双目匹配图，第一个是原图所对应的灰度图，下面这个是通过RGB的一个立体匹配得到的一个深度图，右上角是通过红外投射的一个红外散斑图，右下角是通过结构光得到的一个深度图，我们可以对比一下，同一场景下，他们两得到的深度图差别非常大，结构光法得到的明显比这个好很多。</p><p>我们知道双目匹配的时候，需要提特征点，需要做很多图像上的操作，如果光照不好我们照的图片里面就很难去提取特征点，但是结构光这个相机投的是红外光，所以是没有影响。</p><p>解决依赖图片纹理问题，因为它是往物体上投射图案。提高鲁棒性等等，特点的话，夜里可用，比如说我们的iphonX，晚上关了灯以后人脸解锁也是可以用的。</p><p><img src="https://pic.imgdb.cn/item/6140569744eaada73907012b.jpg"></p><p>结构光方案鼻祖是这家以色列公司，应用于微软的明星产品被广为人知，后来被apple收购了，iPhone X人脸识别这一块做的非常好，非常稳定，其实它硬件是非常好的。右边是它的一个示意图，它投出来的叫伪随机散斑，投到物体上，我们可以看到每一块都是不一样的，通过这种伪随机性，得到一个深度图。</p><p><img src="https://pic.imgdb.cn/item/614056b144eaada7390733ee.jpg"></p><p><img src="https://pic.imgdb.cn/item/614056bd44eaada739074a66.jpg"></p><p><img src="https://pic.imgdb.cn/item/614057f944eaada739099d0a.jpg"></p><p><img src="https://pic.imgdb.cn/item/6140581a44eaada73909d594.jpg"></p><h2 id="RGB-D相机原理（TOF）"><a href="#RGB-D相机原理（TOF）" class="headerlink" title="RGB-D相机原理（TOF）"></a>RGB-D相机原理（TOF）</h2><p><img src="https://pic.imgdb.cn/item/6140582a44eaada73909f1e6.jpg"></p><p>飞行时间法这个我们可以看右边这幅图，通过发射光脉冲到物体上，然后这边有一个接收器，这个接收器通过反射回来的光脉冲，计算时间差来测这个深度和距离，所以叫飞行时间，原理是非常简单的，但是硬件上实现是有很大难度的。目前手机上没有一个特别好的TOF手机。因为功耗大，我们不可能拿着移动设备，还要插着220V电压，来到处跑着测。</p><p>透明物体的影响：比如说s这块玻璃，那么我们红外光发射穿过这块玻璃，跑到外面了，外面空间是无限远的，那我们这个红外光就是肉包子打狗，有去无回了，也就是右边这幅图黑色的情况。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="深度相机" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E7%9B%B8%E6%9C%BA/"/>
    
    <category term="RGB-D" scheme="http://example.com/tags/RGB-D/"/>
    
  </entry>
  
  <entry>
    <title>OpenMVG+PMVS三维重建入门使用</title>
    <link href="http://example.com/2021/09/03/OpenMVG-PMVS%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8/"/>
    <id>http://example.com/2021/09/03/OpenMVG-PMVS%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8/</id>
    <published>2021-09-03T00:48:29.000Z</published>
    <updated>2021-09-03T01:57:55.955Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="OpenMVG-PMVS三维重建入门使用"><a href="#OpenMVG-PMVS三维重建入门使用" class="headerlink" title="OpenMVG+PMVS三维重建入门使用"></a>OpenMVG+PMVS三维重建入门使用</h1><h2 id="两大游戏引擎"><a href="#两大游戏引擎" class="headerlink" title="两大游戏引擎"></a>两大游戏引擎</h2><p><img src="https://pic.imgdb.cn/item/6131717844eaada73964480f.jpg"></p><p>这个表格是关于两大引擎的对比关于他们的开发公司、总部地点、公司市值、开发语言和公司的属性。</p><p>然后关于建模软件如Zbrush、3Dmax等等软件建模出的人物或动物模型如右图这种，在计算机显示的3d图形，它是Mesh的一种表现形式。</p><p>第一个point cloud ，深度传感器扫描得到的深度数据，点云</p><p>第三个Volumetric，将空间划分成三维网格，栅格化。</p><p>第四个Multi-View，用多个角度的图片表示物体</p><p>而第二个Mesh，三角面片在计算机图形学中渲染和建模非常有用。</p><h2 id="使用OpenMVG和MVS三维重建"><a href="#使用OpenMVG和MVS三维重建" class="headerlink" title="使用OpenMVG和MVS三维重建"></a>使用OpenMVG和MVS三维重建</h2><p><img src="https://pic.imgdb.cn/item/613171b544eaada7396494cb.jpg"></p><p>三维重建的流程之前也介绍过，所以这里简单提一下，我们需要从多视角的图像中提取特征，然后进行SFM和MVS。</p><p><img src="https://pic.imgdb.cn/item/613171d744eaada73964be0b.jpg"></p><p>我这里使用的是两个开源库，一个是openMVG、一个是PMVS。PMVS呢，是把sfm的一个输出作为输入，在进行一个密集重建。运行环境为ubuntu，windows安装过程较为繁琐，而且容易出现的问题也很多，可以在Windows下安装虚拟机装好ubuntu系统。</p><p>openMVG和PMVS的安装可参考  <a href="https://blog.yanjingang.com/?p=3329">https://blog.yanjingang.com/?p=3329</a></p><p>出现的问题：</p><p>由于网络问题可能会导致连接不上github，或者导致只安装一部分文件，如：openMVG/src/dependencies里面的cereal、glfw、osi_clp文件为空。</p><p>解决方法：</p><ol><li>网好的时候重新clone</li><li>直接去网站上把整个文件code下来，或者只code下dependencies</li></ol><p>后面安装过程，cmake版本需要注意一下（cmake -version）。中途有个长时间的百分之一到百分之百的等待，出现的提示错误不是进行不下去的可以不用管，到百分百结束。</p><p>解锁方法：（无法获得锁问题/被占用….）</p><p>sudo rm /var/lib/apt/lists/lock</p><p>权限问题一半情况下加上sudo。</p><p><img src="https://pic.imgdb.cn/item/613174da44eaada73968fbae.jpg"></p><p>这是官方给出的测试集</p><p><a href="https://s31.aconvert.com/convert/p3r68-cdx67/qkg98-il4p9.gif">https://s31.aconvert.com/convert/p3r68-cdx67/qkg98-il4p9.gif</a></p><p><a href="https://imgtu.com/i/hyplUU"><img src="https://z3.ax1x.com/2021/09/03/hyplUU.gif" alt="hyplUU.gif"></a></p><p>进行SFM稀疏点云重建的结果</p><p><a href="https://imgtu.com/i/hy9VIO"><img src="https://z3.ax1x.com/2021/09/03/hy9VIO.gif" alt="hy9VIO.gif"></a></p><p>进行MVS重建的结果</p><p><img src="https://pic.imgdb.cn/item/61317b2844eaada73972b3fd.jpg"></p><p><a href="https://imgtu.com/i/hy9Wl9"><img src="https://z3.ax1x.com/2021/09/03/hy9Wl9.gif" alt="hy9Wl9.gif"></a></p><p><img src="https://pic.imgdb.cn/item/61317c1d44eaada739744649.jpg"></p><p><img src="https://pic.imgdb.cn/item/61317c3444eaada7397469d1.jpg"></p><p>注意的问题：</p><p>一个是格式问题，使用微信、qq从手机传到电脑会丢失图像信息，我的解决办法是在Ubuntu系统下安装百度网盘，通过手机把图片传输到网盘，然后再在ubuntu系统上下载下来，还有一个问题是iPhone12拍摄的图片默认格式是heic格式，这个苹果自iOS11以来，默认的图片格式，它体积更小、清晰度更高。如果我们在Windows系统下修改后缀名，如jpg，png是可以正常查看的，但是在ubuntu系统是读取不到图片的，必须得是拍摄的时候在手机设置里修改格式为jpg。</p><p>第二个问题是相机参数问题，Sensor_width(传感器宽度)。在openMVG库的源文件里的senor_width这里面提供了很多不同种类的相机参数，但是iphone系列只添加到iphone型号XS这里，所以我们需要在下面添加上iphone12的参数值。</p><p>具体过程：</p><ol><li>采集好图片后打包放入（car/images）里面</li><li>在openMVG里面创建3dr_test.py文件</li><li>修改文件所在的路径</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># openmvg编译bin目录(可cp -p到/usr/local/bin/)</span></span><br><span class="line">OPENMVG_SFM_BIN = <span class="string">&quot;/home/work/tools/openMVG_Build/Linux-x86_64-RELEASE&quot;</span> </span><br><span class="line"><span class="comment"># 路径注意对好，虚拟机下这个文件通常home后面跟的是用户名，可以找到openMVG文件里的Linux- ... 复制路径过来</span></span><br><span class="line"><span class="comment"># pmvs编译bin目录(可cp -p到/usr/local/bin/)</span></span><br><span class="line">PMVS_BIN = <span class="string">&quot;/home/work/tools/CMVS-PMVS/build/main&quot;</span>  <span class="comment"># 路径问题同上</span></span><br><span class="line"><span class="comment"># openmvg相机参数目录</span></span><br><span class="line">CAMERA_SENSOR_WIDTH_DIRECTORY = <span class="string">&quot;/home/work/tools/openMVG/src/openMVG/exif/sensor_width_database&quot;</span></span><br><span class="line"><span class="comment"># 自己的相机参数如果文件里没有需要添加上去</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0. 下载测试照片</span></span><br><span class="line">os.chdir(os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line">data_dir = os.path.abspath(<span class="string">&quot;./car&quot;</span>) <span class="comment"># 这里修改路径,例如我的是car,修改为你自己数据集的文件夹名</span></span><br><span class="line"><span class="comment">#data_dir = os.path.abspath(&quot;./ImageDataset_SceauxCastle&quot;)</span></span><br><span class="line"><span class="comment"># 下面这一段是官方的测试代码，在github上下载的，测试官方用例网络不好可以直接去网站上面挂vpn下载放入对应路径</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;if not os.path.exists(data_dir):</span></span><br><span class="line"><span class="string">  pImageDataCheckout = subprocess.Popen([ &quot;git&quot;, &quot;clone&quot;, &quot;https://github.com/openMVG/ImageDataset_SceauxCastle.git&quot; ])</span></span><br><span class="line"><span class="string">  pImageDataCheckout.wait()&#x27;&#x27;&#x27;</span></span><br><span class="line">input_dir = os.path.join(data_dir, <span class="string">&quot;images&quot;</span>)</span><br><span class="line">output_dir = data_dir</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Using input dir  : &quot;</span>, input_dir)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;      output_dir : &quot;</span>, output_dir)</span><br><span class="line">matches_dir = os.path.join(output_dir, <span class="string">&quot;matches&quot;</span>)</span><br><span class="line">camera_file_params = os.path.join(CAMERA_SENSOR_WIDTH_DIRECTORY, <span class="string">&quot;sensor_width_camera_database.txt&quot;</span>)    <span class="comment">#相机参数</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(matches_dir):</span><br><span class="line">  os.mkdir(matches_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 从图片数据集中生成场景描述文件sfm_data.json</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;----------1. Intrinsics analysis----------&quot;</span>)</span><br><span class="line">pIntrisics = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, <span class="string">&quot;openMVG_main_SfMInit_ImageListing&quot;</span>),  <span class="string">&quot;-i&quot;</span>, input_dir, <span class="string">&quot;-o&quot;</span>, matches_dir, <span class="string">&quot;-d&quot;</span>, camera_file_params, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;3&quot;</span>] )</span><br><span class="line"><span class="comment">#*注：如果产出的sfm_data.json里intrinsics内容为空，通常是在图片没有exif信息导致获取不到相机焦距、ccd尺寸等参数，用带exif的原图即可。</span></span><br><span class="line">pIntrisics.wait()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 计算图像特征</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;----------2. Compute features----------&quot;</span>)</span><br><span class="line">pFeatures = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, <span class="string">&quot;openMVG_main_ComputeFeatures&quot;</span>),  <span class="string">&quot;-i&quot;</span>, matches_dir+<span class="string">&quot;/sfm_data.json&quot;</span>, <span class="string">&quot;-o&quot;</span>, matches_dir, <span class="string">&quot;-m&quot;</span>, <span class="string">&quot;SIFT&quot;</span>, <span class="string">&quot;-f&quot;</span> , <span class="string">&quot;1&quot;</span>] )</span><br><span class="line">pFeatures.wait()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 计算几何匹配</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;----------3. Compute matches----------&quot;</span>)</span><br><span class="line">pMatches = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, <span class="string">&quot;openMVG_main_ComputeMatches&quot;</span>),  <span class="string">&quot;-i&quot;</span>, matches_dir+<span class="string">&quot;/sfm_data.json&quot;</span>, <span class="string">&quot;-o&quot;</span>, matches_dir, <span class="string">&quot;-f&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;-n&quot;</span>, <span class="string">&quot;ANNL2&quot;</span>] )</span><br><span class="line">pMatches.wait()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 执行增量三维重建</span></span><br><span class="line">reconstruction_dir = os.path.join(output_dir,<span class="string">&quot;reconstruction_sequential&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;----------4. Do Incremental/Sequential reconstruction----------&quot;</span>) <span class="comment">#set manually the initial pair to avoid the prompt question</span></span><br><span class="line">pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, <span class="string">&quot;openMVG_main_IncrementalSfM&quot;</span>),  <span class="string">&quot;-i&quot;</span>, matches_dir+<span class="string">&quot;/sfm_data.json&quot;</span>, <span class="string">&quot;-m&quot;</span>, matches_dir, <span class="string">&quot;-o&quot;</span>, reconstruction_dir] )</span><br><span class="line">pRecons.wait()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 计算场景结构颜色</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;----------5. Colorize Structure----------&quot;</span>)</span><br><span class="line">pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, <span class="string">&quot;openMVG_main_ComputeSfM_DataColor&quot;</span>),  <span class="string">&quot;-i&quot;</span>, reconstruction_dir+<span class="string">&quot;/sfm_data.bin&quot;</span>, <span class="string">&quot;-o&quot;</span>, os.path.join(reconstruction_dir,<span class="string">&quot;colorized.ply&quot;</span>)] )</span><br><span class="line">pRecons.wait()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 测量稳健三角</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;----------6. Structure from Known Poses (robust triangulation)----------&quot;</span>)</span><br><span class="line">pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, <span class="string">&quot;openMVG_main_ComputeStructureFromKnownPoses&quot;</span>),  <span class="string">&quot;-i&quot;</span>, reconstruction_dir+<span class="string">&quot;/sfm_data.bin&quot;</span>, <span class="string">&quot;-m&quot;</span>, matches_dir, <span class="string">&quot;-o&quot;</span>, os.path.join(reconstruction_dir,<span class="string">&quot;robust.ply&quot;</span>)] )</span><br><span class="line">pRecons.wait()</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"># 使用全局SfM管道重建Reconstruction for the global SfM pipeline</span></span><br><span class="line"><span class="string"># 3.1 全局sfm管道几何匹配</span></span><br><span class="line"><span class="string">print (&quot;----------3.1. Compute matches (for the global SfM Pipeline)----------&quot;)</span></span><br><span class="line"><span class="string">pMatches = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_ComputeMatches&quot;),  &quot;-i&quot;, matches_dir+&quot;/sfm_data.json&quot;, &quot;-o&quot;, matches_dir, &quot;-r&quot;, &quot;0.8&quot;, &quot;-g&quot;, &quot;e&quot;] )</span></span><br><span class="line"><span class="string">pMatches.wait()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 4.1 执行全局三维重建</span></span><br><span class="line"><span class="string">reconstruction_dir = os.path.join(output_dir,&quot;reconstruction_global&quot;)</span></span><br><span class="line"><span class="string">print (&quot;----------4.1. Do Global reconstruction----------&quot;)</span></span><br><span class="line"><span class="string">pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_GlobalSfM&quot;),  &quot;-i&quot;, matches_dir+&quot;/sfm_data.json&quot;, &quot;-m&quot;, matches_dir, &quot;-o&quot;, reconstruction_dir] )</span></span><br><span class="line"><span class="string">pRecons.wait()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 5.1 计算场景结构颜色</span></span><br><span class="line"><span class="string">print (&quot;----------5.1. Colorize Structure----------&quot;)</span></span><br><span class="line"><span class="string">pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_ComputeSfM_DataColor&quot;),  &quot;-i&quot;, reconstruction_dir+&quot;/sfm_data.bin&quot;, &quot;-o&quot;, os.path.join(reconstruction_dir,&quot;colorized.ply&quot;)] )</span></span><br><span class="line"><span class="string">pRecons.wait()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 6.1 测量稳健三角</span></span><br><span class="line"><span class="string">print (&quot;----------6.1. Structure from Known Poses (robust triangulation)----------&quot;)</span></span><br><span class="line"><span class="string">pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_ComputeStructureFromKnownPoses&quot;),  &quot;-i&quot;, reconstruction_dir+&quot;/sfm_data.bin&quot;, &quot;-m&quot;, matches_dir, &quot;-o&quot;, os.path.join(reconstruction_dir,&quot;robust.ply&quot;)] )</span></span><br><span class="line"><span class="string">pRecons.wait()</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 把openMVG生成的SfM_Data转为适用于PMVS输入格式的文件</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;----------7. Export to PMVS/CMVS----------&quot;</span>)</span><br><span class="line">pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, <span class="string">&quot;openMVG_main_openMVG2PMVS&quot;</span>),  <span class="string">&quot;-i&quot;</span>, reconstruction_dir+<span class="string">&quot;/sfm_data.bin&quot;</span>, <span class="string">&quot;-o&quot;</span>, reconstruction_dir] )</span><br><span class="line">pRecons.wait()</span><br><span class="line"><span class="comment">#*注：执行后会在-o路径下生成一个PMVS目录，包含 models, txt, visualize 三个子目录：models为空；txt包含对应图像的txt文档，每个里面都是一个3x4的矩阵，大概是相机位姿；visualize包含11张图像，不确定是原图像还是校正过的图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 使用PMVS重建稠密点云、表面、纹理</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;----------8. pmvs2----------&quot;</span>)</span><br><span class="line">pRecons = subprocess.Popen( [os.path.join(PMVS_BIN, <span class="string">&quot;pmvs2&quot;</span>),  reconstruction_dir+<span class="string">&quot;/PMVS/&quot;</span>, <span class="string">&quot;pmvs_options.txt&quot;</span>] )  <span class="comment"># 注：不要修改pmvs_options.txt文件名</span></span><br><span class="line">pRecons.wait()</span><br><span class="line"><span class="comment">#*注：执行后会在./PMVS/models文件夹中生成一个pmvs_options.txt.ply点云文件，用meshlab打开即可看到重建出来的彩色稠密点云。</span></span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/hyCA6s"><img src="https://z3.ax1x.com/2021/09/03/hyCA6s.gif" alt="hyCA6s.gif"></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="三维重建" scheme="http://example.com/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>ResNet</title>
    <link href="http://example.com/2021/08/27/ResNet/"/>
    <id>http://example.com/2021/08/27/ResNet/</id>
    <published>2021-08-27T08:57:35.000Z</published>
    <updated>2021-09-06T10:02:43.305Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p>这是一篇不完善不完整的总结文档（不如看论文），内容不是很全，以后有机会在补充（2021.9.6）</p><p>感谢B站UP主@<strong>霹雳吧啦Wz</strong> </p><p>  :cry: 没人带，摸索着学习真难</p><h2 id="简要介绍"><a href="#简要介绍" class="headerlink" title="简要介绍"></a>简要介绍</h2><p>ResNet在2015年由微软实验室提出，斩获当年lmageNet竞赛中分类任务第一名，目标检测第一名。获得coco数据集中目标检测第一名，图像分割第一名。(啥也别说了，就是NB)</p><h3 id="网络亮点"><a href="#网络亮点" class="headerlink" title="网络亮点"></a>网络亮点</h3><ol><li>超深的网络结构（突破1000层）</li><li>提出residual模块</li><li>使用Batch Normalization加速训练（丢弃dropout）</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://pic.imgdb.cn/item/61356ac044eaada739dff8f1.png"></p><h3 id="Resnet-34"><a href="#Resnet-34" class="headerlink" title="Resnet 34"></a>Resnet 34</h3><p><img src="https://pic.imgdb.cn/item/61356c1c44eaada739e1c4f7.png"></p><h3 id="残差结构"><a href="#残差结构" class="headerlink" title="残差结构"></a>残差结构</h3><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>Batch Normalization是google团队在2015年论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》提出的。通过该方法能够加速网络的收敛并提升准确率。</p><p>Batch Normalization的目的是使我们的一批（Batch） feature map满足均值为0，方差为1的分布规律</p><p><img src="https://pic.imgdb.cn/item/613580f644eaada7390159a8.png"></p><p>μ, σ²在正向传播过程中统计得到 </p><p>γ, β在反向传播过程中训练得到</p><h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>使用迁移学习的优势：</p><ol><li>能够快速的训练出一个理想的结果 </li><li>当数据集较小时也能训练出理想的效果</li></ol><p><img src="https://pic.imgdb.cn/item/6135837244eaada73905518e.png"></p><p><img src="https://pic.imgdb.cn/item/613583d344eaada73905e770.png"></p><p>常见的迁移学习方式： </p><ol><li>载入权重后训练所有参数 </li><li>载入权重后只训练最后几层参数 </li><li>载入权重后在原网络基础上再添加一层全连接层，仅训练最后一个全连接层</li></ol><p><img src="https://pic.imgdb.cn/item/613588d244eaada7390d963f.jpg"></p><h2 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>GoogLeNet</title>
    <link href="http://example.com/2021/08/27/GoogLeNet/"/>
    <id>http://example.com/2021/08/27/GoogLeNet/</id>
    <published>2021-08-27T01:45:40.000Z</published>
    <updated>2021-08-27T08:52:58.855Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h1><h2 id="简要介绍"><a href="#简要介绍" class="headerlink" title="简要介绍"></a>简要介绍</h2><p>GoogLeNet在2014年由Google团队提出，斩获当年IlmageNet竞赛中Classification Task(分类任务)第一名。</p><p>网络中的亮点:</p><ol><li>引入了Inception结构（融合不同尺度的特征信息)</li><li>使用1x1的卷积核进行降维以及映射处理</li><li>添加两个辅助分类器帮助训练</li><li>丢弃全连接层，使用平均池化层（大大减少模型参数)</li></ol><p>GoogLeNet中L大写是为了致敬LeNet</p><p>AlexNet和VGG都只有一个输出层，GoogLeNet有三个输出层（其中两个辅助分类层）</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://pic.imgdb.cn/item/6128487d44eaada73955016c.png"></p><h3 id="Inception结构"><a href="#Inception结构" class="headerlink" title="Inception结构"></a>Inception结构</h3><p>初始版本</p><p><img src="https://pic.imgdb.cn/item/612849b344eaada7395740a2.png"></p><p>之前的网络如AlexNet和VGGNet都是串行结构，一系列的卷积层和最大下采样层串联得到我们的一个网络结构，但是我们的Inception结构出现了一个并行结构。</p><p>将我们所得到的一个特征矩阵同时输入到这4个分支当中，同时进行处理，处理之后将我们同时所得到的这4个分支的特征矩阵，按深度进行拼接，得到我们的一个输出矩阵。</p><p><strong>注意</strong>：每个分支所得的特征矩阵高和宽必须相同，否则我们无法沿深度方向进行一个拼接</p><p>后来版本（加上了一个降维的功能）</p><p><img src="https://pic.imgdb.cn/item/612849bf44eaada739575837.png"></p><p>比上一张图多了3个1x1的卷积层，都是起到一个降维的作用。原理：</p><p><img src="https://pic.imgdb.cn/item/61284c7944eaada7395c6aab.png"></p><h3 id="辅助分类器"><a href="#辅助分类器" class="headerlink" title="辅助分类器"></a>辅助分类器</h3><p><img src="https://pic.imgdb.cn/item/61284d5e44eaada7395e5b51.png"></p><p>GoogLeNet中有两个辅助分类器，他们的结构是一模一样的</p><p>第一层是平均池化下采样的操作</p><p>…</p><h3 id="参数对比"><a href="#参数对比" class="headerlink" title="参数对比"></a>参数对比</h3><p>GoogLeNet模型参数： 6994392</p><p>VGGNet模型参数：138357544</p><p>相差约20倍</p><h2 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h2><p>一般情况下用的人不是很多，因为有辅助分类器等等，搭建起来较为复杂，更多的人使用的是VGG，如果只是用来做分类任务可以使用，准确率还是很高的。</p><p><img src="https://pic.imgdb.cn/item/6128a7c444eaada73929192e.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>游戏制作与引擎</title>
    <link href="http://example.com/2021/08/27/%E6%B8%B8%E6%88%8F%E5%88%B6%E4%BD%9C%E4%B8%8E%E5%BC%95%E6%93%8E/"/>
    <id>http://example.com/2021/08/27/%E6%B8%B8%E6%88%8F%E5%88%B6%E4%BD%9C%E4%B8%8E%E5%BC%95%E6%93%8E/</id>
    <published>2021-08-27T00:49:29.000Z</published>
    <updated>2021-08-27T01:17:48.442Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="游戏制作与引擎相关介绍"><a href="#游戏制作与引擎相关介绍" class="headerlink" title="游戏制作与引擎相关介绍"></a>游戏制作与引擎相关介绍</h1><h2 id="游戏制作与建模"><a href="#游戏制作与建模" class="headerlink" title="游戏制作与建模"></a>游戏制作与建模</h2><p><img src="https://pic.imgdb.cn/item/6128376244eaada739389e0b.jpg"></p><p> 通常游戏的制作分为3个部分：设计、程序和美工。</p><p>设计主要是思考游戏的核心玩法是怎么样的，游戏的故事线背景剧情是什么样的。</p><p>美工只是负责游戏的草图原画，并绘制在计算机中，以及对角色物体场景的三维建模。</p><p>程序则是将游戏的玩法逻辑用代码给实现出来，可以说这3个部分在游戏的制作中是缺一不可的，如果你是一个个人开发者，那么你将可能同时担任所有的角色，而对于新手入门编程来说，有一个好的点子，在网上选一些免费无版权的资源，或者购买一些三维模型和动画，懂一些简单的编程，对小游戏的开发是完全没有问题的，但如果是一家游戏公司任职的话，情况的要复杂的多，制作人，QA（质量保证测试），运营部门， 这里不介绍了。</p><p><img src="https://pic.imgdb.cn/item/6128376244eaada739389e11.jpg"></p><p>那我们讲一下游戏里的物体制作通常分为两步。</p><p>建模就是把2D平面的东西3D立体化，把一个东西立体化之后，你就可以三百六十度旋转观察。这样做出来的东西，我们称之为模型。像王者荣耀、LOL、吃鸡里面的人物和场景就是用这个做的。这就是一个简单的正方体3D模型比2D平面更加优秀的是，观察的更加直观。 </p><p>建模软件常用的有…… 每个软件都有各自的特色。</p><p>Zbrush适合于对生物角色的建模，3dMax在各大公司的使用都非常的广泛，Maya在三维动画和视觉领域非常的出色，Blender是近几年来人气暴增并且开发源代码免费三维建模软件。</p><p>在建模阶段，设计师通过美工的草图或者三视图直接绘制出三维空间中可以自由观看的立体模型，他们看起来都异常的简陋，甚至没有任何颜色，像左图这种，所以在下个阶段，建模师会在三维模型表面绘制出纹理和材质，比如说金属表面的光泽度很高，而玻璃材质的物体不但反光还是透明的，石头的表面有不规则的纹路和裂痕，这些对模型都非常的重要，好的材质会让我们的模型在游戏中显得更加自然和真实。最后我们的模型大概是长这个样子的，他们就是游戏中直接加载的资源，有了模型之后，还需要对人物怪兽这些会动的物体进行动画制作，这里不介绍了。</p><p><img src="https://pic.imgdb.cn/item/6128376244eaada739389e1a.jpg"></p><h2 id="游戏框架（引擎）"><a href="#游戏框架（引擎）" class="headerlink" title="游戏框架（引擎）"></a>游戏框架（引擎）</h2><p><img src="https://pic.imgdb.cn/item/6128376244eaada739389e27.jpg"></p><p>从第一款真正意义的电子游戏诞生至今，游戏已经走过了FC、街机、PC、页游四个时代。在移动互联网新技术的加持下，游戏产业已经来到井喷发展的主机：手游时代。</p><p>目前主流的开发引擎有两个，一个是Unity，一个是UE。</p><p>常见的比如王者荣耀和炉石传说都是使用Unity进行开发的，包括和平精英也是可以使用Unity去实现的。</p><p>Unity它里面封装了很多游戏开发所需要的功能，本身还支持了很多其他的技术领域，例如工艺仿真，动漫，电影，手机APP，包括我们刚才说的虚拟现实和增强现实。</p><h2 id="UE5效果展示"><a href="#UE5效果展示" class="headerlink" title="UE5效果展示"></a>UE5效果展示</h2><p><a href="https://imgtu.com/i/hKwizn"><img src="https://z3.ax1x.com/2021/08/27/hKwizn.gif" alt="hKwizn.gif"></a></p><p>对于「雪」的处理，而在角色、武器向不同方向移动的时候，雪地也会有相应的变化，并把痕迹留存下来。</p><p><a href="https://imgtu.com/i/hKwMW9"><img src="https://z3.ax1x.com/2021/08/27/hKwMW9.gif" alt="hKwMW9.gif"></a></p><p>在原地盘旋的时候，如何表现出白龙身体不同部位向不同角度的扭曲？如何让它有一种快慢交替的节奏？在冲刺的一瞬间，它的肢体又该如何纵向伸展？</p><h2 id="Unity效果展示"><a href="#Unity效果展示" class="headerlink" title="Unity效果展示"></a>Unity效果展示</h2><p><a href="https://imgtu.com/i/hK0jUS"><img src="https://z3.ax1x.com/2021/08/27/hK0jUS.gif" alt="hK0jUS.gif"></a></p><h2 id="Unity与UE"><a href="#Unity与UE" class="headerlink" title="Unity与UE"></a>Unity与UE</h2><p>C#是一种托管语言，和c++比，不够Native，不够高效，和脚本语言比，又不够灵活。</p><p>但它胜在全面。</p><p>强类型，跨平台，语法糖，应有尽有。</p><p>比简单的脚本语言强大，比c++更简单易用。</p><p>使用哪一款游戏引擎去制作一款游戏，并不是根据使用者的喜好来制定的。开发者们的喜好千奇百怪，但最终能在技术选型中影响到结果的，往往是技术积累以及引擎本身的特性。</p><p>参考：<a href="https://www.zhihu.com/question/37124677/answer/1322231841">知乎</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="游戏制作" scheme="http://example.com/tags/%E6%B8%B8%E6%88%8F%E5%88%B6%E4%BD%9C/"/>
    
    <category term="引擎" scheme="http://example.com/tags/%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>三维重建简介</title>
    <link href="http://example.com/2021/08/24/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%AE%80%E4%BB%8B/"/>
    <id>http://example.com/2021/08/24/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%AE%80%E4%BB%8B/</id>
    <published>2021-08-24T01:05:34.000Z</published>
    <updated>2021-08-24T01:27:16.606Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="三维重建简单介绍"><a href="#三维重建简单介绍" class="headerlink" title="三维重建简单介绍"></a>三维重建简单介绍</h1><p><img src="https://pic.imgdb.cn/item/6124467044eaada739561cd8.jpg"></p><p>我们现实世界的物体，比如说有一只兔子，它有几种表示方法。</p><p>很多点数据所构成的表征，来表示这个物体，这是点云，除了点云，还有Mesh的方法，是用很多三角形的面片或正方形的面片所拼成的这样一个物体，还有Volumetric这是一种栅格化的一种表征方法，在这张图我们看到有很多小的蓝色的栅格，很多的栅格就拼成了这样一只兔子，还有一种叫projected View 这是一种通过图片，不同的角度来构成一个立体的一个兔子的形状，它有RGB的颜色和深度的信息。</p><p><a href="https://pic.imgdb.cn/item/6124469444eaada7395649fd.jpg"><img src="https://pic.imgdb.cn/item/6124469444eaada7395649fd"></a></p><p> 点云可以通过激光雷达扫描所得到，也可以通过摄影的方法，获取RGB图像，和深度信息的图像，然后通过透视几何可以反推出一些空间中的点云数据，这是通过一个角度来构造这种三维数据，现在一般是通过多个摄像头，倾斜摄影来构成点云数据。我们可以看到在这张图片上真正的点云一般有位置信息，空间的三维坐标XYZ，通过图像的话还可以获取到颜色信息，当然还有一些其他的方法获取强度信息，法向量等</p><p><img src="https://pic.imgdb.cn/item/6124469444eaada739564a04.jpg"></p><p> 严格来说呢，RGB结合深度信息的方法只能称为2.5D，真正的点云的数据一般是通过激光雷达扫描所得到</p><p> <img src="https://pic.imgdb.cn/item/6124469444eaada739564a0c.jpg"></p><p><img src="https://pic.imgdb.cn/item/6124469444eaada739564a0c.jpg"></p><p><img src="https://pic.imgdb.cn/item/6124469444eaada739564a19.jpg"></p><p>关于深度相机的一些相关介绍</p><p><img src="https://pic.imgdb.cn/item/6124469444eaada7395649f9.jpg"></p><p>我们通过用相机拍摄真实世界的物体、场景，并通过计算机视觉技术进行处理，从而得到物体的三维模型，主动比如说是激光，被动呢可能会是影像。单幅图像是无法重建场景结构的，虽然我们知道投影过程，但是是没有深度信息的，比如我们这右边图，这个人和比萨斜塔，真实情况肯定不是这样的，这是拍出来的效果，所以说只凭单幅图像是无法恢复出深度信息的。下面这个三视图是二视图的一个扩展，比如说这个立方体x1这个点，我们这三个相机都可以观察到这个投影点，那么这个点就是这三个相机的投影点。</p><p><img src="https://pic.imgdb.cn/item/6124469d44eaada7395654cc.jpg"></p><p> 我们先看左边这幅图，首先我们拿到一系列的图集，然后进行特征的提取，特征点的提取和描述，然后进行特征点的匹配和滤波（也就是错误特征点的去除），然后得到我们匹配好的投影点，这一步是sift特征提取，然后估计他的姿态参数，通过Bundle Adjustment进一步优化，得到一个稀疏的点云，也就是一个稀疏的场景结构，那么我们如何想要得到一个密集的场景结构，向右图这种，我们需要做密集点云的一个匹配，这个过程叫MVS，然后我们就可以得到密集的点云。后面的部分还有一些，如点云融合，初始网络重建，网络优化，我们最后得到的一个输出结果（纹理贴图），由……。左侧这边，因为我们有很多图像，这里的话还有一个图像聚类的过程，以及添加控制点的过程，这里就是为了让我们更高精度的完成一个大尺度重建。</p><p><img src="https://pic.imgdb.cn/item/6124469d44eaada7395654d2.jpg"></p><p>附：</p><p>SFM（运动恢复结构）：通过相机运动恢复场景的结构，从图像中恢复出稀疏的三维点坐标，用这些稀疏的三维点坐标，表示场景的结构。</p><p>MVS（密集重建）：关键是<strong>寻找空间中具有图像一致性的点。</strong><br>目前MVS稠密重建这个研究方向都在朝着深度学习的方向来做研究和优化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="三维重建" scheme="http://example.com/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"/>
    
    <category term="三维点云" scheme="http://example.com/tags/%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>三维点云介绍</title>
    <link href="http://example.com/2021/08/16/%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91%E4%BB%8B%E7%BB%8D/"/>
    <id>http://example.com/2021/08/16/%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91%E4%BB%8B%E7%BB%8D/</id>
    <published>2021-08-16T11:23:49.000Z</published>
    <updated>2021-08-16T11:39:35.022Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="点云基础"><a href="#点云基础" class="headerlink" title="点云基础"></a>点云基础</h1><h2 id="点云数据"><a href="#点云数据" class="headerlink" title="点云数据"></a>点云数据</h2><p><a href="https://imgtu.com/i/ffizkT"><img src="https://z3.ax1x.com/2021/08/16/ffizkT.jpg" alt="ffizkT.jpg"></a></p><p>点云是某个坐标系下的点的数据集。<br>点包含了丰富的信息，包括三维坐标X，Y，Z、强度值（因为点云经常是由激光来采取的，强度是指它从表面反射回来所获取的激光能量信息）、当然我们配上相机的话就会有颜色、时间戳是跟无人驾驶有关，比如说在无人车中，车在动，每个点也在动，激光也在转，所以每个点也都有个时间戳，这个时间戳方便我们把不同的点云放到同一个坐标系下面<br>点云可以将现实世界原子化，通过高精度的点云数据可以还原现实世界。<br>上面这张图片我们可以看到是一个居民住处的点云，我们可以看到是已经做好分类的，棕黄色是屋顶，绿色是植被，剩下的是地面。<br>分类：有组织的：点云被布置为类似于图像结构的二维的点矩阵。<br>我们可以根据像素（x,y）信息来访问这个点云，大部分是由深度相机获得的。我们可以看做是一张图片，每个点它有深度信息，可以算他的三维坐标，上面图片我们就无法把它转换成一个二维的图片，除非要损失一些点的信息，这就是无组织的。<br>无组织的：点云是一个点列表<br>有序点云：一般由深度图还原的点云，有序点云按照图方阵一行一行的，从左上角到右下角排列，当然其中有一些无效点因为。有序点云按顺序排列，可以很容易的找到它的相邻点信息。有序点云在某些处理的时候还是很便利的，但是很多情况下是无法获取有序点云的。<br>无序点云：无序点云就是其中的点的集合，点排列之间没有任何顺序，点的顺序交换后没有任何影响。是比较普遍的点云形式，有序点云也可看做无序点云来处理。</p><h2 id="点云获取"><a href="#点云获取" class="headerlink" title="点云获取"></a>点云获取</h2><p><a href="https://imgtu.com/i/ffij00"><img src="https://z3.ax1x.com/2021/08/16/ffij00.jpg" alt="ffij00.jpg"></a></p><p>最常见的是激光扫描仪了，激光打出去，然后收回来得到距离，根据传感器的位置信息就可以反算点的三维坐标就可以得到点云。LiDAR获取的数据就是点云数据，同时也对点云数据进行处理加工以及应用。<br>星载LiDAR采用卫星平台，运行轨道高、观测视野广，基本可以测量到地球的每一个角落，为三维控制点和数字高程模型的获取提供了新的途径，有些星载激光雷达还具有观察整个天体的能力。<br>机载主要借助无人机（UAV{无人驾驶飞机}/UAS{无人机系统}）进行大规模的点云数据采集。<br>地面分为三小种：地上三维激光扫描、车载MMS、手持激光扫描。<br>双目相机是模仿人眼</p><h2 id="点云应用领域"><a href="#点云应用领域" class="headerlink" title="点云应用领域"></a>点云应用领域</h2><p><a href="https://imgtu.com/i/ffiv7V"><img src="https://z3.ax1x.com/2021/08/16/ffiv7V.jpg" alt="ffiv7V.jpg"></a></p><p>点云的应用也日益广泛，应用在机器人和无人驾驶上来获得周围环境和物体的三维信息来帮助自动驾驶的实现，还可用在AR（增强现实），VR（虚拟现实）中，还有工业设计，比如Shape Design。FaceID就是通过点云获取人脸的三维特征来作为区分人脸的标识。</p><h2 id="点云相关研究"><a href="#点云相关研究" class="headerlink" title="点云相关研究"></a>点云相关研究</h2><p><a href="https://imgtu.com/i/ffiXmq"><img src="https://z3.ax1x.com/2021/08/16/ffiXmq.jpg" alt="ffiXmq.jpg"></a></p><p><a href="https://imgtu.com/i/ffiLXn"><img src="https://z3.ax1x.com/2021/08/16/ffiLXn.jpg" alt="ffiLXn.jpg"></a></p><p>点云相关研究，这里只列出3个</p><h2 id="点云数据集"><a href="#点云数据集" class="headerlink" title="点云数据集"></a>点云数据集</h2><p><a href="https://imgtu.com/i/ffFStU"><img src="https://z3.ax1x.com/2021/08/16/ffFStU.jpg" alt="ffFStU.jpg"></a></p><p>关于点云数据集，第一个3d形状分类，常见有ModelNet40、ShapeNet、ScanNet等，关于3d物体的检测和追踪，比如KITTI，是关于自动驾驶场景中所获得的一些数据集，还有ScanNetV2、Waymo Open等，对于3维点云分割，有ScanNet、S3Dis等等。</p><h2 id="从传统技术到深度学习"><a href="#从传统技术到深度学习" class="headerlink" title="从传统技术到深度学习"></a>从传统技术到深度学习</h2><p><a href="https://imgtu.com/i/ffFCp4"><img src="https://z3.ax1x.com/2021/08/16/ffFCp4.jpg" alt="ffFCp4.jpg"></a></p><p><a href="https://imgtu.com/i/ffFphF"><img src="https://z3.ax1x.com/2021/08/16/ffFphF.jpg" alt="ffFphF.jpg"></a></p><p>传统点云的三维重建，大都是通过一些算法构造出三维表面网格模型，比如SFM，是一种基于各种收集到的无序图片进行三维重建的离线算法，从时间系列的2D图像中推算3D信息</p><p>步骤：</p><ol><li><p><strong>特征提取</strong>（SIFT, SURF, FAST等一堆方法）</p></li><li><p><strong>配准</strong>（主流是RANSAC和它的改进版</p></li><li><p><strong>全局优化</strong>bundle adjustment  用来估计相机参数</p></li><li><p><strong>数据融合</strong></p></li></ol><p>3D点云应用深度学习面临的挑战。首先在神经网络上面临的挑战：</p><ol><li>非结构化数据（无网格）：点云是分布在空间中的XYZ点。 没有结构化的网格来帮助CNN滤波器。</li><li>不变性排列：点云本质上是一长串点（nx3矩阵，其中n是点数）。 在几何上，点的顺序不影响它在底层矩阵结构中的表示方式，不同的点排一个顺序还是同一个点云，例如， 相同的点云可以由两个完全不同的矩阵表示。</li><li>点云数量上的变化：在图像中，像素的数量是一个给定的常数，取决于相机。 然而，点云的数量可能会有很大的变化，这取决于各种传感器。</li></ol><p>在点云数据方面的挑战：</p><ol><li>缺少数据：扫描的模型通常被遮挡，部分数据丢失。</li><li>噪音：所有传感器都是嘈杂的。 有几种类型的噪声，包括点云扰动和异常值。 这意味着一个点有一定的概率位于它被采样的地方（扰动）附近的某一半径范围内，或者它可能出现在空间的任意位置（异常值）。</li><li>旋转：一辆车向左转，同一辆车向右转，会有不同的点云代表同一辆车。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="三维点云" scheme="http://example.com/tags/%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>关于hexo博客的一些问题</title>
    <link href="http://example.com/2021/08/06/%E5%85%B3%E4%BA%8Ehexo%E5%8D%9A%E5%AE%A2%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2021/08/06/%E5%85%B3%E4%BA%8Ehexo%E5%8D%9A%E5%AE%A2%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/</id>
    <published>2021-08-06T10:17:13.000Z</published>
    <updated>2021-09-03T02:56:31.544Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h1><p>近些天折腾了许久Hexo博客，包括主题，升级插件，添加评论功能，文章热度统计等记录一下.</p><h2 id="升级hexo"><a href="#升级hexo" class="headerlink" title="升级hexo"></a>升级hexo</h2><p>这是我现在hexo的版本5.4.0，之前是4.2.0，由于只使用npm -update 并不能更新到5.0.0之后的版本（不知道为什么）</p><p><a href="https://imgtu.com/i/fu78M9"><img src="https://z3.ax1x.com/2021/08/07/fu78M9.png" alt="fu78M9.png"></a></p><p>升级办法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//以下命令分别执行即可</span><br><span class="line">npm install -g npm-check     //安装npm-check</span><br><span class="line">npm-check                    //查看系统插件是否需要升级</span><br><span class="line">npm install -g npm-upgrade   //安装npm-upgrade</span><br><span class="line">npm-upgrade        //更新package.json</span><br><span class="line">//在执行npm-upgrade命令后会要求输入yes或者no，直接输入Yes或Y即可</span><br><span class="line">npm update -g      //更新全局插件</span><br><span class="line">npm update --save  //更新系统插件</span><br></pre></td></tr></table></figure><p>Node.js的版本也不能太低，应该</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br><span class="line">v12.8.2</span><br></pre></td></tr></table></figure><p>最后<code> hexo -v</code>查看版本</p><p>因为hexo5.0.0以上有<strong>代码高亮</strong></p><h2 id="hexo-d"><a href="#hexo-d" class="headerlink" title="hexo d"></a>hexo d</h2><p>由于网络等原因，莫名其妙会无法部署上去出现<code>fatal: unable to access ‘https://github.com/...‘</code></p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><ol><li><p>只能尝试，有时重新clean g  d 会解决</p><p>hexo clean  </p><p>hexo g</p><p>hexo d</p></li><li><p>百度的一些命令（）</p><p>在开启shadowsocks的前提下，手动配置git的代理</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.proxy http://127.0.0.1:1080</span><br><span class="line"></span><br><span class="line">git config --global https.proxy http://127.0.0.1:1080</span><br></pre></td></tr></table></figure><p>取消代理：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global --unset http.proxy</span><br><span class="line"></span><br><span class="line">git config --global --unset https.proxy</span><br></pre></td></tr></table></figure></li><li><p>玄学解决，试几次就成功了</p></li><li><p>ipconfig /flushdns</p></li></ol><h2 id="评论系统"><a href="#评论系统" class="headerlink" title="评论系统"></a>评论系统</h2><p>这里着实费了一些时间，由于gal上gitment和gitalk无法使用，于是换了主题改用Valine，具体配置不写了百度很多，遇到了一个小问题</p><h3 id="Code-403-访问被api域名白名单拒绝，请检查你的安全域名设置-·Issue-72-·-xCss-Valine"><a href="#Code-403-访问被api域名白名单拒绝，请检查你的安全域名设置-·Issue-72-·-xCss-Valine" class="headerlink" title="[Code 403: 访问被api域名白名单拒绝，请检查你的安全域名设置. ·Issue #72 · xCss/Valine]"></a>[Code 403: 访问被api域名白名单拒绝，请检查你的安全域名设置. ·Issue #72 · xCss/Valine]</h3><p>问题解决在github上，Valine里的issue里  <a href="https://github.com/xCss/Valine/issues/72">https://github.com/xCss/Valine/issues/72</a></p><p>这里说下我的，在web安全域名上我填写了两个地址：<strong>http与https</strong>，只填写https还是会出现Code 403问题，不知道是不是http的原因。</p><p>由于舍弃了上一个主题，这个主题的live-2d看板娘与Valine评论里的提交重叠，只能舍弃看板娘:cry:</p><h3 id="评论头像问题"><a href="#评论头像问题" class="headerlink" title="评论头像问题"></a>评论头像问题</h3><p><a href="https://imgtu.com/i/fNfkuT"><img src="https://z3.ax1x.com/2021/08/11/fNfkuT.png" alt="fNfkuT.png"></a></p><p>Gravatar官网上注册账号，上传头像，并在评论写上注册的邮箱号即可显示，<strong>缺点是需要翻墙</strong>，页面显示头像和登录Gravatar注册都需要:cry:</p><h2 id="更新hexo完图片读取不出来"><a href="#更新hexo完图片读取不出来" class="headerlink" title="更新hexo完图片读取不出来"></a>更新hexo完图片读取不出来</h2><p>又是一个坑</p><p>由于更新了主题与hexo，插件的功能似乎不能用了，使用了图床的方式粘贴url地址来显示hexo d的图片，不重新搭博客了。</p><p><a href="https://blog.csdn.net/weixin_41010198/article/details/119698015">https://blog.csdn.net/weixin_41010198/article/details/119698015</a></p><h2 id="hexo-tag-aplayer"><a href="#hexo-tag-aplayer" class="headerlink" title="hexo-tag-aplayer"></a>hexo-tag-aplayer</h2><p>设置</p><hr><p>暂时写到这，以后会继续更新</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="hexo" scheme="http://example.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet代码复现</title>
    <link href="http://example.com/2021/08/05/AlexNet%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    <id>http://example.com/2021/08/05/AlexNet%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/</id>
    <published>2021-08-05T03:37:41.000Z</published>
    <updated>2021-08-11T09:19:55.791Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="AlexNet实现代码部分"><a href="#AlexNet实现代码部分" class="headerlink" title="AlexNet实现代码部分"></a>AlexNet实现代码部分</h1><p>[TOC]</p><p>hexo博客不支持目录toc、csdn支持</p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>python == 3.6</p><p>torch-gpu == 1.2.0</p><p>torchvision == 0.4.0</p><p>numpy == 1.19.5</p><p>matplotlib == 3.3.4</p><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><p>pycharm</p><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>代码文件有3个，</p><ol><li>alexnet_inference.py 在预训练好的AlexNet模型里，给定图片，进行TOP5判断，输出5个可能的分类</li><li>alexnet_visualizaton.py 卷积核与特征图的可视化，使用tensorboard</li><li>train_alexnet.py 在kaggle上下载猫狗数据集训练一个AlexNet</li></ol><h2 id="alexnet-inference-py"><a href="#alexnet-inference-py" class="headerlink" title="alexnet_inference.py"></a>alexnet_inference.py</h2><h3 id="路径结构"><a href="#路径结构" class="headerlink" title="路径结构"></a>路径结构</h3><p><a href="https://imgtu.com/i/fuHamn"><img src="https://z3.ax1x.com/2021/08/07/fuHamn.png" alt="fuHamn.png"></a></p><h3 id="导入环境"><a href="#导入环境" class="headerlink" title="导入环境"></a>导入环境</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;NLS_LANG&#x27;</span>] = <span class="string">&#x27;SIMPLIFIED CHINESE_CHINA.UTF8&#x27;</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><p>导包。</p><p>最后一行device，如果cuda可用，使用GPU，如果不可用使用CPU。GPU：device:0 </p><h3 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># config</span></span><br><span class="line">    path_state_dict = os.path.join(BASE_DIR, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;data&quot;</span>, <span class="string">&quot;alexnet-owt-4df8aa71.pth&quot;</span>)</span><br><span class="line">    path_img = os.path.join(BASE_DIR, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;data&quot;</span>, <span class="string">&quot;Golden Retriever from baidu.jpg&quot;</span>)</span><br><span class="line">    <span class="comment"># path_img = os.path.join(BASE_DIR, &quot;..&quot;, &quot;data&quot;, &quot;tiger cat.jpg&quot;)</span></span><br><span class="line">    path_classnames = os.path.join(BASE_DIR, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;data&quot;</span>, <span class="string">&quot;imagenet1000.json&quot;</span>)</span><br><span class="line">    path_classnames_cn = os.path.join(BASE_DIR, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;data&quot;</span>, <span class="string">&quot;imagenet_classnames.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load class names</span></span><br><span class="line">    cls_n, cls_n_cn = load_class_names(path_classnames, path_classnames_cn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1/5 load img</span></span><br><span class="line">    img_tensor, img_rgb = process_img(path_img)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2/5 load model</span></span><br><span class="line">    alexnet_model = get_model(path_state_dict, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3/5 inference  tensor --&gt; vector</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        time_tic = time.time()</span><br><span class="line">        outputs = alexnet_model(img_tensor)</span><br><span class="line">        time_toc = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4/5 index to class names</span></span><br><span class="line">    _, pred_int = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">    _, top5_idx = torch.topk(outputs.data, <span class="number">5</span>, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    pred_idx = <span class="built_in">int</span>(pred_int.cpu().numpy())</span><br><span class="line">    pred_str, pred_cn = cls_n[pred_idx], cls_n_cn[pred_idx]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;img: &#123;&#125; is: &#123;&#125;\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(os.path.basename(path_img), pred_str, pred_cn))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;time consuming:&#123;:.2f&#125;s&quot;</span>.<span class="built_in">format</span>(time_toc - time_tic))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5/5 visualization</span></span><br><span class="line">    plt.imshow(img_rgb)</span><br><span class="line">    plt.title(<span class="string">&quot;predict:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(pred_str))</span><br><span class="line">    top5_num = top5_idx.cpu().numpy().squeeze()</span><br><span class="line">    text_str = [cls_n[t] <span class="keyword">for</span> t <span class="keyword">in</span> top5_num]</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(top5_num)):</span><br><span class="line">        plt.text(<span class="number">5</span>, <span class="number">15</span>+idx*<span class="number">30</span>, <span class="string">&quot;top &#123;&#125;:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(idx+<span class="number">1</span>, text_str[idx]), bbox=<span class="built_in">dict</span>(fc=<span class="string">&#x27;yellow&#x27;</span>))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>alexnet网络模型需提前下载好，然后加载路径到path_state_dict，同时加载好path_img，需要测试的图片，下面是注释掉另一张测试图片。</p><p>path_classnames和path_classnames_cn对应我们想要输出图片的1000个类别，一个是json文件，英文版。_cn是中文版，txt文件</p><p>然后加载  load class names </p><p>load img </p><p>load model tensor –&gt; vector</p><p>……</p><h3 id="其他函数"><a href="#其他函数" class="headerlink" title="其他函数"></a>其他函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_transform</span>(<span class="params">img_rgb, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将数据转换为模型读取的形式</span></span><br><span class="line"><span class="string">    :param img_rgb: PIL Image</span></span><br><span class="line"><span class="string">    :param transform: torchvision.transform</span></span><br><span class="line"><span class="string">    :return: tensor</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> transform <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;找不到transform！必须有transform对img进行处理&quot;</span>)</span><br><span class="line"></span><br><span class="line">    img_t = transform(img_rgb)</span><br><span class="line">    <span class="keyword">return</span> img_t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_class_names</span>(<span class="params">p_clsnames, p_clsnames_cn</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载标签名</span></span><br><span class="line"><span class="string">    :param p_clsnames:</span></span><br><span class="line"><span class="string">    :param p_clsnames_cn:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(p_clsnames, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        class_names = json.load(f)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(p_clsnames_cn, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:  <span class="comment"># 设置文件对象</span></span><br><span class="line">        class_names_cn = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> class_names, class_names_cn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span>(<span class="params">path_state_dict, vis_model=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创建模型，加载参数</span></span><br><span class="line"><span class="string">    :param path_state_dict:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = models.alexnet()</span><br><span class="line">    pretrained_state_dict = torch.load(path_state_dict)</span><br><span class="line">    model.load_state_dict(pretrained_state_dict)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> vis_model:</span><br><span class="line">        <span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line">        summary(model, input_size=(<span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>), device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_img</span>(<span class="params">path_img</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># hard code</span></span><br><span class="line">    norm_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">    norm_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">    inference_transform = transforms.Compose([</span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        transforms.CenterCrop((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(norm_mean, norm_std),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># path --&gt; img</span></span><br><span class="line">    img_rgb = Image.<span class="built_in">open</span>(path_img).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># img --&gt; tensor</span></span><br><span class="line">    img_tensor = img_transform(img_rgb, inference_transform)</span><br><span class="line">    img_tensor.unsqueeze_(<span class="number">0</span>)        <span class="comment"># chw --&gt; bchw</span></span><br><span class="line">    img_tensor = img_tensor.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img_tensor, img_rgb</span><br></pre></td></tr></table></figure><h3 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h3><p><a href="https://imgtu.com/i/fuHBkV"><img src="https://z3.ax1x.com/2021/08/07/fuHBkV.png" alt="fuHBkV.png"></a></p><h2 id="alexnet-visualizaton-py"><a href="#alexnet-visualizaton-py" class="headerlink" title="alexnet_visualizaton.py"></a>alexnet_visualizaton.py</h2><p>导包：略</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torchvision.utils <span class="keyword">as</span> vutils</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="主函数-1"><a href="#主函数-1" class="headerlink" title="主函数"></a>主函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    log_dir = os.path.join(BASE_DIR, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;results&quot;</span>)</span><br><span class="line">    <span class="comment"># ----------------------------------- kernel visualization -----------------------------------</span></span><br><span class="line">    writer = SummaryWriter(log_dir=log_dir, filename_suffix=<span class="string">&quot;_kernel&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># m1</span></span><br><span class="line">    <span class="comment"># alexnet = models.alexnet(pretrained=True)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># m2</span></span><br><span class="line">    path_state_dict = os.path.join(BASE_DIR, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;data&quot;</span>, <span class="string">&quot;alexnet-owt-4df8aa71.pth&quot;</span>)</span><br><span class="line">    alexnet = models.alexnet()</span><br><span class="line">    pretrained_state_dict = torch.load(path_state_dict)</span><br><span class="line">    alexnet.load_state_dict(pretrained_state_dict)</span><br><span class="line"></span><br><span class="line">    kernel_num = -<span class="number">1</span></span><br><span class="line">    vis_max = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> sub_module <span class="keyword">in</span> alexnet.modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(sub_module, nn.Conv2d):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        kernel_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> kernel_num &gt; vis_max:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        kernels = sub_module.weight</span><br><span class="line">        c_out, c_int, k_h, k_w = <span class="built_in">tuple</span>(kernels.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 拆分channel</span></span><br><span class="line">        <span class="keyword">for</span> o_idx <span class="keyword">in</span> <span class="built_in">range</span>(c_out):</span><br><span class="line">            kernel_idx = kernels[o_idx, :, :, :].unsqueeze(<span class="number">1</span>)  <span class="comment"># 获得(3, h, w), 但是make_grid需要 BCHW，这里拓展C维度变为（3， 1， h, w）</span></span><br><span class="line">            kernel_grid = vutils.make_grid(kernel_idx, normalize=<span class="literal">True</span>, scale_each=<span class="literal">True</span>, nrow=c_int)</span><br><span class="line">            writer.add_image(<span class="string">&#x27;&#123;&#125;_Convlayer_split_in_channel&#x27;</span>.<span class="built_in">format</span>(kernel_num), kernel_grid, global_step=o_idx)</span><br><span class="line"></span><br><span class="line">        kernel_all = kernels.view(-<span class="number">1</span>, <span class="number">3</span>, k_h, k_w)  <span class="comment"># 3, h, w</span></span><br><span class="line">        kernel_grid = vutils.make_grid(kernel_all, normalize=<span class="literal">True</span>, scale_each=<span class="literal">True</span>, nrow=<span class="number">8</span>)  <span class="comment"># c, h, w</span></span><br><span class="line">        writer.add_image(<span class="string">&#x27;&#123;&#125;_all&#x27;</span>.<span class="built_in">format</span>(kernel_num), kernel_grid, global_step=<span class="number">620</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;_convlayer shape:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(kernel_num, <span class="built_in">tuple</span>(kernels.shape)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------------------- feature map visualization ------------------------</span></span><br><span class="line">    writer = SummaryWriter(log_dir=log_dir, filename_suffix=<span class="string">&quot;_feature map&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据</span></span><br><span class="line">    path_img = os.path.join(BASE_DIR, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;data&quot;</span>, <span class="string">&quot;tiger cat.jpg&quot;</span>)  <span class="comment"># your path to image</span></span><br><span class="line">    normMean = [<span class="number">0.49139968</span>, <span class="number">0.48215827</span>, <span class="number">0.44653124</span>]</span><br><span class="line">    normStd = [<span class="number">0.24703233</span>, <span class="number">0.24348505</span>, <span class="number">0.26158768</span>]</span><br><span class="line">    norm_transform = transforms.Normalize(normMean, normStd)</span><br><span class="line">    img_transforms = transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        norm_transform</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    img_pil = Image.<span class="built_in">open</span>(path_img).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">    img_tensor = img_transforms(img_pil)</span><br><span class="line">    img_tensor.unsqueeze_(<span class="number">0</span>)  <span class="comment"># chw --&gt; bchw</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型</span></span><br><span class="line">    <span class="comment"># alexnet = models.alexnet(pretrained=True)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    convlayer1 = alexnet.features[<span class="number">0</span>]</span><br><span class="line">    fmap_1 = convlayer1(img_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理</span></span><br><span class="line">    fmap_1.transpose_(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># bchw=(1, 64, 55, 55) --&gt; (64, 1, 55, 55)</span></span><br><span class="line">    fmap_1_grid = vutils.make_grid(fmap_1, normalize=<span class="literal">True</span>, scale_each=<span class="literal">True</span>, nrow=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    writer.add_image(<span class="string">&#x27;feature map in conv1&#x27;</span>, fmap_1_grid, global_step=<span class="number">620</span>)</span><br><span class="line">    writer.close()</span><br></pre></td></tr></table></figure><h2 id="train-alexnet-py"><a href="#train-alexnet-py" class="headerlink" title="train_alexnet.py"></a>train_alexnet.py</h2><p>这一部分是关于训练一个AlexNet实现一个二分类问题</p><h3 id="首先代码结构"><a href="#首先代码结构" class="headerlink" title="首先代码结构"></a>首先代码结构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">构建DataLoader--&gt;构建模型</span><br><span class="line">构建模型--&gt;构建损失函数</span><br><span class="line">构建损失函数--&gt;构建优化器</span><br><span class="line">构建优化器--&gt;迭代训练</span><br></pre></td></tr></table></figure><p>分为5个步骤。</p><h3 id="关于train数据集"><a href="#关于train数据集" class="headerlink" title="关于train数据集"></a>关于train数据集</h3><p>25000张图片，猫狗各占一半，图片内容比较复杂，不仅仅只有猫和狗，有的图片有一些栅栏背景，有的图片有人，而且人的大小大于猫或狗大小。</p><p>我们这里只需要train这个文件夹。</p><p>导包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> tools.my_dataset <span class="keyword">import</span> CatDogDataset</span><br><span class="line"></span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><p>最后一行代码意义和第一个.py含义一样，if available GPU else CPU</p><h3 id="主函数-2"><a href="#主函数-2" class="headerlink" title="主函数"></a>主函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config</span></span><br><span class="line">   data_dir = os.path.join(BASE_DIR, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;data&quot;</span>, <span class="string">&quot;train&quot;</span>)</span><br><span class="line">   path_state_dict = os.path.join(BASE_DIR, <span class="string">&quot;..&quot;</span>, <span class="string">&quot;data&quot;</span>, <span class="string">&quot;alexnet-owt-4df8aa71.pth&quot;</span>)</span><br><span class="line">   num_classes = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">   MAX_EPOCH = <span class="number">3</span>       <span class="comment"># 可自行修改</span></span><br><span class="line">   BATCH_SIZE = <span class="number">128</span>    <span class="comment"># 可自行修改</span></span><br><span class="line">   LR = <span class="number">0.001</span>          <span class="comment"># 可自行修改</span></span><br><span class="line">   log_interval = <span class="number">1</span>    <span class="comment"># 可自行修改</span></span><br><span class="line">   val_interval = <span class="number">1</span>    <span class="comment"># 可自行修改</span></span><br><span class="line">   classes = <span class="number">2</span></span><br><span class="line">   start_epoch = -<span class="number">1</span></span><br><span class="line">   lr_decay_step = <span class="number">1</span>   <span class="comment"># 可自行修改</span></span><br></pre></td></tr></table></figure><p>主函数的config，超参数和路径可以自己设置</p><p>然后是5个模块，和第一个python文件5步骤对应</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ============================ step 1/5 数据 ============================</span></span><br><span class="line">   norm_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">   norm_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"></span><br><span class="line">   train_transform = transforms.Compose([</span><br><span class="line">       transforms.Resize((<span class="number">256</span>)),      <span class="comment"># (256, 256) 区别</span></span><br><span class="line">       transforms.CenterCrop(<span class="number">256</span>),</span><br><span class="line">       transforms.RandomCrop(<span class="number">224</span>),</span><br><span class="line">       transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),</span><br><span class="line">       transforms.ToTensor(),</span><br><span class="line">       transforms.Normalize(norm_mean, norm_std),</span><br><span class="line">   ])</span><br><span class="line"></span><br><span class="line">   normalizes = transforms.Normalize(norm_mean, norm_std)</span><br><span class="line">   valid_transform = transforms.Compose([</span><br><span class="line">       transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">       transforms.TenCrop(<span class="number">224</span>, vertical_flip=<span class="literal">False</span>),</span><br><span class="line">       transforms.Lambda(<span class="keyword">lambda</span> crops: torch.stack([normalizes(transforms.ToTensor()(crop)) <span class="keyword">for</span> crop <span class="keyword">in</span> crops])),</span><br><span class="line">   ])</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 构建MyDataset实例</span></span><br><span class="line">   train_data = CatDogDataset(data_dir=data_dir, mode=<span class="string">&quot;train&quot;</span>, transform=train_transform)</span><br><span class="line">   valid_data = CatDogDataset(data_dir=data_dir, mode=<span class="string">&quot;valid&quot;</span>, transform=valid_transform)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 构建DataLoder</span></span><br><span class="line">   train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line">   valid_loader = DataLoader(dataset=valid_data, batch_size=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>对数据模块进行设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ============================ step 2/5 模型 ==========================</span></span><br><span class="line">alexnet_model = get_model(path_state_dict, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">num_ftrs = alexnet_model.classifier._modules[<span class="string">&quot;6&quot;</span>].in_features</span><br><span class="line">alexnet_model.classifier._modules[<span class="string">&quot;6&quot;</span>] = nn.Linear(num_ftrs, num_classes)</span><br><span class="line"></span><br><span class="line">alexnet_model.to(device)</span><br><span class="line"><span class="comment"># ============================ step 3/5 损失函数 =======================</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># ============================ step 4/5 优化器 =========================</span></span><br><span class="line"><span class="comment"># 冻结卷积层</span></span><br><span class="line">flag = <span class="number">0</span></span><br><span class="line"><span class="comment"># flag = 1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    fc_params_id = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, alexnet_model.classifier.parameters()))  <span class="comment"># 返回的是parameters的 内存地址</span></span><br><span class="line">    base_params = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: <span class="built_in">id</span>(p) <span class="keyword">not</span> <span class="keyword">in</span> fc_params_id, alexnet_model.parameters())</span><br><span class="line">    optimizer = optim.SGD([</span><br><span class="line">        &#123;<span class="string">&#x27;params&#x27;</span>: base_params, <span class="string">&#x27;lr&#x27;</span>: LR * <span class="number">0.1</span>&#125;,  <span class="comment"># 0</span></span><br><span class="line">        &#123;<span class="string">&#x27;params&#x27;</span>: alexnet_model.classifier.parameters(), <span class="string">&#x27;lr&#x27;</span>: LR&#125;], momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    optimizer = optim.SGD(alexnet_model.parameters(), lr=LR, momentum=<span class="number">0.9</span>)  <span class="comment"># 选择优化器</span></span><br><span class="line"></span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=<span class="number">0.1</span>)  <span class="comment"># 设置学习率下降策略</span></span><br><span class="line"><span class="comment"># scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(patience=5)</span></span><br></pre></td></tr></table></figure><p>对模型模块进行设置，加载模型，修改它的输出层，两个类的一个分类。</p><p>设置损失函数和优化器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"># ============================ step 5/5 训练 ============================</span><br><span class="line">    train_curve = list()</span><br><span class="line">    valid_curve = list()</span><br><span class="line"></span><br><span class="line">    for epoch in range(start_epoch + 1, MAX_EPOCH):</span><br><span class="line"></span><br><span class="line">        loss_mean = 0.</span><br><span class="line">        correct = 0.</span><br><span class="line">        total = 0.</span><br><span class="line"></span><br><span class="line">        alexnet_model.train()</span><br><span class="line">        for i, data in enumerate(train_loader):</span><br><span class="line"></span><br><span class="line">            # if i &gt; 1:</span><br><span class="line">            #     break</span><br><span class="line"></span><br><span class="line">            # forward</span><br><span class="line">            inputs, labels = data</span><br><span class="line">            inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">            outputs = alexnet_model(inputs)</span><br><span class="line"></span><br><span class="line">            # backward</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            # update weights</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            # 统计分类情况</span><br><span class="line">            _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">            total += labels.size(0)</span><br><span class="line">            correct += (predicted == labels).squeeze().cpu().sum().numpy()</span><br><span class="line"></span><br><span class="line">            # 打印训练信息</span><br><span class="line">            loss_mean += loss.item()</span><br><span class="line">            train_curve.append(loss.item())</span><br><span class="line">            if (i+1) % log_interval == 0:</span><br><span class="line">                loss_mean = loss_mean / log_interval</span><br><span class="line">                print(&quot;Training:Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Loss: &#123;:.4f&#125; Acc:&#123;:.2%&#125;&quot;.format(</span><br><span class="line">                    epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total))</span><br><span class="line">                loss_mean = 0.</span><br><span class="line"></span><br><span class="line">        scheduler.step()  # 更新学习率</span><br><span class="line"></span><br><span class="line">        # validate the model</span><br><span class="line">        if (epoch+1) % val_interval == 0:</span><br><span class="line"></span><br><span class="line">            correct_val = 0.</span><br><span class="line">            total_val = 0.</span><br><span class="line">            loss_val = 0.</span><br><span class="line">            alexnet_model.eval()</span><br><span class="line">            with torch.no_grad():</span><br><span class="line">                for j, data in enumerate(valid_loader):</span><br><span class="line">                    inputs, labels = data</span><br><span class="line">                    inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line"></span><br><span class="line">                    bs, ncrops, c, h, w = inputs.size()     # [4, 10, 3, 224, 224</span><br><span class="line">                    outputs = alexnet_model(inputs.view(-1, c, h, w))</span><br><span class="line">                    outputs_avg = outputs.view(bs, ncrops, -1).mean(1)</span><br><span class="line"></span><br><span class="line">                    loss = criterion(outputs_avg, labels)</span><br><span class="line"></span><br><span class="line">                    _, predicted = torch.max(outputs_avg.data, 1)</span><br><span class="line">                    total_val += labels.size(0)</span><br><span class="line">                    correct_val += (predicted == labels).squeeze().cpu().sum().numpy()</span><br><span class="line"></span><br><span class="line">                    loss_val += loss.item()</span><br><span class="line"></span><br><span class="line">                loss_val_mean = loss_val/len(valid_loader)</span><br><span class="line">                valid_curve.append(loss_val_mean)</span><br><span class="line">                print(&quot;Valid:\t Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Loss: &#123;:.4f&#125; Acc:&#123;:.2%&#125;&quot;.format(</span><br><span class="line">                    epoch, MAX_EPOCH, j+1, len(valid_loader), loss_val_mean, correct_val / total_val))</span><br><span class="line">            alexnet_model.train()</span><br><span class="line"></span><br><span class="line">    train_x = range(len(train_curve))</span><br><span class="line">    train_y = train_curve</span><br><span class="line"></span><br><span class="line">    train_iters = len(train_loader)</span><br><span class="line">    valid_x = np.arange(1, len(valid_curve)+1) * train_iters*val_interval # 由于valid中记录的是epochloss，需要对记录点进行转换到iterations</span><br><span class="line">    valid_y = valid_curve</span><br><span class="line"></span><br><span class="line">    plt.plot(train_x, train_y, label=&#x27;Train&#x27;)</span><br><span class="line">    plt.plot(valid_x, valid_y, label=&#x27;Valid&#x27;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&#x27;upper right&#x27;)</span><br><span class="line">    plt.ylabel(&#x27;loss value&#x27;)</span><br><span class="line">    plt.xlabel(&#x27;Iteration&#x27;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><p>只设置了3个epoch（0,1,2），差不多10分钟左右</p><p><a href="https://imgtu.com/i/fuHwT0"><img src="https://z3.ax1x.com/2021/08/07/fuHwT0.png" alt="fuHwT0.png"></a></p><p>训练验证曲线</p><p><a href="https://imgtu.com/i/fuHdwq"><img src="https://z3.ax1x.com/2021/08/07/fuHdwq.png" alt="fuHdwq.png"></a></p><p>完。</p><h1 id="AlexNet应用于其他数据集"><a href="#AlexNet应用于其他数据集" class="headerlink" title="AlexNet应用于其他数据集"></a>AlexNet应用于其他数据集</h1><h2 id="花分类与蚂蚁蜜蜂的分类"><a href="#花分类与蚂蚁蜜蜂的分类" class="headerlink" title="花分类与蚂蚁蜜蜂的分类"></a>花分类与蚂蚁蜜蜂的分类</h2><p>windows下<code>nvidia-smi.exe -l 5</code>(每5秒更新一次gpu的信息，更新频率最高可设置为1s/次，ctrl+c停止更新)</p><h3 id="花分类"><a href="#花分类" class="headerlink" title="花分类"></a>花分类</h3><p>训练结果</p><p><a href="https://imgtu.com/i/fGcQVx"><img src="https://z3.ax1x.com/2021/08/10/fGcQVx.png" alt="fGcQVx.png"></a></p><p>预测结果</p><p><a href="https://imgtu.com/i/fGglOs"><img src="https://z3.ax1x.com/2021/08/10/fGglOs.png" alt="fGglOs.png"></a></p><h3 id="蚂蚁与蜜蜂分类"><a href="#蚂蚁与蜜蜂分类" class="headerlink" title="蚂蚁与蜜蜂分类"></a>蚂蚁与蜜蜂分类</h3><p><a href="https://imgtu.com/i/fG2PhT"><img src="https://z3.ax1x.com/2021/08/10/fG2PhT.png" alt="fG2PhT.png"></a></p><p>预测结果</p><p><a href="https://imgtu.com/i/fG2cbn"><img src="https://z3.ax1x.com/2021/08/10/fG2cbn.png" alt="fG2cbn.png"></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="神经网络模型" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>markdown基本语法-Typora</title>
    <link href="http://example.com/2021/08/03/markdown%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95-Typora/"/>
    <id>http://example.com/2021/08/03/markdown%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95-Typora/</id>
    <published>2021-08-03T06:17:11.000Z</published>
    <updated>2021-08-04T01:10:54.934Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Markdown段落和换行"><a href="#Markdown段落和换行" class="headerlink" title="Markdown段落和换行"></a>Markdown段落和换行</h1><ul><li><p>实例如下</p><p>&emsp;&emsp;我是首句</p><p>&ensp;&ensp;我是中间</p><p>我是结尾</p></li></ul><h2 id="换行"><a href="#换行" class="headerlink" title="换行"></a>换行</h2><p>​        Typora 一直是我认为桌面端笔记应用应有的终极形态。用我之前 <a href="https://sspai.com/post/54122">一篇文章</a> 中的话来说就是，它的功能之强大、设计之冷静、体验之美妙、理念之先进，我认为值得所有笔记应用厂商学习。<br>&emsp;&emsp;  但一件很尴尬的事情是，由于它极简的设计理念，有许多使用者并没能完全地了解到 Typora 的全部强大功能。我想在这篇文章中由浅入深地介绍 Typora 的功能亮点。无论你从未用过 Typora，还是已经体验了很久，我相信你都能在这篇文章中发现 Typora 新的惊喜。</p><h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><p>错误写法</p><p>=======This is an H1 ===============================</p><p>——This is an H2 ———-</p><p>正确写法</p><h1 id="我是主题"><a href="#我是主题" class="headerlink" title="我是主题"></a>我是主题</h1><h2 id="我是副主题"><a href="#我是副主题" class="headerlink" title="我是副主题"></a>我是副主题</h2><p>这是一种类Setex的写法</p><p>下面是类atx的写法</p><p>常用：1到6个# <br></p><p>不写了</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><blockquote><p>&emsp;&emsp;Markdown 是用来编写结构化文档的一种纯文本格式，它使我们在双手不离开键盘的情况下，可以对文本进行一定程度的格式排版。你可以在 <a href="https://sspai.com/post/36610">这篇文章</a> 中快速入门 Markdown。</p><h2 id="我是里面的标题"><a href="#我是里面的标题" class="headerlink" title="我是里面的标题"></a>我是里面的标题</h2><blockquote><blockquote><blockquote><p>每加一个&gt;  的效果</p></blockquote></blockquote></blockquote><p>&emsp;&emsp;由于目前还没有一个权威机构对 Markdown 的语法进行规范，各应用厂商制作时遵循的 Markdown 语法也是不尽相同的。其中比较受到认可的是 <a href="https://github.github.com/gfm/">GFM 标准</a>，它是由著名代码托管网站 <a href="https://github.com/">GitHub</a> 所制定的。Typora 主要使用的也是 GFM 标准。同时，你还可以在 <code>文件 - 偏好设置 - Markdown 语法偏好 - 严格模式</code> 中将标准设置为「更严格地遵循 GFM 标准」。具体内容你可以在官方的 <a href="http://support.typora.io/Strict-Mode/">这篇文档</a> 中查看。</p></blockquote><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><h3 id="无序展示"><a href="#无序展示" class="headerlink" title="无序展示"></a>无序展示</h3><p>注意*、+、-后面加空格</p><ul><li>语文 </li><li>数学</li><li>英语</li></ul><ul><li>物理</li><li>化学</li><li>生物</li></ul><ul><li>历史</li><li>地理</li><li>政治</li></ul><h3 id="有序展示"><a href="#有序展示" class="headerlink" title="有序展示"></a>有序展示</h3><ol><li>努力</li><li>学习</li><li>运用<h1 id=""><a href="#" class="headerlink" title=""></a></h1></li><li>你好</li><li>你是谁</li><li>你是哪里人</li></ol><h2 id="代码区块"><a href="#代码区块" class="headerlink" title="代码区块"></a>代码区块</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartTimeEmployee</span> <span class="keyword">extends</span> <span class="title">Employee</span> </span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">double</span> hourlyPay;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">PartTimeEmployee</span><span class="params">(String na,String nu,<span class="keyword">double</span> h)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">super</span>(na,nu);</span><br><span class="line">hourlyPay=h;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHourlyPay</span><span class="params">(<span class="keyword">double</span> pay)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">hourlyPay=pay;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">calculateWeeklyPay</span><span class="params">(<span class="keyword">int</span> hour)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> hourlyPay*hour;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是一个代码块</span><br></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">    <span class="keyword">var</span> option=<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">        <span class="comment">//调用后台请求</span></span><br><span class="line">        <span class="comment">//回显结果</span></span><br><span class="line">    &#125;</span><br><span class="line">&lt;script&gt;</span><br></pre></td></tr></table></figure><h2 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h2><p>分割线之前</p><hr><hr><hr><p>+++</p><p>分割线之后</p><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><h3 id="行内式"><a href="#行内式" class="headerlink" title="行内式"></a>行内式</h3><p>这是一个<a href="http://baidu.com/" title="百度">例子</a> 放上去有提示。</p><p><a href="http://zhangwei19980131.github.io/">zw</a></p><h3 id="参考式"><a href="#参考式" class="headerlink" title="参考式"></a>参考式</h3><p>这是一个<a href="http://baidu.com/" title="百度">示例</a></p><h2 id="强调"><a href="#强调" class="headerlink" title="强调"></a>强调</h2><p><em>全体注意</em></p><p><em>全体注意</em></p><p><strong>全体注意</strong></p><p>==全体注意==</p><h2 id="一句代码块"><a href="#一句代码块" class="headerlink" title="一句代码块"></a>一句代码块</h2><p>Java中输出语句<code> System.out.println(m)</code></p><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><p>略</p><p>也是两种格式：行内式与参考式</p><h2 id="反斜杠"><a href="#反斜杠" class="headerlink" title="反斜杠"></a>反斜杠</h2><p><strong>全体注意</strong></p><p>*全体注意*</p><p>\</p><p>!</p><p>\</p><h2 id="自动链接转换"><a href="#自动链接转换" class="headerlink" title="自动链接转换"></a>自动链接转换</h2><p><a href="http://baidu.com/">http://baidu.com</a></p><p><a href="mailto:&#x31;&#51;&#x32;&#48;&#53;&#51;&#x39;&#50;&#x31;&#x31;&#64;&#x71;&#113;&#46;&#99;&#x6f;&#x6d;">&#x31;&#51;&#x32;&#48;&#53;&#51;&#x39;&#50;&#x31;&#x31;&#64;&#x71;&#113;&#46;&#99;&#x6f;&#x6d;</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="Markdown" scheme="http://example.com/categories/Markdown/"/>
    
    
  </entry>
  
  <entry>
    <title>VGGNet</title>
    <link href="http://example.com/2021/08/02/VGGNet/"/>
    <id>http://example.com/2021/08/02/VGGNet/</id>
    <published>2021-08-02T08:04:14.000Z</published>
    <updated>2021-08-06T16:41:57.903Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="第一张ppt"><a href="#第一张ppt" class="headerlink" title="第一张ppt"></a>第一张ppt</h2><p><a href="https://imgtu.com/i/fuHO0I"><img src="https://z3.ax1x.com/2021/08/07/fuHO0I.jpg" alt="fuHO0I.jpg"></a></p><p>首先介绍一下VGG是什么…</p><p>论文题目：应用于大规模图像识别的非常深的卷积网络。</p><p>VGGNet最主要的目标是试图回答“如何设计网络结构”的问题。随着AlexNet提出，很多人开始利用卷积神经网络来解决图像识别的问题。一般的做法都是重复几层卷积网络，每个卷积网络之后接一些池化层，最后再加上几个全连接层。而VGGNet的提出，给这些结构设计带来了一些标准参考。</p><p>虽然VGGNet是比较老，14年提出，但是现在还在用，比如说SSD 使用单个深度神经网络来检测图像中的目标的方法，是目标检测中应用最广泛的算法，前端使用VGG16网络，</p><p>所以我们需要了解一下。</p><p><a href="https://blog.csdn.net/u013044310/article/details/89380273">https://blog.csdn.net/u013044310/article/details/89380273</a></p><p>贡献：</p><h2 id="第二张ppt"><a href="#第二张ppt" class="headerlink" title="第二张ppt"></a>第二张ppt</h2><p><a href="https://imgtu.com/i/fuHLnA"><img src="https://z3.ax1x.com/2021/08/07/fuHLnA.jpg" alt="fuHLnA.jpg"></a></p><p>论文中，作者的主要目标是探索深度学习深度的影响。作者固定了网络中其他的参数，通过缓慢的增加网络的深度来探索网络的效果。这个表格里给了我们6个VGG网络的配置，</p><p>有11层、13层、16层和19层、还有这里是否是用LRN，这种规范化并不能提高在ILSVRC数据集上的性能，我们常用的是VGG16、16是怎么来的，13个卷积层和3个全连接层。我们观察表格，A和A-LRN都是11层，这个使用了LRN，C和D的区别一个卷积核是1<em>1，一个是3</em>3。</p><p>VGGNet最重要的贡献证明了分类任务可以通过使用小的卷积核增加CNN的深度来提高精度。</p><h2 id="第三张ppt"><a href="#第三张ppt" class="headerlink" title="第三张ppt"></a>第三张ppt</h2><p><a href="https://imgtu.com/i/fuHvAP"><img src="https://z3.ax1x.com/2021/08/07/fuHvAP.jpg" alt="fuHvAP.jpg"></a></p><p>这一部分主要是讲如何通过堆叠两个3×3的卷积核来替代大尺度的卷积核，先要了解一个感受野的感念。</p><p>通俗的解释是，输出feature map上的一个单元对应输入层上的区域大小。我们来看这样一个例子，在我们左边，最下面呢是一个9<em>9</em>1的一个特征矩阵，首先我们通过一个卷积层1、它的大小是3<em>3，步长为2，根据公式我们会得到1个4</em>4<em>1的这么一个大小，然后在经过一个最大池化下采样的一个操作，它所得到的特征层大小是2</em>2<em>1。所以在我们第三层中一个单元对应到我们第二层的感受野就是一个2</em>2的一个区域，对应到我们原图的感受野就是一个5<em>5的区域，那这个2</em>2与5*5怎么计算呢</p><p><a href="https://imgtu.com/i/fuHX7t"><img src="https://z3.ax1x.com/2021/08/07/fuHX7t.jpg" alt="fuHX7t.jpg"></a></p><p>我们把第三层的一个单元带入公式进行计算，感受野是从后往前推算的，所以说我们第三层的一个单元在原图当中有5*5这样一个感受野</p><p><a href="https://imgtu.com/i/fuHzh8"><img src="https://z3.ax1x.com/2021/08/07/fuHzh8.jpg" alt="fuHzh8.jpg"></a></p><p>VGG网络中Stride默认是等于1的。我们假设现在一个特征矩阵经过3层3<em>3的卷积得到一个feature map，那么feature map上的一个单元所对应我们上一层的感受野，那就是1-1</em>1+3等于3，我们在计算上一层的3-1<em>1+3 =5，我们在计算上一层5-1</em>1+3=7。也就是说我们经过3层3<em>3的卷积得到的特征图上的一个单元对应的感受野就是7</em>7的大小，那也就和我们采用一个7*7的一个卷积核是一样的。</p><p>那么我们这样做的目的是就是为了减少参数、这两个C分别的代表卷积核的深度以及个数</p><p>很明显我们的参数减少了很多。</p><h2 id="第四张ppt"><a href="#第四张ppt" class="headerlink" title="第四张ppt"></a>第四张ppt</h2><p><a href="https://imgtu.com/i/fubp9S"><img src="https://z3.ax1x.com/2021/08/07/fubp9S.jpg" alt="fubp9S.jpg"></a></p><p>我们通常使用D这个模型</p><p>首先呢，我们的输入图像是224<em>224</em>3这样一个RGB图像，然后我们通过两层3*3的一个卷积层，然后maxpool最大下采样……然后在经过3个FC层，然后在经过一个softmax处理，就得到我们的一个概率分布了。那么我们这个表中并没有给出参数计算，所以这里简单介绍一下、卷积层stride=1、padding=1、我们带入公式、-Fsize 也就是-3 +2P +2 /1不用考虑也就是-1 再加1 化简也就是 卷积层中outsize = input size 也就是经过卷积输出尺寸和输入尺寸是一样的。池化我们代入计算，-2 + 这里P=0，然后除以2 +1 约掉之后相当于除以2。  结果是输出的特征图宽和高缩减为原来的一半。</p><p>这样我们来看这幅图 黑色框…红色框</p><p>第一个卷积操作，我们进行了两个3<em>3的卷积层之后呢，它所得到的特征图大小是224</em>224，因为我们之前说了根据公式，outsize=inputsize，我们采用的卷积核的个数是64，那么我们输出的深度也就是64，然后是maxpool最大下采样操作，之前说过根据公式、相当于缩减一半、得到了112<em>112，池化是不改变我们的深度的，我们注意这里红色框的厚度和前面卷积层的厚度一样，所以我们得到的是112</em>112<em>64、图上标的这个112</em>112*120是下一个卷积操作的卷积层的具体值，紧接着下一个卷积操作。</p><p>最后我们需要注意的是最后一个全连接层是不需要加上ReLU激活函数的，我们需要加softmax激活函数将我们的预测结果转化为一个概率分布。</p><h2 id="第五张ppt"><a href="#第五张ppt" class="headerlink" title="第五张ppt"></a>第五张ppt</h2><p><a href="https://imgtu.com/i/fub91g"><img src="https://z3.ax1x.com/2021/08/07/fub91g.jpg" alt="fub91g.jpg"></a></p><p><a href="https://imgtu.com/i/fubCcQ"><img src="https://z3.ax1x.com/2021/08/07/fubCcQ.jpg" alt="fubCcQ.jpg"></a></p><p>​       德国有一个激光切割领域的博士，他把VGG做了一个改进，他用最后的全连接输出直接回归了所有参数。就像我们把激光切割的一张图片喂到模型里面，它会预测出你用的是什么样的激光切割的工艺量。比如说焦点深度，进给量，保护气的气压等等。而目前的话人类是做不到这一点。</p><p>​        他改进的VGG模型，预测出激光切割的一个工艺参数，并且远远超过了人类专家的水平。</p><p>​        第二个贡献是：用已有的可视化的方法，类似于我们热力图的方法，标记出了预测值贡献最大的区域，他向专家和工人们展示了这样的结果，提高了专家和工人们预测参数的能力，这就不仅仅是人教机器学习了，不是machine learning 而是machine teaching</p><p>​        所以总结来说他就是用了自己领域的数据集，改动了一下VGG，训练了一个远强于人类专家的网络，并且还让这个网络教会了人提高自己的技能。</p><p>​        我们看到这个人写了很多激光加工领域的论文，因为做激光加工领域的人不懂计算机视觉，而真正内卷计算机视觉的人他不懂激光切割。所以当我们学会了这个技能是可以借助这种信息不对称，用计算机视觉和AI赋能到我们自己的行业，这是我觉得目前AI领域所能为社会创造价值的途径，如果我们只是局限于改改trick，调参，炼丹，为了kaggle上的一个竞赛去刷榜，比别人高个一些精确度而努力，这些呢我觉得对于我来说可能也是一个需要经历的过程，但意义不大。而我们把这些技术应用于自己的垂直领域，垂直行业，甚至说之前看上去跟CV或者是计算机无关的领域。</p><p>​         现在CV发展非常的快，这些算法也大大降低了AI开发的水平面，这个水平面降下来以后呢很多领域都可以使用人工智能来达到该领域最优的性能，这也是摆脱内卷的一个途径。现在人工智能各种开发的设施都已完备，数据集，算力，算法都已完备了，缺的就是能够为各行各业有实际落地应用价值的人才。</p><h2 id="软件开发领域的人工智能"><a href="#软件开发领域的人工智能" class="headerlink" title="软件开发领域的人工智能"></a>软件开发领域的人工智能</h2><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><p>​        作为软件生命周期（Software Development Life Cycle，即SDLC）中的一个概念梳理环节，收集客户需求很大程度上依赖人工介入。</p><p>​        对此，AI技术提供了一些技术工具，譬如的Google ML Kit和Infosys Nia，它们可以通过AI使某些过程实现自动化，从而在一定程度上将人为干预最小化。</p><p>​        在这个阶段中可以将设计前的漏洞检测纳入工作流程。一种称为“自然语言处理”（Natural Language Processing）的AI技术通过让机器理解自然语言去了解用户需求，并产出高级软件模型，当然，这种方法存在一些问题，包括难以在已开发的系统间作出平衡，但它仍旧是当今的热门研究主题之一。</p><p>​        项目的规划和设计需要依靠大量的经验积累来提出一个明确的解决方案。</p><p>​        设计工作每个阶段都有一个“共同认可”的结果，其实这对设计师来说是非常困难的，现实是设计师通常需要多次来回修改方案，直到客户满意为止。如果借助AI自动执行一些复杂的程序来满足客户需求，以此方法来产出方案，将会大大提高效率。</p><h3 id="代码自动生成"><a href="#代码自动生成" class="headerlink" title="代码自动生成"></a>代码自动生成</h3><p>​        在软件开发启动前，我们通常要去理解一个无比巨大的项目的实际目标以及需求方的各种细碎想法，这往往非常耗时，其间的大量沟通可以视作一种“重复性工作”。</p><p>​        为了节省时间和金钱，专家们提出了一种在开始开发启动前先编写部分代码的解决方案，不过，这种方法存在很多不确定性，譬如，该前置编写代码阶段也需要拿到一些信息，如果此时信息仍不清晰，这样的前置工作是无效的。</p><p>​        这时通过人工智能介入信息收集，即，通过自然语言把思路告知智能系统，这样的前置风险就会被大大降低，因为智能系统效率高于人工，同时不会去“抱怨”客观条件下的小小弯路。</p><h3 id="测试服务中的AI"><a href="#测试服务中的AI" class="headerlink" title="测试服务中的AI"></a>测试服务中的AI</h3><p>​        软件测试是软件开发中的关键阶段，一定程度上可保证软件产品的质量，但如果每次更改源代码时都需要重复测试，既费时又费钱。</p><p>​        AI又可以在这个阶段起到作用——目前已有部分工具可使用AI创建测试用例并执行回归测试，这些AI工具可以自动化测试，并进一步确保测试无错误。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="神经网络模型" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet</title>
    <link href="http://example.com/2021/08/02/AlexNet/"/>
    <id>http://example.com/2021/08/02/AlexNet/</id>
    <published>2021-08-02T07:29:55.000Z</published>
    <updated>2021-08-06T16:37:46.867Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="内容以PPT形式展示"><a href="#内容以PPT形式展示" class="headerlink" title="内容以PPT形式展示"></a>内容以PPT形式展示</h1><h2 id="第一张ppt"><a href="#第一张ppt" class="headerlink" title="第一张ppt"></a>第一张ppt</h2><p><a href="https://imgtu.com/i/fuHSy9"><img src="https://z3.ax1x.com/2021/08/07/fuHSy9.png" alt="fuHSy9.png"></a></p><p>论文题目是基于深度卷积神经网络的图像网络分类</p><p>首先说明ImageNet和ILSVRC-2012是不同的两个概念，ImageNet项目是一个用于视觉对象识别软件研究的大型可视化数据库。有14,197,122 张图片, 21841 多个类别。</p><p>ILSVRC-2012是一个大型的挑战赛，从ImageNet中挑选1000类的1200000张作为训练集。</p><h2 id="第二张ppt"><a href="#第二张ppt" class="headerlink" title="第二张ppt"></a>第二张ppt</h2><p><a href="https://imgtu.com/i/fuHio6"><img src="https://z3.ax1x.com/2021/08/07/fuHio6.png" alt="fuHio6.png"></a></p><p>ILSVRC有一个分类任务，对于每张图像，算法将按置信度的降序生成最多 5 个对象类别的列表。将根据与图像的真实标签最匹配的标签来评估标签的质量。</p><p>这5个类别就是我们计算TOP-5error的数据</p><p>为什么：</p><p>这个想法是允许算法识别图像中的多个对象，并且如果识别出的对象之一确实存在，但不包含在基本事实中，则不会受到惩罚。</p><p>如果这5个类别我们有一个分类正确了，就不会惩罚我们的模型，继续优化。</p><h2 id="第三张ppt"><a href="#第三张ppt" class="headerlink" title="第三张ppt"></a>第三张ppt</h2><p><a href="https://imgtu.com/i/fuHCe1"><img src="https://z3.ax1x.com/2021/08/07/fuHCe1.png" alt="fuHCe1.png"></a></p><p>1CNN︰训练一个AlexNet</p><p>5CNNs:训练五个AlexNet取平均值</p><p>1CNN*在最后一个池化层之后，额外添加第六个卷积层,并使用lmageNet 2011(秋）数据集上预训练</p><p>7CNNs*两个预训练模型微调，与5CNNs取平均值</p><h2 id="第四张ppt"><a href="#第四张ppt" class="headerlink" title="第四张ppt"></a>第四张ppt</h2><p><a href="https://imgtu.com/i/fuHpLR"><img src="https://z3.ax1x.com/2021/08/07/fuHpLR.png" alt="fuHpLR.png"></a></p><p>包含5个卷积层（某些卷积层后面带有池化层）和3个全连接层，网络的输入为150,528，224×224×3</p><p>网络剩余层中的神经元数量由253,440–186,624–64,896–64,896–43,264–4096–4096–1000.</p><p><a href="https://imgtu.com/i/fuHPdx"><img src="https://z3.ax1x.com/2021/08/07/fuHPdx.png" alt="fuHPdx.png"></a></p><p>从图片当中，从原始的RGB，224 × 224，经过一个卷积，得到了一个55×55的特征图，经历一个Relu，经历池化，然后就变成了后面这个27，并且加有一个LRN这个操作</p><p><a href="https://imgtu.com/i/fuHkFK"><img src="https://z3.ax1x.com/2021/08/07/fuHkFK.png" alt="fuHkFK.png"></a></p><p>公式含义：输入特征图的大小 – kernel size(卷积核的大小) + 2p p是padding 也就是像素，有多少个pixel。除以stride 也就是步长</p><p>这里227与上面图的224是有所不同的，其实在早期的Alexnet网络结构图当中，它就是227，为什么227这里经过卷积是55，论文当中224卷积也是55呢。我们算一下，（227-11）/4=55 。而224 计算的话 224-11 213 除以4 是53余1 向下取整再加1是54，在pytorch中，实现卷积是向下取整的。加两个像素，224 -11 +2×2 / 4 在加1</p><p>​    然后我们得到了55×55，那有多少通道呢，就是有多少个卷积核就有多少个通道，也可以用公式这样算，K这里是池化窗的大小，算完经过第一个池化，特征图变为了27×27，通道数不变，再往下也是重复的卷积池化卷积池化，在全连接层前面画一条线，通常我们也会在这里切一刀，认为前面的这一系列操作就是对原始图像的特征提取，最后的这个就是特征，进入全连接层认为这是分类。</p><h2 id="第五张ppt"><a href="#第五张ppt" class="headerlink" title="第五张ppt"></a>第五张ppt</h2><p><a href="https://imgtu.com/i/fuHEWD"><img src="https://z3.ax1x.com/2021/08/07/fuHEWD.png" alt="fuHEWD.png"></a></p><p>第一列：名称 第二列 卷积核的数量 第三列padding 第四列：卷积核大小</p><p>第五列：步长 第六列：img size我们之前算的特征图的高和宽</p><p>​    公式解释：Fi 我们输入图片的通道数 Ks kernel size 卷积核的大小 Kn 卷积核的数量</p><p>也就是输出的通道数+ 偏置</p><p>​    我们可以发现总共有6000多万个参数，第一个FC层（全连接层）3700多万，大于百分之50，这也就是我们后面的神经网络不太爱使用这个FC层，因为实在是太占我们的内存了，占我们的参数。</p><h2 id="第六张ppt"><a href="#第六张ppt" class="headerlink" title="第六张ppt"></a>第六张ppt</h2><p>[<img src="https://z3.ax1x.com/2021/08/07/fuHAJO.png" alt="fuHAJO.png"></p><p>这个图意思是他们做了一个实验去比较使用relu和使用tanh当达到0.25这个error的时候的一个速度比较，relu可以加快神经网络的训练，从这个图上可以看出差不多6倍左右。</p><p><a href="https://imgtu.com/i/fuHZSe"><img src="https://z3.ax1x.com/2021/08/07/fuHZSe.png" alt="fuHZSe.png"></a></p><p>受到真实的神经元侧抑制的启发。</p><p>这个抑制信号我们需要注意一下，这个公式想实现的功能就是抑制。</p><p>等号左边就是经过LRN之后每个神经元的激活值，分子是原始神经元的一个激活值。所以呢我们可以看到原始神经元a，经过LRN后呢是神经元b。i代表当前的通道channel，x，y代表像素的位置。我们想如果分母越大，那么b就会越小，这样就反映出对神经元一个抑制的效果，k是一个超参数，常数，α和β都是常数，由原型中的alpha和belta指定、论文也有介绍，我们不关心这个，我们来看这个Σ，这个Σ值越大，抑制效果就越强。</p><p>a代表的是feature map里面的i对应像素的具体值，j是平方累加索引，我们来关注j的取值，j=max(0,i-n/2)，它是从0开始的，为了防止它超出别界，0，N-1是为了防止计算超过这个特征图，我们来看这个i-二分之n和i+二分之n，在图上i-二分之n代表往左考虑二分之n个通道，i+二分之n代表往右考虑二分之n个通道，也就是表示它有一个范围，这个范围就表示对应的我们真实神经元这个周围，这个周围是通过n来体现的。</p><p><a href="https://imgtu.com/i/fuHH6H"><img src="https://z3.ax1x.com/2021/08/07/fuHH6H.md.png" alt="fuHH6H.md.png"></a></p><p>第四个神经元的值非常大，我们假如说是100，第5个是5，那么我们第三个神经元计算的时候，我们看公式，这个分母就会被这个100所控制了，也就是被抑制了，由于这个神经元的值非常大，它就抑制了周围的神经元的输出。</p><p>我们再来看这个公式，它要看分母，分母当中它要考虑周围的一些神经元，那这些神经元当中如果有很大的值，他就会抑制，这就是由侧抑制启发得来的一个公式。</p><h2 id="第七张ppt"><a href="#第七张ppt" class="headerlink" title="第七张ppt"></a>第七张ppt</h2><p><a href="https://imgtu.com/i/fuHmyd"><img src="https://z3.ax1x.com/2021/08/07/fuHmyd.png" alt="fuHmyd.png"></a></p><p>我们常常会遇到数据不足的情况。比如，你遇到的一个任务，目前只有小几百的数据，然而，你知道目前现在流行的最先进的神经网络都是成千上万的图片数据。</p><p><a href="https://imgtu.com/i/fuHeQH"><img src="https://z3.ax1x.com/2021/08/07/fuHeQH.png" alt="fuHeQH.png"></a></p><p>Hinton说他的灵感之一来自于银行的防欺诈机制。用他自己的话来说：“我去银行办理业务。柜员不停地换人，于是我问其中一人这是为什么。他说他不知道，但他们经常换来换去。我猜想，银行工作人员要想成功欺诈银行，他们之间要互相合作才行。这让我意识到，在每个样本中随机删除不同的部分神经元，可以阻止它们的阴谋，因此可以降低过拟合。” 其核心思想是在层的输出值中引入噪声，打破不显著的偶然模式（Hinton 称之为阴谋）。如果没有噪声的话，网络将会记住这些偶然模式。</p><h2 id="第八张ppt"><a href="#第八张ppt" class="headerlink" title="第八张ppt"></a>第八张ppt</h2><p><a href="https://imgtu.com/i/fuHKeI"><img src="https://z3.ax1x.com/2021/08/07/fuHKeI.png" alt="fuHKeI.png"></a></p><p>上面是48个下面是48个，上面是GPU1，下面是GPU2,上面是不同的方向，不同的频率，下面呈现的是不同的色彩，更多的是颜色块，而且上下基本上是不重叠的，不同的GPU有不同的选择，这有助于我们理解卷积核学习了什么内容，我们一提到神经网络，通常会想到这是一个黑盒子，Alexnet一共有5个卷积层，为什么第一个这个卷积层能够可视化呢？</p><p>两个原因：第一个是kernels size的原因 11×11 比较大，它可视化出来还能看出一些内容，121个像素，后面的卷积核5*×5 3×3 3×3只有9个像素，我们几乎看不出什么模式的，这也就是为什么没有可视化后面的卷积核，可视化出来也分析不出什么东西</p><p>第二个原因是关于特征，因为我们知道卷积神经网络越往后，特征越高级，越抽象的，我们人类的眼睛很难去识别它，那么我们第一个卷积层是最底层的最低级的特征，最接近人类观察到原始图片上的一些信息的，也就是这些颜色色彩纹理边缘，符合特征由低级到高级这么一个过程。</p><h2 id="第九张ppt"><a href="#第九张ppt" class="headerlink" title="第九张ppt"></a>第九张ppt</h2><p><a href="https://imgtu.com/i/fuHnOA"><img src="https://z3.ax1x.com/2021/08/07/fuHnOA.png" alt="fuHnOA.png"></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="神经网络模型" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Spring</title>
    <link href="http://example.com/2021/04/06/Spring/"/>
    <id>http://example.com/2021/04/06/Spring/</id>
    <published>2021-04-06T04:01:30.000Z</published>
    <updated>2021-08-06T16:41:59.075Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Spring基础</p><p>spring全家桶：Spring springmvc  spring boot  spring cloud</p><p>spring: 解决企业开发的难度，减轻对项目模块之间的管理，类和类之间的管理，帮助开发人员创建对象，管理对象之间的关系</p><p>核心技术： IOC aop  实现模块之间，类之间的解耦合</p><p>依赖： class a中使用class b的属性或方法，叫做class a 依赖 class b</p><p>maven 管理的是项目中需要的jar包 管理模块 管理资源（项目之前）</p><p>Spring管理的是项目之中写的类，模块（项目之中）</p><p>(1) 轻量 运行占用的资源少，运行效率高，不依赖其它jar<br>(2) 针对接口编程，解耦合<br>(3) AOP编程的指出<br>(4) 方便集成各种优秀框架</p><p>容器：存的是java对象</p><p>mybatis– 访问数据库，对表中的数据执行增删改查</p><p>框架要完成一个功能，需要一定的步骤支持的。<br>框架内部怎么做，原理是什么<br>自定义框架</p><h1 id="IOC"><a href="#IOC" class="headerlink" title="IOC"></a>IOC</h1><p>是一种理论、概念、思想。 把对象的创建，赋值，管理工作都交给代码之外的容器实现，也就是对象的创建是由其他的外部资源来完成。</p><p>控制： 创建对象，对象的属性赋值，对象之间的关系管理<br>反转： 把原来的开发人员管理，创建对象的权限转移给代码之外的容器实现。由容器代替开发人员管理对象。创建对象，给属性赋值<br>正转： 由开发人员在代码中，使用new构造方法创建对象，开发人员主动管理对象。</p><p>Student student = new Student();//在代码中，创建对象  –正转</p><p>为什么使用IOC： 减少对代码的改动，也能实现不同的功能（实现解耦合）</p><p>java中创建对象有哪些方式：<br>1、构造方法 , new Student()<br>2、反射<br>3、序列化<br>4、克隆<br>5、ioc：容器创建对象<br>6、动态代理</p><p>IOC不需要在你的程序里写代码就可以获取对象</p><p>以往学过的内容中，IOC思想存在、体现？</p><p>servlet 1、创建类继承HttpServlet<br>        2、在web.xml 注册servlet, 使用<servlet-name>myservlet</servlet-name><br>        3、没有创建servlet对象，没有MyServlet myservlet = new MyServlet()<br>        4、Servlet 是Tomcat服务器它为你创建的</p><pre><code>    Tomcat也称为容器，里面存放的有servlet对象，Listener，Filter 等</code></pre><p>IOC的技术实现：<br>    DI 是IOC的技术实现，<br>    DI ：依赖注入 ， 我们只需要在程序中提供要使用的对象名称就可以，至于对象如何在容器中创建，赋值，查找都由容器内部实现。</p><p>spring是使用的di实现了IOC的功能，spring底层创建对象，使用的是反射机制</p><p>总结：spring是一个容器，管理对象，给属性赋值，底层是反射创建对象</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="JAVA" scheme="http://example.com/categories/JAVA/"/>
    
    
    <category term="Spring" scheme="http://example.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>机器学习第二章</title>
    <link href="http://example.com/2021/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0/"/>
    <id>http://example.com/2021/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E7%AB%A0/</id>
    <published>2021-04-05T15:35:30.000Z</published>
    <updated>2021-08-06T16:22:45.754Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>第二章<br>模型评估与选择</p><span id="more"></span><h1 id="一种训练集一种算法"><a href="#一种训练集一种算法" class="headerlink" title="一种训练集一种算法"></a>一种训练集一种算法</h1><h2 id="2-1经验误差与过拟合"><a href="#2-1经验误差与过拟合" class="headerlink" title="2.1经验误差与过拟合"></a>2.1经验误差与过拟合</h2><pre><code>m 样本数量 Y 样本正确的结果错误率 精度 误差</code></pre><h2 id="2-2评估方法"><a href="#2-2评估方法" class="headerlink" title="2.2评估方法"></a>2.2评估方法</h2><h3 id="训练集验证集与测试集"><a href="#训练集验证集与测试集" class="headerlink" title="训练集验证集与测试集"></a>训练集验证集与测试集</h3><p>测试集的保留方法：<br>留出法 37分 28分</p><p>交叉验证法 K折交叉验证<br><a href="https://imgtu.com/i/fu7qoV"><img src="https://z3.ax1x.com/2021/08/07/fu7qoV.png" alt="fu7qoV.png"></a><br>一个训练集 一个测试集 对应结果 然后取平均 来作为这个模型的衡量标准</p><p>自助法<br>  自助采样的定义：给定包含 m 个样本的数据集 D，我们对它进行采样产生的数据集 D’ ：每次随机从 D 中挑选一个样本，将其拷贝放入 D’ 中，然后再将该样本放回初始数据集 D 中，使得该样本在下次采样时仍有可能被采到；重复这个过程 m 次，我们就得到了包含 m 个样本的数据集 D’，这就是自助采样的结果。</p><p>在初始数据集D中， m 次采样中始终不被采到的概率是 36.8% ，将 D’ 作为训练集，将 D \ D’用作测试集。<br><a href="https://imgtu.com/i/fu7bd0"><img src="https://z3.ax1x.com/2021/08/07/fu7bd0.png" alt="fu7bd0.png"></a></p><p>优点：自助法在数据集较小、难以有效划分训练/测试集时很有用<br>缺点：由于产生的数据集改变了初始数据集的分布，会引入估计偏差，因此初始数据量足够时，留出法和交叉验证发更常用一些。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第二章&lt;br&gt;模型评估与选择&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习第一章</title>
    <link href="http://example.com/2021/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0/"/>
    <id>http://example.com/2021/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0/</id>
    <published>2021-04-05T15:25:24.000Z</published>
    <updated>2021-08-04T01:11:57.665Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>第一章</p><span id="more"></span><h1 id="1-1基本术语"><a href="#1-1基本术语" class="headerlink" title="1.1基本术语"></a>1.1基本术语</h1><p>有了数据 </p><p>通过某种学习算法 </p><p>得到模型 有监督（分类和回归）无监督（聚类）</p><p>进行预测 测试 测试样本 泛化能力（它能预测它没见过的数据的能力）</p><h1 id="1-2假设空间"><a href="#1-2假设空间" class="headerlink" title="1.2假设空间"></a>1.2假设空间</h1><p>科学的推理手段    归纳 演绎 …</p><h1 id="1-3归纳偏好"><a href="#1-3归纳偏好" class="headerlink" title="1.3归纳偏好"></a>1.3归纳偏好</h1><p>同一个数据集训练出了不同的模型，如何选择模型</p><p>原则  奥卡姆剃刀（选择最简单的那个）</p><h1 id="1-4发展历程"><a href="#1-4发展历程" class="headerlink" title="1.4发展历程"></a>1.4发展历程</h1><p>一种程序，有自我改善的能力，人为干预越少越好</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一章&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>调研报告</title>
    <link href="http://example.com/2020/10/31/%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/"/>
    <id>http://example.com/2020/10/31/%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/</id>
    <published>2020-10-31T15:46:31.000Z</published>
    <updated>2021-08-06T16:20:21.319Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>关于图像识别、CV、视频理解的调研报告</p><span id="more"></span><p>人工智能（即AI）是一个很大的体系，有远不止机器学习这个理论的理论体系，有远不止计算机视觉这个应用的应用领域。<br>如今依然是弱AI百花齐放的时代，一句话解释：阿尔法狗是不认识狗的。当强AI的时代来临，AI将变成人类这种通才，但是又可以兼具阿尔法狗的围棋能力，又兼具深蓝的博学多才。<br>我们可以把人工智能这个庞然大物看作一个超级生命，下面来看一下它的生存能力图：<br><a href="https://imgtu.com/i/fu7IMj"><img src="https://z3.ax1x.com/2021/08/07/fu7IMj.md.png" alt="fu7IMj.md.png"></a></p><p>Computer Vision（计算机视觉）和 Natural Language Processing （自然语言处理）一直是两个独立的研究方向。计算机视觉是一门研究如何使机器“看”的科学，而自然语言处理是人工智能和语言学领域的分支学科，主要探索的是如何使机器“读”和“写”的科学。它们相通的地方是，都需要用到很多机器学习，模式识别等技术，同时，它们也都受益于近几年的深度神经网络的进步，可以说这两个领域目前的 state-of-art，都是基于神经网络的，而且很多任务，比如 CV 里的物体识别检测，NLP 里的机器翻译，都已经达到了可以实用的程度。于是从 2015 年开始，有一个趋势就是将视觉与语言进行一定程度的结合，从而产生出一些新的应用与挑战。比如 image captioning，visual question answering（视觉问答） 等比较经典的 vision-and-language 任务。<br>在经典的 vision-language 任务上，比如 image captioning 和 VQA（Visual Question Answering），能够增长的空间已经很小，已经过了暴力的通过数据去学习的阶段。真正的挑战其实是一些细分的领域，比如多样性、可控性、推理以及如何将 vision-language 应用在真实的场景当中。<br>结合计算机视觉、机器人领域5大顶会（CVPR/ICCV/IROS/ICRA/ECCV），以及产业界的需求，现有3个当下热门及前沿的研究领域：<br>1、三维视觉<br>三维视觉是传统的研究领域，但最近5年内得到快速发展。三维视觉主要研究内容有：三维感知（点云获取及处理）、位姿估计（视觉SLAM）、三维重建（大规模场景的三维重建、动态三维重建）、三维理解（三维物体的识别、检测及分割等）。<br>2、视频理解<br>随着新型网络媒体的出现，以及5G时代的到来，视频呈现爆炸式增长，已成为移动互联网最主要的内容形式。面对于海量的视频信息，仅靠人工处理是无法完成的，因此实现视频的智能化理解则成为了亟待解决的问题。自2012年，深度学习在图像理解的问题上取得了较大的突破，但视觉理解比图像的目标检测识别要复杂的多。这是因为视频常有许多动作，动作往往是一个复杂概念的集合，可以是简单的行为，但也可能是带有复杂的情绪、意图。举个简单的例子，对一段视频分类，与对一幅图像分类，哪个更容易一些？从最近几年知名的计算机视觉竞赛，也可以看出，图像层面的竞赛在减少，视频层面的竞赛在增加。<br>3、多模态融合<br>多模态融合的知识获取是指从文本、图片、视频、音频等不同模态数据中交叉融合获取知识的过程。<br>随着计算机视觉越来越成熟之后，有一些计算机视觉解决不了的问题慢慢就会更多地依赖于多个传感器之间的相互保护和融合。</p><p>关于CV和实际工作需求方面的调研：<br>1、本人机器人&amp;无人驾驶研发从业两年，个人感觉这个CV行业供求关系是不平衡的。其次发文章不代表你的工程和产品能力好，不发文章也不能证明你的工程能力差。所以我的建议是在学习期间先把技能掌握好，比如CV的基本算法。<br>从基本的图像处理开始，二值化–光留—霍夫变换<br>识别，roi–各种算子<br>直到CNN网络                                          —-2018年<br>2、开始一直执迷于算法工作，其实我觉得我们这个做算法做好的都是一些创业型公司。 后来又担心去了公司做“调参侠”，最后的最后放弃了算法，改做研发了。我知道别人也有这样情况的，跟我想的差不多，最后去做了后台研发。这样的话不应该在这说，可能会给大家带不好的头【但其实我自己也不清楚去公司是不是去调参，我也只是看网上有人说，又看了某些人的微博动态罢了】 但是一直又放不下，感觉学的还不错，但是要全部丢掉了，舍不得。 最重要的是很多人说人工智能是未来的趋势，可能我们会被迷惑，也不能这样说，好像也确实是趋势，反正我是蛮纠结的，目前的定位不是算法。希望你也可以想清楚吧。                                            —2016年<br>3、面试造航母, 工作拧螺丝。<br>4、个人觉得人才并没有过剩，甚至还短缺。若干人以为学过cs231n等一些课，然后会import一些库或者git clone，就声称自己熟悉相关领域了。很多人对自己所做的任务也没有很好地调研，比如不知道目标检测应该分成one stage和two stage的方法，具体这些方法有什么异同，同类方法有什么不同之处。个人觉得还是应该静下心来，读点论文，至少知道领域上下几十年，然后写点代码，做点项目，成为一个有用的人吧。另外目前CV领域还有很多问题可以去解决，比如三维视觉，虽然这个领域还会牵扯到图形学了，但是毕竟这个世界还是三维的。还有比如visual reasoning，做一些带推理的VQA或者caption。再比如Person Re-ID、Activity Recognition等等。总之，人才大多情况下不会过剩，有些只是虚假繁荣吧，要想立足还是必须要有真才实学的，不然多读读书、写写代码。 —2019年<br>    5、<br>(1)先说最熟悉的CV， CV大公司和小公司差距非常大，基本上被旷视，云从，百度等垄断。他们不光垄断了市场，垄断了技术，甚至还垄断了高校，云从+南洋理工，旷视+西交，南大，港科大，上科大，百度更不用说，吴恩达等都曾在职过。巨头拥有的海量数据和资本支撑再配合上高校恐怖的科研能力(一个实验室一堆免费的研究生搬砖)。有几家公司能抗衡?更别提现在就连高校都快被抢完了，我曾在职的公司就勉强争取到了上海某985高校的合作。还有大量没钱没资源的公司根本拿不到合作。这些都导致了CV领域被巨头占领，普通公司生存环境越来越恶劣，大部分已经跑到工业领域降维打击去了。企业的情况基本就是研发岗位的现状，想进小厂，有个硕士文凭，做过视觉相关就能进，但是，要想搞出像样的算法，非大厂不可，而大厂的算法都有超级大牛们带头，应届生去也是搬砖，不过提升应该很快，搬几年砖空降某些原来做视频方面的厂搞算法，基本就是带头大哥。<br>(2)音频，我曾入职一家专门做音频和nlp的创业公司，大牛非常多，硕士，博士博后一大把。问题是没数据，没落地方案。国内专业做音频的也就讯飞和一堆xx精灵，xx同学，市场不大，暂时还在开拓应用场景，做的人也比较少。基本都要结合nlp做语音识别（感觉还不如搞nlp)<br>(3)Nlp，范围挺大，手机助手，xx精灵，xx搜索，xx翻译，知识图谱等等都要用。算是Al比较高逼格的领域，也是近期热门，顶会paper也很多，取得突破的话应该能使人工智能往前迈进一大步。喜欢研究的话可以深耕这个领域。<br>(4)推荐系统，这个方向是我现在选定的方向，就业面很广，kaggle上的比赛基本都能打，广告推荐，用户画像基本各个互联网公司都要用。门槛比较低，毕竟机器学习入门就是线性回归。<br>(5)金融，了解不多，据业内前辈说是一个比较独立的领域，金融互联网行业是现在最赚钱的行业，想来以后待遇不会差。        —2019年</p><p>关于个人想研究的方向（学习规划）：<br>    首先，个人看来要先学习的内容是：<br>课程方面：<br>1、数据分析<br>机器学习的主要前提是数据分析，数据分析是完成工作所需的第一项技能。<br>    2、Coursera（大型公开在线课程项目）上一门免费的机器学习课程<br>    一边跟着吴恩达的课，一边看李沐大佬的动手学深度学习。<br>    3、神经网络、卷积神经网络<br>卷积神经网络，也就是convolutional neural networks （简称CNN），现在已经被用来应用于各个领域，物体分割、风格转换、自动上色，但是，CNN真正能做的，只是起到一个特征提取器的作用！所以这些应用，都是建立在CNN对图像进行特征提取的基础上进行的。<br>    卷积神经网络 (LeCun 等, 1998) 作为深度学习的代表性网络，在图像识别、目标检测等方面取得了巨大的成功。卷积神经网络通过使用可训练的滤波器和特征池化操作的层次结构，自动学习视觉目标识别任务所需的复杂特征，从而实现比手工制作特征卓越的性能。直接将原始图像输入网络便可得到特征，传统算法往往需要复杂的特征设计以及统计策略，<br>而现在只需要设计网络结构和选择训练超参数。计算机视觉领域，研究深度学习已经成为这几年的主流。              –摘自《基于深度学习的视频识别研究》</p><p>第一学期打好基础，“小布慢跑”，学一个算法要学透（公式推导+代码编写）。<br>学好理论基础,磨刀不误砍柴功。</p><p>工具方面：<br>1、Matlab<br>MathWork公司的Matlab软件可以说是算法研究的利器，它的强大之处在于其方便快捷的矩阵运算能力和图形仿真能力。<br>做图像处理方面的研究，Matlab是必须掌握的，而且是熟练掌握。当你有一些想法需要验证时，最好明智的先用matlab编写出来测试。如果你上来就用看似高大上的C++来实验，不仅错误BUG一大堆，到头来可能效果还不佳，就算效果好，时间也会耽搁不少，毕竟算法开发还是要快的，这样才能赶在别人之前发论文。总之，只要是接触图像算法，终究逃不过Matlab。<br>    2、OpenCV<br>Opencv是Intel公司开发的C++图像处理工具包，形象的理解为就是C++版的Matlab。当初Intel公司开发这个工具包的初衷也是方便大家共享，希望大家能够在一个共同架构的基础上共同建造摩天大楼，而不是各自在自己的地基上盖平房。与Matlab不同，Opencv是面向开发的，稳定性好，异常处理机制周全，但有一点需要注意，由于Opencv是开源的，那么如果你在项目中直接调用了它的API，那就意味着你的项目也必须开源。因此在真正的产品开发过程中，往往需要从Opencv库里面挖代码，而不是直接调用。<br>3、Python<br>Python在图像处理算法方面除了其自身简洁的编程优势外，还得益于两个重要的Python类库——Numpy和Theano。<br>Numpy是Python的线性代数库，对于矩阵运算能提供很好的支持，并且能够在此基础上进行很多机器学习相关算法的开发仿真。</p><pre><code>最后是关于研究方向：图像分割</code></pre><p>图像分割分为语义分割和实例分割两种方式，语义分割是根据图像的语义信息来进行分割，在无人驾驶系统中主要用于可通行区域检测、车道线检测等。实例分割是在语义分割的基础上提出来的新概念，不仅可以预测像素所属类别，还能够区别同一类别的不同实例，主要用于车辆、行人等可数目标的识别与距离判断语义分割是指像素级的图像理解，即对图像中的每个像素标注所属的类别。<br>国内外研究现状：<br>图像分割是计算机视觉中的一种重要的图像预处理方法，是根据图像的灰度变化、空间纹理、几何形状等特征把图像划分成若干个互不相交的区域，使得这些特征在同一区域内具有一致性或相似性，而不同区域之间具有明显的不同点。<br>传统的图像分割方法多数只利用了图像的低层特征，较为经典的算法有基于阈值的图像分割、基于边缘检测的图像分割以及基于图论的图像分割等。近年来，随着深度学习的发展，研究重点发展为图像的高层特征，先后提出了语义分割和实例分割的概念，并将先验知识引入到了图像分割算法中，推出了新的算法理念。<br>语义分割对于无人驾驶感知系统的场景解析具有重要的作用，目前，常用于无人驾驶的语义分割网络主要有：ｖｉｊａｙ等人［３２］提出编码器－解码器网络结构，成为经典的图像分割网络结构之一，常用于可通行区域的分割；Ｌｉｎ等人［３３１提出一种多阶段的提炼网络，采用长距离残差连接，能够有效的将下采样中缺失的信息融合进来，提高了对于道路上的小目标的分割精度；Ｚｈａｏ等人％］提出一种金字塔场景解析模块，能够聚合不同区域的上下文信息，在目前的无人驾驶的图像分割算法中被广泛应用；Ｈａｎｇ等人［３５］提出了一种上下文编码网络，用于捕获场景的上下文语义并选择性的突出与类别相关的特征图，在复杂的道路场景中的分割结果具有明显的改善。上述方法在空间维度和特征通道两个方面均提出了很多新的提升图像分割精度的方法。<br>实例分割是在语义分割的基础上，不仅可以预测像素所属类别，还能够区别同一类别的不同实例。用于无人驾驶技术的实例分割网络主要有：Ｂａｅｋ等人提出一种基于环视监控的无人驾驶场景解析网络，实现了在环视图像中进行车辆的实例分割，对于无人驾驶的感知系统具有重要的意义；Ｋｉｒｉｌｌｏｖ等人提出一种全景分割方法，通过将扩张卷积增强的语义分割网络与具有特征金字塔骨架的实例分割网络相结合的方法，用于无人驾驶感知系统，对摄像头范围内的场景进行全方位的解析；Neven等人提出一种端到端的车道线实例分割算法，分为分割和聚类两个部分，在应用中不受所需检测车道数的限制，同时还能应对车辆变道，为无人驾驶系统的车道线识别开辟了新的研究方向。<br>基于深度卷积网的图像分割<br>随着深度学习的发展，基于深度卷积网的图像分割方法在一定程度上取代了传统的方法成为目前主流的图像分割方法。将深度卷积网应用到图像分割领域最重要的突破是实现了网络模型的全卷积化，从而将图像分割任务实现端到端得训练。随后提出了编码器－解码器的网络结构，以及空洞卷积、特征聚合等多种优化图像分割的方法，对于图像分割技术的发展均具有重要的意义。<br>—摘自《基于深度学习的无人驾驶图像分割算法研究》</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;关于图像识别、CV、视频理解的调研报告&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
</feed>
