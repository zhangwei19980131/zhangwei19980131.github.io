{"meta":{"title":"夜忆香","subtitle":"","description":"","author":"AliceMerser","url":"http://example.com","root":"/"},"pages":[{"title":"404","text":"","path":"404/index.html","date":"12-09","excerpt":""},{"title":"search","text":"","path":"search/index.html","date":"12-09","excerpt":""},{"title":"about","text":"QQ: 1320539211WeChat: zw19980131","path":"about/index.html","date":"12-09","excerpt":""}],"posts":[{"title":"Typora使用笔记","text":"#typora入门使用 标题一级标题使用一个#两级标题使用两个#三级标题使用三个#最多6个 文字删除线使用~ 文字 斜体与加粗 张维 张维 张维 下划线 张维 这个加上 或者ctrl + u 张维 ==张维== 水 H2O 面积 m^2^ :smile: : :100: :arrow_lower_right: 蚌埠住了:sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: :sweat_smile: 表格1234| name | price|---|---coffee | 19chicken | 25 name price coffee 19 chicken 25 引用 啦啦啦啦啦啦 列表无序列表 哈哈哈哈 啦啦啦啦 有序列表 后面要加空格 是这样的 代码123public static void main(String[] args)&#123; &#125; 12for i in range[10]: i++ 分隔线 *** 与 —都可以 跳转外部跳转百度一下 bilibili 内部跳转跳回标题 自动链接http://baidu.com 图片网上的图片目前hexo没弄出来网上链接图片，这个是本地的 本地图片一起测试 画图利用Mermaid画流程图，状态图，甘特图。生成的不是一张图片，而是html代码 流程图1234graph TBA--&gt;BB--&gt;CC--&gt;A 1234graph LRA--&gt;BB--&gt;CC--&gt;A 常用符号1234567graph TBAB[矩形节点]C(圆角矩形节点)D((圆形节点))E&#123;菱形节点&#125;F&gt;右向旗帜状节点] 例子12345graph TBbegin(出门)--&gt;buy(买炸鸡)buy--&gt;IsRemaining&#123;&quot;还有没有炸鸡？&quot;&#125;IsRemaining--&gt;|Yes|happy[买完炸鸡开心]--&gt;goback(回家)IsRemaining--&gt;|No|sak[伤心]--&gt;goback(回家) 连线1234567graph LRa--&gt;bc---de-.-fg--test---hi--test--&gt;lm-.-&gt;n …… 子图表1234567graph TB subgraph 买炸鸡前 begin(出门)--&gt;buy(买炸鸡) endbuy--&gt;IsRemaining&#123;&quot;还有没有炸鸡？&quot;&#125;IsRemaining--&gt;|Yes|happy[买完炸鸡开心]--&gt;goback(回家)IsRemaining--&gt;|No|sak[伤心]--&gt;goback(回家) 饼图12345pie title piechart &quot;Dog&quot; : 256 &quot;Cat&quot; : 412 &quot;Rat&quot; : 888 甘特图12345678910111213141516ganttdateFormat YYYY-MM-DDtitle Shop项目交付计划section 里程碑 0.1 数据库设计 :active, p1, 2021-08-15, 3d详细设计 : p2, after p1, 2dsection 里程碑 0.2后端开发 : p3, 2021-08-22, 20d前端开发 : p4, 2021-08-22, 15dsection 里程碑 0.3功能测试 : p6, after p3, 5d上线 : p7, after p6, 2d交付 : p8, afterp7, 2d 总结markdown: 标签的形式组织排版 代码的形式存储信息 跨平台共享内容 专注于内容本身","path":"2021/12/09/typora下编写Markdown文件/","date":"12-09","excerpt":"","tags":[]},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","path":"2021/12/09/hello-world/","date":"12-09","excerpt":"","tags":[{"name":"Gitalk","slug":"Gitalk","permalink":"http://example.com/tags/Gitalk/"}]},{"title":"图像分割","text":"图像分割 检测任务（Detection）是检测出图片中的物体位置，一般需要进行画框。比如下图中把人、羊，还有狗都框出来了，具体来说，网络需要输出框的坐标。在上图的检测任务中，矩形框还是比较粗糙的，并不知道每个像素具体属于哪个物体。下图中语义分割任务输出的绿色像素是背景，蓝色像素属于羊，红色像素是狗，还有一个颜色的像素属于人。当然，可以再精细一些，比如不同的羊的像素点用不同的颜色标记出来，那就是实例分割。 语义分割就是把每个像素都打上标签（这个像素点是人，树，背景等）（语义分割只区分类别，不区分类别中的具体单位） 实例分割不光要区别类别，还要区分类别中的每一个个体 在详细解释一下分割是要在原始图像中逐像素的找到你需要的家伙，有点像抠图，我们把一张图片上的人物或动物从背景中提取出来。 那深度学习下的图像分割过程也非常简单，下图中一个encoder,decoder，一个编码器，一个解码器，编码器通过把原来图像编码成特征图，它可能是一个几千维，几万维的特征，然后通过解码器把这个特征在解码成我们之前预先设定好的一个分割结果，比如说现在的一个分割结果是猪，天，草地，很明显这是一个语义分割。那基于这个架构的话我们有很多个尝试。比如说下面这个FCN(Fully Convolutional Networks)，encoder就用传统的分类网络，把分类器去掉，只做前面卷积也就是特征提取的部分，如VGG，从224一直卷到14，也就是我们的特征图了，然后再用反卷积的操作，比如上采样等等，相对称的decoder出骑自行车的人的效果图，我们可以发现整个网路是没有全连接层，这是一个全卷积网络。但是这个存在一个问题，我们原来这么大一张图卷积到这么小，然后我们在放成这么大，会存在图片失真也就是图像质量下降的现象，所以后来又出了Unet，Unet呢，卷积的过程是有下采样的，每次下采样的过程中保留一些特征，在下采样的过程中不断保留这些特征，等到上采样的时候decoder的时候，我在把它还回去，把缺少的东西再给补回来，所以说这个在各种情况下的应用效果都还不错。 假设我们输入是一个224×224×3的一个图像，上面有一个人和背景，那我们输出的图片大小应该也是224×224，不应该是变大变小，而维度，图像分割是做一个逐像素点的分割，是一个二分类的问题，比如说这一个点是人还是不是人，所以输出的维度应该是2，所有输出是224×224×2。 然后就是进行卷积+池化进行一个特征提取，我们的图片越来越小，这是一个下采样的操作，然后进行上采样，上采样呢，一般来说我们叫做插值，例如现在有一个像素值是5还有一个像素值是20，我们往其中插入两个值比如10和15。 整体来说就是一个编码解码的过程，计算机把我们能看懂的图片数据转换成它能识别的特征或者说是矩阵，这是编码，解码就是计算机用我们这个特征或者说矩阵帮我们输出出最终我们想要的一个结果。 最早在医学领域做细胞分割，为什么说在医学领域做的比较好或者说做的比较多？医学当中很多待分割的目标，比如说是一个小细胞，都是比较小的东西。而网络结构有什么特点呢？越深层的网络，看到东西越多，更适合做一些大目标的分割，网络结构浅的话适合做一些小目标的分割，U-net比较浅层的原因，或者说设计最初的思想就是想在医学当中解决一些小目标的问题。 U-net里面前面这些层视为浅层，后面为深层，在这里箭头代表的是一个拼接操作，既融进来浅层特征，也融进来深层特征，拼接要注意尺寸大小要对应上。 这是一个简图，下采样上采样拼接，但是有一个小问题，这两个拼接还好，第一层和最后一层，一个只是浅层的一个是非常深的，他俩拼接就显得并不是很合适，有一些跨度，跨度有些地方有点太大了。怎么解决这个问题呢？ 提出了U-Net++，我们标号1,2,3,4,1不能和2拼接，因为大小不一样，所以1和2的上采样拼接，2和3的上采样拼接..我们看这个X0,2。这是由3个进行拼接的，X0,0 X0,1 X1,1，这回呢我们拼接的方法更多了，距离也更近一些，论文当中也通过实验证明这样拼接呢效果会更好一些。这是第一个改进的地方，第二是关于损失函数的改进，论文当中在这里，这里，这里都连上了loss，为什么这么做呢，举个例子是我们网络费了好大劲最后输出一个结果，就好比我们读书上学，上小学、初中、高中、大学，最终的损失函数就是考大学，高考，我们考的怎么样，这是一个损失函数，它是通过最终的结果来评判我们怎么去优化，但是呢我们想，要是最终想要高考考的好，前提呢需要是考上好的小学，好的初中，好的高中，最后才能上好的大学，就是我们的损失函数不仅仅是在最后一步，在中间的部分也加上损失函数，相当于让网络关注的点更多了，不仅仅要最终输出的好，每一步都要输出的好。","path":"2021/10/04/图像分割/","date":"10-04","excerpt":"","tags":[{"name":"图像分割","slug":"图像分割","permalink":"http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"}]},{"title":"域名备案","text":"关于hexo博客想要使用cdn加速又要备案的问题马上放国庆假了 :smile: :smile: :smile:域名问题与实名认证的问题由于我的域名购买是在godday上购买，而godday国外域名服务商购买时似乎省略了实名认证的步骤，也没有提供域名备案方法，而hexo博客的服务器github page 是在国外，一般情况不需要备案，但是要是想要使用cdn加速，还是要备案。。。。。。 这是一个很麻烦的问题鸽了~~ 等域名过期后，在阿里云或腾讯云购买域名再说（2021.9.26）可申请备案域名查询https://seo.juziseo.com/doc/tld/cnicp","path":"2021/09/26/域名备案/","date":"09-26","excerpt":"","tags":[{"name":"备案","slug":"备案","permalink":"http://example.com/tags/%E5%A4%87%E6%A1%88/"}]},{"title":"深度相机","text":"深度相机与RGB-D相机什么是深度图 左侧是一个普通的RGB图像，右侧是一个Depth图像。如何获得呢？第一个是深度相机采集，第二个是立体匹配计算，就是我们用双目相机来计算出一张深度图，第三个是数据集，比如说我们右边这两幅图，它的来源就是德国TUM大学的一个比较知名的一个开源数据集 这个是使用工具ImageJ打开的一张深度图，我们可以看到这个光标在动，x,y的坐标也在发生变化，value值也在变化，值代表的是距离，是当前的被测物体到镜头之间的距离。比方说我们的这个value值是6000，除以尺度因子就是它的一个物理距离，不同的相机它的尺度因子是不一样的，TUM这个采集数据集的相机的尺度因子是5000 什么是深度相机与RGB-D相机 深度相机是一种比较广泛的说法，能测出深度都可以叫深度相机，不管是通过软件算法间接计算还是物理方法直接测量，我们通常说的RGB-D相机是通过物理方法测试的比如说结构光、TOF(飞行时间) 我们看一下右边有这么多种深度相机，有单目结构光和双目结构光相机，这个是只有深度值的相机，下面这个普通双目是通过立体匹配来计算深度，还有tof等等，单目和双目有什么区别呢，单目就是说我们有一个红外发射器，然后有一个红外接收器，而双目是有一个红外发射器，两个红外接收器，就是得到两个红外图。 RGB-D相机分类 Kinect 是最早做RGB-D相机的，是微软他们推出的设备，比如说Kinect v1(结构光法)后来iphone X是最早在手机上推出深度相机的，tof的呢，有Kinect v2，还有Phab 2 pro，这是2016年由谷歌和联想共同推出的一款手机，主要是在国外市场，国内大家可能不太清楚。 RGB-D相机原理（结构光） 关于RGB-D相机原理是比较复杂的，这里我就简单介绍带过一下，我们通常拿到设备就是直接用，不需要深入了解里面的硬件是怎么做的。 结构光呢，是为了解决双目匹配问题产生的，我们看到右边这两张图，这是RGB的一个双目匹配图，第一个是原图所对应的灰度图，下面这个是通过RGB的一个立体匹配得到的一个深度图，右上角是通过红外投射的一个红外散斑图，右下角是通过结构光得到的一个深度图，我们可以对比一下，同一场景下，他们两得到的深度图差别非常大，结构光法得到的明显比这个好很多。 我们知道双目匹配的时候，需要提特征点，需要做很多图像上的操作，如果光照不好我们照的图片里面就很难去提取特征点，但是结构光这个相机投的是红外光，所以是没有影响。 解决依赖图片纹理问题，因为它是往物体上投射图案。提高鲁棒性等等，特点的话，夜里可用，比如说我们的iphonX，晚上关了灯以后人脸解锁也是可以用的。 结构光方案鼻祖是这家以色列公司，应用于微软的明星产品被广为人知，后来被apple收购了，iPhone X人脸识别这一块做的非常好，非常稳定，其实它硬件是非常好的。右边是它的一个示意图，它投出来的叫伪随机散斑，投到物体上，我们可以看到每一块都是不一样的，通过这种伪随机性，得到一个深度图。 RGB-D相机原理（TOF） 飞行时间法这个我们可以看右边这幅图，通过发射光脉冲到物体上，然后这边有一个接收器，这个接收器通过反射回来的光脉冲，计算时间差来测这个深度和距离，所以叫飞行时间，原理是非常简单的，但是硬件上实现是有很大难度的。目前手机上没有一个特别好的TOF手机。因为功耗大，我们不可能拿着移动设备，还要插着220V电压，来到处跑着测。 透明物体的影响：比如说s这块玻璃，那么我们红外光发射穿过这块玻璃，跑到外面了，外面空间是无限远的，那我们这个红外光就是肉包子打狗，有去无回了，也就是右边这幅图黑色的情况。","path":"2021/09/14/深度相机/","date":"09-14","excerpt":"","tags":[{"name":"深度相机","slug":"深度相机","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E7%9B%B8%E6%9C%BA/"},{"name":"RGB-D","slug":"RGB-D","permalink":"http://example.com/tags/RGB-D/"}]},{"title":"OpenMVG+PMVS三维重建入门使用","text":"OpenMVG+PMVS三维重建入门使用两大游戏引擎 这个表格是关于两大引擎的对比关于他们的开发公司、总部地点、公司市值、开发语言和公司的属性。 然后关于建模软件如Zbrush、3Dmax等等软件建模出的人物或动物模型如右图这种，在计算机显示的3d图形，它是Mesh的一种表现形式。 第一个point cloud ，深度传感器扫描得到的深度数据，点云 第三个Volumetric，将空间划分成三维网格，栅格化。 第四个Multi-View，用多个角度的图片表示物体 而第二个Mesh，三角面片在计算机图形学中渲染和建模非常有用。 使用OpenMVG和MVS三维重建 三维重建的流程之前也介绍过，所以这里简单提一下，我们需要从多视角的图像中提取特征，然后进行SFM和MVS。 我这里使用的是两个开源库，一个是openMVG、一个是PMVS。PMVS呢，是把sfm的一个输出作为输入，在进行一个密集重建。运行环境为ubuntu，windows安装过程较为繁琐，而且容易出现的问题也很多，可以在Windows下安装虚拟机装好ubuntu系统。 openMVG和PMVS的安装可参考 https://blog.yanjingang.com/?p=3329 出现的问题： 由于网络问题可能会导致连接不上github，或者导致只安装一部分文件，如：openMVG/src/dependencies里面的cereal、glfw、osi_clp文件为空。 解决方法： 网好的时候重新clone 直接去网站上把整个文件code下来，或者只code下dependencies 后面安装过程，cmake版本需要注意一下（cmake -version）。中途有个长时间的百分之一到百分之百的等待，出现的提示错误不是进行不下去的可以不用管，到百分百结束。 解锁方法：（无法获得锁问题/被占用….） sudo rm /var/lib/apt/lists/lock 权限问题一半情况下加上sudo。 这是官方给出的测试集 https://s31.aconvert.com/convert/p3r68-cdx67/qkg98-il4p9.gif 进行SFM稀疏点云重建的结果 进行MVS重建的结果 注意的问题： 一个是格式问题，使用微信、qq从手机传到电脑会丢失图像信息，我的解决办法是在Ubuntu系统下安装百度网盘，通过手机把图片传输到网盘，然后再在ubuntu系统上下载下来，还有一个问题是iPhone12拍摄的图片默认格式是heic格式，这个苹果自iOS11以来，默认的图片格式，它体积更小、清晰度更高。如果我们在Windows系统下修改后缀名，如jpg，png是可以正常查看的，但是在ubuntu系统是读取不到图片的，必须得是拍摄的时候在手机设置里修改格式为jpg。 第二个问题是相机参数问题，Sensor_width(传感器宽度)。在openMVG库的源文件里的senor_width这里面提供了很多不同种类的相机参数，但是iphone系列只添加到iphone型号XS这里，所以我们需要在下面添加上iphone12的参数值。 具体过程： 采集好图片后打包放入（car/images）里面 在openMVG里面创建3dr_test.py文件 修改文件所在的路径 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import osimport subprocessimport sys# openmvg编译bin目录(可cp -p到/usr/local/bin/)OPENMVG_SFM_BIN = &quot;/home/work/tools/openMVG_Build/Linux-x86_64-RELEASE&quot; # 路径注意对好，虚拟机下这个文件通常home后面跟的是用户名，可以找到openMVG文件里的Linux- ... 复制路径过来# pmvs编译bin目录(可cp -p到/usr/local/bin/)PMVS_BIN = &quot;/home/work/tools/CMVS-PMVS/build/main&quot; # 路径问题同上# openmvg相机参数目录CAMERA_SENSOR_WIDTH_DIRECTORY = &quot;/home/work/tools/openMVG/src/openMVG/exif/sensor_width_database&quot;# 自己的相机参数如果文件里没有需要添加上去# 0. 下载测试照片os.chdir(os.path.dirname(os.path.abspath(__file__)))data_dir = os.path.abspath(&quot;./car&quot;) # 这里修改路径,例如我的是car,修改为你自己数据集的文件夹名#data_dir = os.path.abspath(&quot;./ImageDataset_SceauxCastle&quot;)# 下面这一段是官方的测试代码，在github上下载的，测试官方用例网络不好可以直接去网站上面挂vpn下载放入对应路径&#x27;&#x27;&#x27;if not os.path.exists(data_dir): pImageDataCheckout = subprocess.Popen([ &quot;git&quot;, &quot;clone&quot;, &quot;https://github.com/openMVG/ImageDataset_SceauxCastle.git&quot; ]) pImageDataCheckout.wait()&#x27;&#x27;&#x27;input_dir = os.path.join(data_dir, &quot;images&quot;)output_dir = data_dirprint (&quot;Using input dir : &quot;, input_dir)print (&quot; output_dir : &quot;, output_dir)matches_dir = os.path.join(output_dir, &quot;matches&quot;)camera_file_params = os.path.join(CAMERA_SENSOR_WIDTH_DIRECTORY, &quot;sensor_width_camera_database.txt&quot;) #相机参数if not os.path.exists(matches_dir): os.mkdir(matches_dir)# 1. 从图片数据集中生成场景描述文件sfm_data.jsonprint (&quot;----------1. Intrinsics analysis----------&quot;)pIntrisics = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_SfMInit_ImageListing&quot;), &quot;-i&quot;, input_dir, &quot;-o&quot;, matches_dir, &quot;-d&quot;, camera_file_params, &quot;-c&quot;, &quot;3&quot;] )#*注：如果产出的sfm_data.json里intrinsics内容为空，通常是在图片没有exif信息导致获取不到相机焦距、ccd尺寸等参数，用带exif的原图即可。pIntrisics.wait()# 2. 计算图像特征print (&quot;----------2. Compute features----------&quot;)pFeatures = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_ComputeFeatures&quot;), &quot;-i&quot;, matches_dir+&quot;/sfm_data.json&quot;, &quot;-o&quot;, matches_dir, &quot;-m&quot;, &quot;SIFT&quot;, &quot;-f&quot; , &quot;1&quot;] )pFeatures.wait()# 3. 计算几何匹配print (&quot;----------3. Compute matches----------&quot;)pMatches = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_ComputeMatches&quot;), &quot;-i&quot;, matches_dir+&quot;/sfm_data.json&quot;, &quot;-o&quot;, matches_dir, &quot;-f&quot;, &quot;1&quot;, &quot;-n&quot;, &quot;ANNL2&quot;] )pMatches.wait()# 4. 执行增量三维重建reconstruction_dir = os.path.join(output_dir,&quot;reconstruction_sequential&quot;)print (&quot;----------4. Do Incremental/Sequential reconstruction----------&quot;) #set manually the initial pair to avoid the prompt questionpRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_IncrementalSfM&quot;), &quot;-i&quot;, matches_dir+&quot;/sfm_data.json&quot;, &quot;-m&quot;, matches_dir, &quot;-o&quot;, reconstruction_dir] )pRecons.wait()# 5. 计算场景结构颜色print (&quot;----------5. Colorize Structure----------&quot;)pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_ComputeSfM_DataColor&quot;), &quot;-i&quot;, reconstruction_dir+&quot;/sfm_data.bin&quot;, &quot;-o&quot;, os.path.join(reconstruction_dir,&quot;colorized.ply&quot;)] )pRecons.wait()# 6. 测量稳健三角print (&quot;----------6. Structure from Known Poses (robust triangulation)----------&quot;)pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_ComputeStructureFromKnownPoses&quot;), &quot;-i&quot;, reconstruction_dir+&quot;/sfm_data.bin&quot;, &quot;-m&quot;, matches_dir, &quot;-o&quot;, os.path.join(reconstruction_dir,&quot;robust.ply&quot;)] )pRecons.wait()&#x27;&#x27;&#x27;# 使用全局SfM管道重建Reconstruction for the global SfM pipeline# 3.1 全局sfm管道几何匹配print (&quot;----------3.1. Compute matches (for the global SfM Pipeline)----------&quot;)pMatches = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_ComputeMatches&quot;), &quot;-i&quot;, matches_dir+&quot;/sfm_data.json&quot;, &quot;-o&quot;, matches_dir, &quot;-r&quot;, &quot;0.8&quot;, &quot;-g&quot;, &quot;e&quot;] )pMatches.wait()# 4.1 执行全局三维重建reconstruction_dir = os.path.join(output_dir,&quot;reconstruction_global&quot;)print (&quot;----------4.1. Do Global reconstruction----------&quot;)pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_GlobalSfM&quot;), &quot;-i&quot;, matches_dir+&quot;/sfm_data.json&quot;, &quot;-m&quot;, matches_dir, &quot;-o&quot;, reconstruction_dir] )pRecons.wait()# 5.1 计算场景结构颜色print (&quot;----------5.1. Colorize Structure----------&quot;)pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_ComputeSfM_DataColor&quot;), &quot;-i&quot;, reconstruction_dir+&quot;/sfm_data.bin&quot;, &quot;-o&quot;, os.path.join(reconstruction_dir,&quot;colorized.ply&quot;)] )pRecons.wait()# 6.1 测量稳健三角print (&quot;----------6.1. Structure from Known Poses (robust triangulation)----------&quot;)pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_ComputeStructureFromKnownPoses&quot;), &quot;-i&quot;, reconstruction_dir+&quot;/sfm_data.bin&quot;, &quot;-m&quot;, matches_dir, &quot;-o&quot;, os.path.join(reconstruction_dir,&quot;robust.ply&quot;)] )pRecons.wait()&#x27;&#x27;&#x27;# 7. 把openMVG生成的SfM_Data转为适用于PMVS输入格式的文件print (&quot;----------7. Export to PMVS/CMVS----------&quot;)pRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, &quot;openMVG_main_openMVG2PMVS&quot;), &quot;-i&quot;, reconstruction_dir+&quot;/sfm_data.bin&quot;, &quot;-o&quot;, reconstruction_dir] )pRecons.wait()#*注：执行后会在-o路径下生成一个PMVS目录，包含 models, txt, visualize 三个子目录：models为空；txt包含对应图像的txt文档，每个里面都是一个3x4的矩阵，大概是相机位姿；visualize包含11张图像，不确定是原图像还是校正过的图像# 8. 使用PMVS重建稠密点云、表面、纹理print (&quot;----------8. pmvs2----------&quot;)pRecons = subprocess.Popen( [os.path.join(PMVS_BIN, &quot;pmvs2&quot;), reconstruction_dir+&quot;/PMVS/&quot;, &quot;pmvs_options.txt&quot;] ) # 注：不要修改pmvs_options.txt文件名pRecons.wait()#*注：执行后会在./PMVS/models文件夹中生成一个pmvs_options.txt.ply点云文件，用meshlab打开即可看到重建出来的彩色稠密点云。","path":"2021/09/03/OpenMVG-PMVS三维重建入门使用/","date":"09-03","excerpt":"","tags":[{"name":"三维重建","slug":"三维重建","permalink":"http://example.com/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"}]},{"title":"ResNet","text":"ResNet这是一篇不完善不完整的总结文档（不如看论文），内容不是很全，以后有机会在补充（2021.9.6） 感谢B站UP主@霹雳吧啦Wz :cry: 没人带，摸索着学习真难 简要介绍ResNet在2015年由微软实验室提出，斩获当年lmageNet竞赛中分类任务第一名，目标检测第一名。获得coco数据集中目标检测第一名，图像分割第一名。(啥也别说了，就是NB) 网络亮点 超深的网络结构（突破1000层） 提出residual模块 使用Batch Normalization加速训练（丢弃dropout） 网络结构 Resnet 34 残差结构Batch NormalizationBatch Normalization是google团队在2015年论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》提出的。通过该方法能够加速网络的收敛并提升准确率。 Batch Normalization的目的是使我们的一批（Batch） feature map满足均值为0，方差为1的分布规律 μ, σ²在正向传播过程中统计得到 γ, β在反向传播过程中训练得到 迁移学习使用迁移学习的优势： 能够快速的训练出一个理想的结果 当数据集较小时也能训练出理想的效果 常见的迁移学习方式： 载入权重后训练所有参数 载入权重后只训练最后几层参数 载入权重后在原网络基础上再添加一层全连接层，仅训练最后一个全连接层 代码复现","path":"2021/08/27/ResNet/","date":"08-27","excerpt":"","tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"GoogLeNet","text":"GoogLeNet简要介绍GoogLeNet在2014年由Google团队提出，斩获当年IlmageNet竞赛中Classification Task(分类任务)第一名。 网络中的亮点: 引入了Inception结构（融合不同尺度的特征信息) 使用1x1的卷积核进行降维以及映射处理 添加两个辅助分类器帮助训练 丢弃全连接层，使用平均池化层（大大减少模型参数) GoogLeNet中L大写是为了致敬LeNet AlexNet和VGG都只有一个输出层，GoogLeNet有三个输出层（其中两个辅助分类层） 网络结构 Inception结构初始版本 之前的网络如AlexNet和VGGNet都是串行结构，一系列的卷积层和最大下采样层串联得到我们的一个网络结构，但是我们的Inception结构出现了一个并行结构。 将我们所得到的一个特征矩阵同时输入到这4个分支当中，同时进行处理，处理之后将我们同时所得到的这4个分支的特征矩阵，按深度进行拼接，得到我们的一个输出矩阵。 注意：每个分支所得的特征矩阵高和宽必须相同，否则我们无法沿深度方向进行一个拼接 后来版本（加上了一个降维的功能） 比上一张图多了3个1x1的卷积层，都是起到一个降维的作用。原理： 辅助分类器 GoogLeNet中有两个辅助分类器，他们的结构是一模一样的 第一层是平均池化下采样的操作 … 参数对比GoogLeNet模型参数： 6994392 VGGNet模型参数：138357544 相差约20倍 代码复现一般情况下用的人不是很多，因为有辅助分类器等等，搭建起来较为复杂，更多的人使用的是VGG，如果只是用来做分类任务可以使用，准确率还是很高的。","path":"2021/08/27/GoogLeNet/","date":"08-27","excerpt":"","tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"游戏制作与引擎","text":"游戏制作与引擎相关介绍游戏制作与建模 通常游戏的制作分为3个部分：设计、程序和美工。 设计主要是思考游戏的核心玩法是怎么样的，游戏的故事线背景剧情是什么样的。 美工只是负责游戏的草图原画，并绘制在计算机中，以及对角色物体场景的三维建模。 程序则是将游戏的玩法逻辑用代码给实现出来，可以说这3个部分在游戏的制作中是缺一不可的，如果你是一个个人开发者，那么你将可能同时担任所有的角色，而对于新手入门编程来说，有一个好的点子，在网上选一些免费无版权的资源，或者购买一些三维模型和动画，懂一些简单的编程，对小游戏的开发是完全没有问题的，但如果是一家游戏公司任职的话，情况的要复杂的多，制作人，QA（质量保证测试），运营部门， 这里不介绍了。 那我们讲一下游戏里的物体制作通常分为两步。 建模就是把2D平面的东西3D立体化，把一个东西立体化之后，你就可以三百六十度旋转观察。这样做出来的东西，我们称之为模型。像王者荣耀、LOL、吃鸡里面的人物和场景就是用这个做的。这就是一个简单的正方体3D模型比2D平面更加优秀的是，观察的更加直观。 建模软件常用的有…… 每个软件都有各自的特色。 Zbrush适合于对生物角色的建模，3dMax在各大公司的使用都非常的广泛，Maya在三维动画和视觉领域非常的出色，Blender是近几年来人气暴增并且开发源代码免费三维建模软件。 在建模阶段，设计师通过美工的草图或者三视图直接绘制出三维空间中可以自由观看的立体模型，他们看起来都异常的简陋，甚至没有任何颜色，像左图这种，所以在下个阶段，建模师会在三维模型表面绘制出纹理和材质，比如说金属表面的光泽度很高，而玻璃材质的物体不但反光还是透明的，石头的表面有不规则的纹路和裂痕，这些对模型都非常的重要，好的材质会让我们的模型在游戏中显得更加自然和真实。最后我们的模型大概是长这个样子的，他们就是游戏中直接加载的资源，有了模型之后，还需要对人物怪兽这些会动的物体进行动画制作，这里不介绍了。 游戏框架（引擎） 从第一款真正意义的电子游戏诞生至今，游戏已经走过了FC、街机、PC、页游四个时代。在移动互联网新技术的加持下，游戏产业已经来到井喷发展的主机：手游时代。 目前主流的开发引擎有两个，一个是Unity，一个是UE。 常见的比如王者荣耀和炉石传说都是使用Unity进行开发的，包括和平精英也是可以使用Unity去实现的。 Unity它里面封装了很多游戏开发所需要的功能，本身还支持了很多其他的技术领域，例如工艺仿真，动漫，电影，手机APP，包括我们刚才说的虚拟现实和增强现实。 UE5效果展示 对于「雪」的处理，而在角色、武器向不同方向移动的时候，雪地也会有相应的变化，并把痕迹留存下来。 在原地盘旋的时候，如何表现出白龙身体不同部位向不同角度的扭曲？如何让它有一种快慢交替的节奏？在冲刺的一瞬间，它的肢体又该如何纵向伸展？ Unity效果展示 Unity与UEC#是一种托管语言，和c++比，不够Native，不够高效，和脚本语言比，又不够灵活。 但它胜在全面。 强类型，跨平台，语法糖，应有尽有。 比简单的脚本语言强大，比c++更简单易用。 使用哪一款游戏引擎去制作一款游戏，并不是根据使用者的喜好来制定的。开发者们的喜好千奇百怪，但最终能在技术选型中影响到结果的，往往是技术积累以及引擎本身的特性。 参考：知乎","path":"2021/08/27/游戏制作与引擎/","date":"08-27","excerpt":"","tags":[{"name":"游戏制作","slug":"游戏制作","permalink":"http://example.com/tags/%E6%B8%B8%E6%88%8F%E5%88%B6%E4%BD%9C/"},{"name":"引擎","slug":"引擎","permalink":"http://example.com/tags/%E5%BC%95%E6%93%8E/"}]},{"title":"三维重建简介","text":"三维重建简单介绍 我们现实世界的物体，比如说有一只兔子，它有几种表示方法。 很多点数据所构成的表征，来表示这个物体，这是点云，除了点云，还有Mesh的方法，是用很多三角形的面片或正方形的面片所拼成的这样一个物体，还有Volumetric这是一种栅格化的一种表征方法，在这张图我们看到有很多小的蓝色的栅格，很多的栅格就拼成了这样一只兔子，还有一种叫projected View 这是一种通过图片，不同的角度来构成一个立体的一个兔子的形状，它有RGB的颜色和深度的信息。 点云可以通过激光雷达扫描所得到，也可以通过摄影的方法，获取RGB图像，和深度信息的图像，然后通过透视几何可以反推出一些空间中的点云数据，这是通过一个角度来构造这种三维数据，现在一般是通过多个摄像头，倾斜摄影来构成点云数据。我们可以看到在这张图片上真正的点云一般有位置信息，空间的三维坐标XYZ，通过图像的话还可以获取到颜色信息，当然还有一些其他的方法获取强度信息，法向量等 严格来说呢，RGB结合深度信息的方法只能称为2.5D，真正的点云的数据一般是通过激光雷达扫描所得到 关于深度相机的一些相关介绍 我们通过用相机拍摄真实世界的物体、场景，并通过计算机视觉技术进行处理，从而得到物体的三维模型，主动比如说是激光，被动呢可能会是影像。单幅图像是无法重建场景结构的，虽然我们知道投影过程，但是是没有深度信息的，比如我们这右边图，这个人和比萨斜塔，真实情况肯定不是这样的，这是拍出来的效果，所以说只凭单幅图像是无法恢复出深度信息的。下面这个三视图是二视图的一个扩展，比如说这个立方体x1这个点，我们这三个相机都可以观察到这个投影点，那么这个点就是这三个相机的投影点。 我们先看左边这幅图，首先我们拿到一系列的图集，然后进行特征的提取，特征点的提取和描述，然后进行特征点的匹配和滤波（也就是错误特征点的去除），然后得到我们匹配好的投影点，这一步是sift特征提取，然后估计他的姿态参数，通过Bundle Adjustment进一步优化，得到一个稀疏的点云，也就是一个稀疏的场景结构，那么我们如何想要得到一个密集的场景结构，向右图这种，我们需要做密集点云的一个匹配，这个过程叫MVS，然后我们就可以得到密集的点云。后面的部分还有一些，如点云融合，初始网络重建，网络优化，我们最后得到的一个输出结果（纹理贴图），由……。左侧这边，因为我们有很多图像，这里的话还有一个图像聚类的过程，以及添加控制点的过程，这里就是为了让我们更高精度的完成一个大尺度重建。 附： SFM（运动恢复结构）：通过相机运动恢复场景的结构，从图像中恢复出稀疏的三维点坐标，用这些稀疏的三维点坐标，表示场景的结构。 MVS（密集重建）：关键是寻找空间中具有图像一致性的点。目前MVS稠密重建这个研究方向都在朝着深度学习的方向来做研究和优化。","path":"2021/08/24/三维重建简介/","date":"08-24","excerpt":"","tags":[{"name":"三维重建","slug":"三维重建","permalink":"http://example.com/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"},{"name":"三维点云","slug":"三维点云","permalink":"http://example.com/tags/%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91/"}]},{"title":"三维点云介绍","text":"点云基础点云数据 点云是某个坐标系下的点的数据集。点包含了丰富的信息，包括三维坐标X，Y，Z、强度值（因为点云经常是由激光来采取的，强度是指它从表面反射回来所获取的激光能量信息）、当然我们配上相机的话就会有颜色、时间戳是跟无人驾驶有关，比如说在无人车中，车在动，每个点也在动，激光也在转，所以每个点也都有个时间戳，这个时间戳方便我们把不同的点云放到同一个坐标系下面点云可以将现实世界原子化，通过高精度的点云数据可以还原现实世界。上面这张图片我们可以看到是一个居民住处的点云，我们可以看到是已经做好分类的，棕黄色是屋顶，绿色是植被，剩下的是地面。分类：有组织的：点云被布置为类似于图像结构的二维的点矩阵。我们可以根据像素（x,y）信息来访问这个点云，大部分是由深度相机获得的。我们可以看做是一张图片，每个点它有深度信息，可以算他的三维坐标，上面图片我们就无法把它转换成一个二维的图片，除非要损失一些点的信息，这就是无组织的。无组织的：点云是一个点列表有序点云：一般由深度图还原的点云，有序点云按照图方阵一行一行的，从左上角到右下角排列，当然其中有一些无效点因为。有序点云按顺序排列，可以很容易的找到它的相邻点信息。有序点云在某些处理的时候还是很便利的，但是很多情况下是无法获取有序点云的。无序点云：无序点云就是其中的点的集合，点排列之间没有任何顺序，点的顺序交换后没有任何影响。是比较普遍的点云形式，有序点云也可看做无序点云来处理。 点云获取 最常见的是激光扫描仪了，激光打出去，然后收回来得到距离，根据传感器的位置信息就可以反算点的三维坐标就可以得到点云。LiDAR获取的数据就是点云数据，同时也对点云数据进行处理加工以及应用。星载LiDAR采用卫星平台，运行轨道高、观测视野广，基本可以测量到地球的每一个角落，为三维控制点和数字高程模型的获取提供了新的途径，有些星载激光雷达还具有观察整个天体的能力。机载主要借助无人机（UAV{无人驾驶飞机}/UAS{无人机系统}）进行大规模的点云数据采集。地面分为三小种：地上三维激光扫描、车载MMS、手持激光扫描。双目相机是模仿人眼 点云应用领域 点云的应用也日益广泛，应用在机器人和无人驾驶上来获得周围环境和物体的三维信息来帮助自动驾驶的实现，还可用在AR（增强现实），VR（虚拟现实）中，还有工业设计，比如Shape Design。FaceID就是通过点云获取人脸的三维特征来作为区分人脸的标识。 点云相关研究 点云相关研究，这里只列出3个 点云数据集 关于点云数据集，第一个3d形状分类，常见有ModelNet40、ShapeNet、ScanNet等，关于3d物体的检测和追踪，比如KITTI，是关于自动驾驶场景中所获得的一些数据集，还有ScanNetV2、Waymo Open等，对于3维点云分割，有ScanNet、S3Dis等等。 从传统技术到深度学习 传统点云的三维重建，大都是通过一些算法构造出三维表面网格模型，比如SFM，是一种基于各种收集到的无序图片进行三维重建的离线算法，从时间系列的2D图像中推算3D信息 步骤： 特征提取（SIFT, SURF, FAST等一堆方法） 配准（主流是RANSAC和它的改进版 全局优化bundle adjustment 用来估计相机参数 数据融合 3D点云应用深度学习面临的挑战。首先在神经网络上面临的挑战： 非结构化数据（无网格）：点云是分布在空间中的XYZ点。 没有结构化的网格来帮助CNN滤波器。 不变性排列：点云本质上是一长串点（nx3矩阵，其中n是点数）。 在几何上，点的顺序不影响它在底层矩阵结构中的表示方式，不同的点排一个顺序还是同一个点云，例如， 相同的点云可以由两个完全不同的矩阵表示。 点云数量上的变化：在图像中，像素的数量是一个给定的常数，取决于相机。 然而，点云的数量可能会有很大的变化，这取决于各种传感器。 在点云数据方面的挑战： 缺少数据：扫描的模型通常被遮挡，部分数据丢失。 噪音：所有传感器都是嘈杂的。 有几种类型的噪声，包括点云扰动和异常值。 这意味着一个点有一定的概率位于它被采样的地方（扰动）附近的某一半径范围内，或者它可能出现在空间的任意位置（异常值）。 旋转：一辆车向左转，同一辆车向右转，会有不同的点云代表同一辆车。","path":"2021/08/16/三维点云介绍/","date":"08-16","excerpt":"","tags":[{"name":"三维点云","slug":"三维点云","permalink":"http://example.com/tags/%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91/"}]},{"title":"关于hexo博客的一些问题","text":"Hexo近些天折腾了许久Hexo博客，包括主题，升级插件，添加评论功能，文章热度统计等记录一下. 升级hexo这是我现在hexo的版本5.4.0，之前是4.2.0，由于只使用npm -update 并不能更新到5.0.0之后的版本（不知道为什么） 升级办法： 12345678//以下命令分别执行即可npm install -g npm-check //安装npm-checknpm-check //查看系统插件是否需要升级npm install -g npm-upgrade //安装npm-upgradenpm-upgrade //更新package.json//在执行npm-upgrade命令后会要求输入yes或者no，直接输入Yes或Y即可npm update -g //更新全局插件npm update --save //更新系统插件 Node.js的版本也不能太低，应该 12node -vv12.8.2 最后 hexo -v查看版本 因为hexo5.0.0以上有代码高亮 hexo d由于网络等原因，莫名其妙会无法部署上去出现fatal: unable to access ‘https://github.com/...‘ 解决办法 只能尝试，有时重新clean g d 会解决 hexo clean hexo g hexo d 百度的一些命令（） 在开启shadowsocks的前提下，手动配置git的代理 123git config --global http.proxy http://127.0.0.1:1080git config --global https.proxy http://127.0.0.1:1080 取消代理： 123git config --global --unset http.proxygit config --global --unset https.proxy 玄学解决，试几次就成功了 ipconfig /flushdns 评论系统这里着实费了一些时间，由于gal上gitment和gitalk无法使用，于是换了主题改用Valine，具体配置不写了百度很多，遇到了一个小问题 [Code 403: 访问被api域名白名单拒绝，请检查你的安全域名设置. ·Issue #72 · xCss/Valine]问题解决在github上，Valine里的issue里 https://github.com/xCss/Valine/issues/72 这里说下我的，在web安全域名上我填写了两个地址：http与https，只填写https还是会出现Code 403问题，不知道是不是http的原因。 由于舍弃了上一个主题，这个主题的live-2d看板娘与Valine评论里的提交重叠，只能舍弃看板娘:cry: 评论头像问题 Gravatar官网上注册账号，上传头像，并在评论写上注册的邮箱号即可显示，缺点是需要翻墙，页面显示头像和登录Gravatar注册都需要:cry: 更新hexo完图片读取不出来又是一个坑 由于更新了主题与hexo，插件的功能似乎不能用了，使用了图床的方式粘贴url地址来显示hexo d的图片，不重新搭博客了。 https://blog.csdn.net/weixin_41010198/article/details/119698015 hexo-tag-aplayer设置 暂时写到这，以后会继续更新","path":"2021/08/06/关于hexo博客的一些问题/","date":"08-06","excerpt":"","tags":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"}]},{"title":"AlexNet代码复现","text":"AlexNet实现代码部分[TOC] hexo博客不支持目录toc、csdn支持 环境python == 3.6 torch-gpu == 1.2.0 torchvision == 0.4.0 numpy == 1.19.5 matplotlib == 3.3.4 工具pycharm 写在前面代码文件有3个， alexnet_inference.py 在预训练好的AlexNet模型里，给定图片，进行TOP5判断，输出5个可能的分类 alexnet_visualizaton.py 卷积核与特征图的可视化，使用tensorboard train_alexnet.py 在kaggle上下载猫狗数据集训练一个AlexNet alexnet_inference.py路径结构 导入环境123456789101112import osos.environ[&#x27;NLS_LANG&#x27;] = &#x27;SIMPLIFIED CHINESE_CHINA.UTF8&#x27;import timeimport jsonimport torch.nn as nnimport torchimport torchvision.transforms as transformsfrom PIL import Imagefrom matplotlib import pyplot as pltimport torchvision.models as modelsBASE_DIR = os.path.dirname(os.path.abspath(__file__))device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 导包。 最后一行device，如果cuda可用，使用GPU，如果不可用使用CPU。GPU：device:0 主函数1234567891011121314151617181920212223242526272829303132333435363738394041if __name__ == &quot;__main__&quot;: # config path_state_dict = os.path.join(BASE_DIR, &quot;..&quot;, &quot;data&quot;, &quot;alexnet-owt-4df8aa71.pth&quot;) path_img = os.path.join(BASE_DIR, &quot;..&quot;, &quot;data&quot;, &quot;Golden Retriever from baidu.jpg&quot;) # path_img = os.path.join(BASE_DIR, &quot;..&quot;, &quot;data&quot;, &quot;tiger cat.jpg&quot;) path_classnames = os.path.join(BASE_DIR, &quot;..&quot;, &quot;data&quot;, &quot;imagenet1000.json&quot;) path_classnames_cn = os.path.join(BASE_DIR, &quot;..&quot;, &quot;data&quot;, &quot;imagenet_classnames.txt&quot;) # load class names cls_n, cls_n_cn = load_class_names(path_classnames, path_classnames_cn) # 1/5 load img img_tensor, img_rgb = process_img(path_img) # 2/5 load model alexnet_model = get_model(path_state_dict, True) # 3/5 inference tensor --&gt; vector with torch.no_grad(): time_tic = time.time() outputs = alexnet_model(img_tensor) time_toc = time.time() # 4/5 index to class names _, pred_int = torch.max(outputs.data, 1) _, top5_idx = torch.topk(outputs.data, 5, dim=1) pred_idx = int(pred_int.cpu().numpy()) pred_str, pred_cn = cls_n[pred_idx], cls_n_cn[pred_idx] print(&quot;img: &#123;&#125; is: &#123;&#125;\\n&#123;&#125;&quot;.format(os.path.basename(path_img), pred_str, pred_cn)) print(&quot;time consuming:&#123;:.2f&#125;s&quot;.format(time_toc - time_tic)) # 5/5 visualization plt.imshow(img_rgb) plt.title(&quot;predict:&#123;&#125;&quot;.format(pred_str)) top5_num = top5_idx.cpu().numpy().squeeze() text_str = [cls_n[t] for t in top5_num] for idx in range(len(top5_num)): plt.text(5, 15+idx*30, &quot;top &#123;&#125;:&#123;&#125;&quot;.format(idx+1, text_str[idx]), bbox=dict(fc=&#x27;yellow&#x27;)) plt.show() alexnet网络模型需提前下载好，然后加载路径到path_state_dict，同时加载好path_img，需要测试的图片，下面是注释掉另一张测试图片。 path_classnames和path_classnames_cn对应我们想要输出图片的1000个类别，一个是json文件，英文版。_cn是中文版，txt文件 然后加载 load class names load img load model tensor –&gt; vector …… 其他函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def img_transform(img_rgb, transform=None): &quot;&quot;&quot; 将数据转换为模型读取的形式 :param img_rgb: PIL Image :param transform: torchvision.transform :return: tensor &quot;&quot;&quot; if transform is None: raise ValueError(&quot;找不到transform！必须有transform对img进行处理&quot;) img_t = transform(img_rgb) return img_tdef load_class_names(p_clsnames, p_clsnames_cn): &quot;&quot;&quot; 加载标签名 :param p_clsnames: :param p_clsnames_cn: :return: &quot;&quot;&quot; with open(p_clsnames, &quot;r&quot;) as f: class_names = json.load(f) with open(p_clsnames_cn, encoding=&#x27;UTF-8&#x27;) as f: # 设置文件对象 class_names_cn = f.readlines() return class_names, class_names_cndef get_model(path_state_dict, vis_model=False): &quot;&quot;&quot; 创建模型，加载参数 :param path_state_dict: :return: &quot;&quot;&quot; model = models.alexnet() pretrained_state_dict = torch.load(path_state_dict) model.load_state_dict(pretrained_state_dict) model.eval() if vis_model: from torchsummary import summary summary(model, input_size=(3, 224, 224), device=&quot;cpu&quot;) model.to(device) return modeldef process_img(path_img): # hard code norm_mean = [0.485, 0.456, 0.406] norm_std = [0.229, 0.224, 0.225] inference_transform = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop((224, 224)), transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std), ]) # path --&gt; img img_rgb = Image.open(path_img).convert(&#x27;RGB&#x27;) # img --&gt; tensor img_tensor = img_transform(img_rgb, inference_transform) img_tensor.unsqueeze_(0) # chw --&gt; bchw img_tensor = img_tensor.to(device) return img_tensor, img_rgb 输出结果 alexnet_visualizaton.py导包：略 12345678910import osimport torchimport torch.nn as nnfrom PIL import Imageimport torchvision.transforms as transformsfrom torch.utils.tensorboard import SummaryWriterimport torchvision.utils as vutilsimport torchvision.models as modelsBASE_DIR = os.path.dirname(os.path.abspath(__file__))device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 主函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970if __name__ == &quot;__main__&quot;: log_dir = os.path.join(BASE_DIR, &quot;..&quot;, &quot;results&quot;) # ----------------------------------- kernel visualization ----------------------------------- writer = SummaryWriter(log_dir=log_dir, filename_suffix=&quot;_kernel&quot;) # m1 # alexnet = models.alexnet(pretrained=True) # m2 path_state_dict = os.path.join(BASE_DIR, &quot;..&quot;, &quot;data&quot;, &quot;alexnet-owt-4df8aa71.pth&quot;) alexnet = models.alexnet() pretrained_state_dict = torch.load(path_state_dict) alexnet.load_state_dict(pretrained_state_dict) kernel_num = -1 vis_max = 1 for sub_module in alexnet.modules(): if not isinstance(sub_module, nn.Conv2d): continue kernel_num += 1 if kernel_num &gt; vis_max: break kernels = sub_module.weight c_out, c_int, k_h, k_w = tuple(kernels.shape) # 拆分channel for o_idx in range(c_out): kernel_idx = kernels[o_idx, :, :, :].unsqueeze(1) # 获得(3, h, w), 但是make_grid需要 BCHW，这里拓展C维度变为（3， 1， h, w） kernel_grid = vutils.make_grid(kernel_idx, normalize=True, scale_each=True, nrow=c_int) writer.add_image(&#x27;&#123;&#125;_Convlayer_split_in_channel&#x27;.format(kernel_num), kernel_grid, global_step=o_idx) kernel_all = kernels.view(-1, 3, k_h, k_w) # 3, h, w kernel_grid = vutils.make_grid(kernel_all, normalize=True, scale_each=True, nrow=8) # c, h, w writer.add_image(&#x27;&#123;&#125;_all&#x27;.format(kernel_num), kernel_grid, global_step=620) print(&quot;&#123;&#125;_convlayer shape:&#123;&#125;&quot;.format(kernel_num, tuple(kernels.shape))) # ----------------------------- feature map visualization ------------------------ writer = SummaryWriter(log_dir=log_dir, filename_suffix=&quot;_feature map&quot;) # 数据 path_img = os.path.join(BASE_DIR, &quot;..&quot;, &quot;data&quot;, &quot;tiger cat.jpg&quot;) # your path to image normMean = [0.49139968, 0.48215827, 0.44653124] normStd = [0.24703233, 0.24348505, 0.26158768] norm_transform = transforms.Normalize(normMean, normStd) img_transforms = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), norm_transform ]) img_pil = Image.open(path_img).convert(&#x27;RGB&#x27;) img_tensor = img_transforms(img_pil) img_tensor.unsqueeze_(0) # chw --&gt; bchw # 模型 # alexnet = models.alexnet(pretrained=True) # forward convlayer1 = alexnet.features[0] fmap_1 = convlayer1(img_tensor) # 预处理 fmap_1.transpose_(0, 1) # bchw=(1, 64, 55, 55) --&gt; (64, 1, 55, 55) fmap_1_grid = vutils.make_grid(fmap_1, normalize=True, scale_each=True, nrow=8) writer.add_image(&#x27;feature map in conv1&#x27;, fmap_1_grid, global_step=620) writer.close() train_alexnet.py这一部分是关于训练一个AlexNet实现一个二分类问题 首先代码结构12345graph TB构建DataLoader--&gt;构建模型构建模型--&gt;构建损失函数构建损失函数--&gt;构建优化器构建优化器--&gt;迭代训练 分为5个步骤。 关于train数据集25000张图片，猫狗各占一半，图片内容比较复杂，不仅仅只有猫和狗，有的图片有一些栅栏背景，有的图片有人，而且人的大小大于猫或狗大小。 我们这里只需要train这个文件夹。 导包 12345678910111213import osimport numpy as npimport torch.nn as nnimport torchfrom torch.utils.data import DataLoaderimport torchvision.transforms as transformsimport torch.optim as optimfrom matplotlib import pyplot as pltimport torchvision.models as modelsfrom tools.my_dataset import CatDogDatasetBASE_DIR = os.path.dirname(os.path.abspath(__file__))device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 最后一行代码意义和第一个.py含义一样，if available GPU else CPU 主函数12345678910111213# config data_dir = os.path.join(BASE_DIR, &quot;..&quot;, &quot;data&quot;, &quot;train&quot;) path_state_dict = os.path.join(BASE_DIR, &quot;..&quot;, &quot;data&quot;, &quot;alexnet-owt-4df8aa71.pth&quot;) num_classes = 2 MAX_EPOCH = 3 # 可自行修改 BATCH_SIZE = 128 # 可自行修改 LR = 0.001 # 可自行修改 log_interval = 1 # 可自行修改 val_interval = 1 # 可自行修改 classes = 2 start_epoch = -1 lr_decay_step = 1 # 可自行修改 主函数的config，超参数和路径可以自己设置 然后是5个模块，和第一个python文件5步骤对应 123456789101112131415161718192021222324252627# ============================ step 1/5 数据 ============================ norm_mean = [0.485, 0.456, 0.406] norm_std = [0.229, 0.224, 0.225] train_transform = transforms.Compose([ transforms.Resize((256)), # (256, 256) 区别 transforms.CenterCrop(256), transforms.RandomCrop(224), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std), ]) normalizes = transforms.Normalize(norm_mean, norm_std) valid_transform = transforms.Compose([ transforms.Resize((256, 256)), transforms.TenCrop(224, vertical_flip=False), transforms.Lambda(lambda crops: torch.stack([normalizes(transforms.ToTensor()(crop)) for crop in crops])), ]) # 构建MyDataset实例 train_data = CatDogDataset(data_dir=data_dir, mode=&quot;train&quot;, transform=train_transform) valid_data = CatDogDataset(data_dir=data_dir, mode=&quot;valid&quot;, transform=valid_transform) # 构建DataLoder train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True) valid_loader = DataLoader(dataset=valid_data, batch_size=4) 对数据模块进行设置 12345678910111213141516171819202122232425# ============================ step 2/5 模型 ==========================alexnet_model = get_model(path_state_dict, False)num_ftrs = alexnet_model.classifier._modules[&quot;6&quot;].in_featuresalexnet_model.classifier._modules[&quot;6&quot;] = nn.Linear(num_ftrs, num_classes)alexnet_model.to(device)# ============================ step 3/5 损失函数 =======================criterion = nn.CrossEntropyLoss()# ============================ step 4/5 优化器 =========================# 冻结卷积层flag = 0# flag = 1if flag: fc_params_id = list(map(id, alexnet_model.classifier.parameters())) # 返回的是parameters的 内存地址 base_params = filter(lambda p: id(p) not in fc_params_id, alexnet_model.parameters()) optimizer = optim.SGD([ &#123;&#x27;params&#x27;: base_params, &#x27;lr&#x27;: LR * 0.1&#125;, # 0 &#123;&#x27;params&#x27;: alexnet_model.classifier.parameters(), &#x27;lr&#x27;: LR&#125;], momentum=0.9)else: optimizer = optim.SGD(alexnet_model.parameters(), lr=LR, momentum=0.9) # 选择优化器scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=0.1) # 设置学习率下降策略# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(patience=5) 对模型模块进行设置，加载模型，修改它的输出层，两个类的一个分类。 设置损失函数和优化器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# ============================ step 5/5 训练 ============================ train_curve = list() valid_curve = list() for epoch in range(start_epoch + 1, MAX_EPOCH): loss_mean = 0. correct = 0. total = 0. alexnet_model.train() for i, data in enumerate(train_loader): # if i &gt; 1: # break # forward inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) outputs = alexnet_model(inputs) # backward optimizer.zero_grad() loss = criterion(outputs, labels) loss.backward() # update weights optimizer.step() # 统计分类情况 _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).squeeze().cpu().sum().numpy() # 打印训练信息 loss_mean += loss.item() train_curve.append(loss.item()) if (i+1) % log_interval == 0: loss_mean = loss_mean / log_interval print(&quot;Training:Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Loss: &#123;:.4f&#125; Acc:&#123;:.2%&#125;&quot;.format( epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total)) loss_mean = 0. scheduler.step() # 更新学习率 # validate the model if (epoch+1) % val_interval == 0: correct_val = 0. total_val = 0. loss_val = 0. alexnet_model.eval() with torch.no_grad(): for j, data in enumerate(valid_loader): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) bs, ncrops, c, h, w = inputs.size() # [4, 10, 3, 224, 224 outputs = alexnet_model(inputs.view(-1, c, h, w)) outputs_avg = outputs.view(bs, ncrops, -1).mean(1) loss = criterion(outputs_avg, labels) _, predicted = torch.max(outputs_avg.data, 1) total_val += labels.size(0) correct_val += (predicted == labels).squeeze().cpu().sum().numpy() loss_val += loss.item() loss_val_mean = loss_val/len(valid_loader) valid_curve.append(loss_val_mean) print(&quot;Valid:\\t Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Loss: &#123;:.4f&#125; Acc:&#123;:.2%&#125;&quot;.format( epoch, MAX_EPOCH, j+1, len(valid_loader), loss_val_mean, correct_val / total_val)) alexnet_model.train() train_x = range(len(train_curve)) train_y = train_curve train_iters = len(train_loader) valid_x = np.arange(1, len(valid_curve)+1) * train_iters*val_interval # 由于valid中记录的是epochloss，需要对记录点进行转换到iterations valid_y = valid_curve plt.plot(train_x, train_y, label=&#x27;Train&#x27;) plt.plot(valid_x, valid_y, label=&#x27;Valid&#x27;) plt.legend(loc=&#x27;upper right&#x27;) plt.ylabel(&#x27;loss value&#x27;) plt.xlabel(&#x27;Iteration&#x27;) plt.show() 训练结果只设置了3个epoch（0,1,2），差不多10分钟左右 训练验证曲线 完。 AlexNet应用于其他数据集花分类与蚂蚁蜜蜂的分类windows下nvidia-smi.exe -l 5(每5秒更新一次gpu的信息，更新频率最高可设置为1s/次，ctrl+c停止更新) 花分类训练结果 预测结果 蚂蚁与蜜蜂分类 预测结果","path":"2021/08/05/AlexNet代码复现/","date":"08-05","excerpt":"","tags":[{"name":"神经网络模型","slug":"神经网络模型","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}]},{"title":"markdown基本语法-Typora","text":"Markdown段落和换行 实例如下 &emsp;&emsp;我是首句 &ensp;&ensp;我是中间 我是结尾 换行​ Typora 一直是我认为桌面端笔记应用应有的终极形态。用我之前 一篇文章 中的话来说就是，它的功能之强大、设计之冷静、体验之美妙、理念之先进，我认为值得所有笔记应用厂商学习。&emsp;&emsp; 但一件很尴尬的事情是，由于它极简的设计理念，有许多使用者并没能完全地了解到 Typora 的全部强大功能。我想在这篇文章中由浅入深地介绍 Typora 的功能亮点。无论你从未用过 Typora，还是已经体验了很久，我相信你都能在这篇文章中发现 Typora 新的惊喜。 标题错误写法 =======This is an H1 =============================== ——This is an H2 ———- 正确写法 我是主题我是副主题这是一种类Setex的写法 下面是类atx的写法 常用：1到6个# 不写了 引用 &emsp;&emsp;Markdown 是用来编写结构化文档的一种纯文本格式，它使我们在双手不离开键盘的情况下，可以对文本进行一定程度的格式排版。你可以在 这篇文章 中快速入门 Markdown。 我是里面的标题 每加一个&gt; 的效果 &emsp;&emsp;由于目前还没有一个权威机构对 Markdown 的语法进行规范，各应用厂商制作时遵循的 Markdown 语法也是不尽相同的。其中比较受到认可的是 GFM 标准，它是由著名代码托管网站 GitHub 所制定的。Typora 主要使用的也是 GFM 标准。同时，你还可以在 文件 - 偏好设置 - Markdown 语法偏好 - 严格模式 中将标准设置为「更严格地遵循 GFM 标准」。具体内容你可以在官方的 这篇文档 中查看。 列表无序展示注意*、+、-后面加空格 语文 数学 英语 物理 化学 生物 历史 地理 政治 有序展示 努力 学习 运用 你好 你是谁 你是哪里人 代码区块1234567891011121314151617class PartTimeEmployee extends Employee &#123; private double hourlyPay; public PartTimeEmployee(String na,String nu,double h) &#123; super(na,nu); hourlyPay=h; &#125; public void setHourlyPay(double pay) &#123; hourlyPay=pay; &#125; public double calculateWeeklyPay(int hour) &#123; return hourlyPay*hour; &#125;&#125; 1这是一个代码块 123456&lt;script&gt; var option=function()&#123; //调用后台请求 //回显结果 &#125;&lt;script&gt; 分割线分割线之前 +++ 分割线之后 链接行内式这是一个例子 放上去有提示。 zw 参考式这是一个示例 强调全体注意 全体注意 全体注意 ==全体注意== 一句代码块Java中输出语句 System.out.println(m) 图片略 也是两种格式：行内式与参考式 反斜杠全体注意 *全体注意* \\ ! \\ 自动链接转换http://baidu.com &#49;&#x33;&#x32;&#x30;&#x35;&#51;&#57;&#50;&#x31;&#x31;&#x40;&#x71;&#113;&#46;&#99;&#x6f;&#x6d;","path":"2021/08/03/markdown基本语法-Typora/","date":"08-03","excerpt":"","tags":[]},{"title":"VGGNet","text":"第一张ppt 首先介绍一下VGG是什么… 论文题目：应用于大规模图像识别的非常深的卷积网络。 VGGNet最主要的目标是试图回答“如何设计网络结构”的问题。随着AlexNet提出，很多人开始利用卷积神经网络来解决图像识别的问题。一般的做法都是重复几层卷积网络，每个卷积网络之后接一些池化层，最后再加上几个全连接层。而VGGNet的提出，给这些结构设计带来了一些标准参考。 虽然VGGNet是比较老，14年提出，但是现在还在用，比如说SSD 使用单个深度神经网络来检测图像中的目标的方法，是目标检测中应用最广泛的算法，前端使用VGG16网络， 所以我们需要了解一下。 https://blog.csdn.net/u013044310/article/details/89380273 贡献： 第二张ppt 论文中，作者的主要目标是探索深度学习深度的影响。作者固定了网络中其他的参数，通过缓慢的增加网络的深度来探索网络的效果。这个表格里给了我们6个VGG网络的配置， 有11层、13层、16层和19层、还有这里是否是用LRN，这种规范化并不能提高在ILSVRC数据集上的性能，我们常用的是VGG16、16是怎么来的，13个卷积层和3个全连接层。我们观察表格，A和A-LRN都是11层，这个使用了LRN，C和D的区别一个卷积核是11，一个是33。 VGGNet最重要的贡献证明了分类任务可以通过使用小的卷积核增加CNN的深度来提高精度。 第三张ppt 这一部分主要是讲如何通过堆叠两个3×3的卷积核来替代大尺度的卷积核，先要了解一个感受野的感念。 通俗的解释是，输出feature map上的一个单元对应输入层上的区域大小。我们来看这样一个例子，在我们左边，最下面呢是一个991的一个特征矩阵，首先我们通过一个卷积层1、它的大小是33，步长为2，根据公式我们会得到1个441的这么一个大小，然后在经过一个最大池化下采样的一个操作，它所得到的特征层大小是221。所以在我们第三层中一个单元对应到我们第二层的感受野就是一个22的一个区域，对应到我们原图的感受野就是一个55的区域，那这个22与5*5怎么计算呢 我们把第三层的一个单元带入公式进行计算，感受野是从后往前推算的，所以说我们第三层的一个单元在原图当中有5*5这样一个感受野 VGG网络中Stride默认是等于1的。我们假设现在一个特征矩阵经过3层33的卷积得到一个feature map，那么feature map上的一个单元所对应我们上一层的感受野，那就是1-11+3等于3，我们在计算上一层的3-11+3 =5，我们在计算上一层5-11+3=7。也就是说我们经过3层33的卷积得到的特征图上的一个单元对应的感受野就是77的大小，那也就和我们采用一个7*7的一个卷积核是一样的。 那么我们这样做的目的是就是为了减少参数、这两个C分别的代表卷积核的深度以及个数 很明显我们的参数减少了很多。 第四张ppt 我们通常使用D这个模型 首先呢，我们的输入图像是2242243这样一个RGB图像，然后我们通过两层3*3的一个卷积层，然后maxpool最大下采样……然后在经过3个FC层，然后在经过一个softmax处理，就得到我们的一个概率分布了。那么我们这个表中并没有给出参数计算，所以这里简单介绍一下、卷积层stride=1、padding=1、我们带入公式、-Fsize 也就是-3 +2P +2 /1不用考虑也就是-1 再加1 化简也就是 卷积层中outsize = input size 也就是经过卷积输出尺寸和输入尺寸是一样的。池化我们代入计算，-2 + 这里P=0，然后除以2 +1 约掉之后相当于除以2。 结果是输出的特征图宽和高缩减为原来的一半。 这样我们来看这幅图 黑色框…红色框 第一个卷积操作，我们进行了两个33的卷积层之后呢，它所得到的特征图大小是224224，因为我们之前说了根据公式，outsize=inputsize，我们采用的卷积核的个数是64，那么我们输出的深度也就是64，然后是maxpool最大下采样操作，之前说过根据公式、相当于缩减一半、得到了112112，池化是不改变我们的深度的，我们注意这里红色框的厚度和前面卷积层的厚度一样，所以我们得到的是11211264、图上标的这个112112*120是下一个卷积操作的卷积层的具体值，紧接着下一个卷积操作。 最后我们需要注意的是最后一个全连接层是不需要加上ReLU激活函数的，我们需要加softmax激活函数将我们的预测结果转化为一个概率分布。 第五张ppt ​ 德国有一个激光切割领域的博士，他把VGG做了一个改进，他用最后的全连接输出直接回归了所有参数。就像我们把激光切割的一张图片喂到模型里面，它会预测出你用的是什么样的激光切割的工艺量。比如说焦点深度，进给量，保护气的气压等等。而目前的话人类是做不到这一点。 ​ 他改进的VGG模型，预测出激光切割的一个工艺参数，并且远远超过了人类专家的水平。 ​ 第二个贡献是：用已有的可视化的方法，类似于我们热力图的方法，标记出了预测值贡献最大的区域，他向专家和工人们展示了这样的结果，提高了专家和工人们预测参数的能力，这就不仅仅是人教机器学习了，不是machine learning 而是machine teaching ​ 所以总结来说他就是用了自己领域的数据集，改动了一下VGG，训练了一个远强于人类专家的网络，并且还让这个网络教会了人提高自己的技能。 ​ 我们看到这个人写了很多激光加工领域的论文，因为做激光加工领域的人不懂计算机视觉，而真正内卷计算机视觉的人他不懂激光切割。所以当我们学会了这个技能是可以借助这种信息不对称，用计算机视觉和AI赋能到我们自己的行业，这是我觉得目前AI领域所能为社会创造价值的途径，如果我们只是局限于改改trick，调参，炼丹，为了kaggle上的一个竞赛去刷榜，比别人高个一些精确度而努力，这些呢我觉得对于我来说可能也是一个需要经历的过程，但意义不大。而我们把这些技术应用于自己的垂直领域，垂直行业，甚至说之前看上去跟CV或者是计算机无关的领域。 ​ 现在CV发展非常的快，这些算法也大大降低了AI开发的水平面，这个水平面降下来以后呢很多领域都可以使用人工智能来达到该领域最优的性能，这也是摆脱内卷的一个途径。现在人工智能各种开发的设施都已完备，数据集，算力，算法都已完备了，缺的就是能够为各行各业有实际落地应用价值的人才。 软件开发领域的人工智能需求分析​ 作为软件生命周期（Software Development Life Cycle，即SDLC）中的一个概念梳理环节，收集客户需求很大程度上依赖人工介入。 ​ 对此，AI技术提供了一些技术工具，譬如的Google ML Kit和Infosys Nia，它们可以通过AI使某些过程实现自动化，从而在一定程度上将人为干预最小化。 ​ 在这个阶段中可以将设计前的漏洞检测纳入工作流程。一种称为“自然语言处理”（Natural Language Processing）的AI技术通过让机器理解自然语言去了解用户需求，并产出高级软件模型，当然，这种方法存在一些问题，包括难以在已开发的系统间作出平衡，但它仍旧是当今的热门研究主题之一。 ​ 项目的规划和设计需要依靠大量的经验积累来提出一个明确的解决方案。 ​ 设计工作每个阶段都有一个“共同认可”的结果，其实这对设计师来说是非常困难的，现实是设计师通常需要多次来回修改方案，直到客户满意为止。如果借助AI自动执行一些复杂的程序来满足客户需求，以此方法来产出方案，将会大大提高效率。 代码自动生成​ 在软件开发启动前，我们通常要去理解一个无比巨大的项目的实际目标以及需求方的各种细碎想法，这往往非常耗时，其间的大量沟通可以视作一种“重复性工作”。 ​ 为了节省时间和金钱，专家们提出了一种在开始开发启动前先编写部分代码的解决方案，不过，这种方法存在很多不确定性，譬如，该前置编写代码阶段也需要拿到一些信息，如果此时信息仍不清晰，这样的前置工作是无效的。 ​ 这时通过人工智能介入信息收集，即，通过自然语言把思路告知智能系统，这样的前置风险就会被大大降低，因为智能系统效率高于人工，同时不会去“抱怨”客观条件下的小小弯路。 测试服务中的AI​ 软件测试是软件开发中的关键阶段，一定程度上可保证软件产品的质量，但如果每次更改源代码时都需要重复测试，既费时又费钱。 ​ AI又可以在这个阶段起到作用——目前已有部分工具可使用AI创建测试用例并执行回归测试，这些AI工具可以自动化测试，并进一步确保测试无错误。","path":"2021/08/02/VGGNet/","date":"08-02","excerpt":"","tags":[{"name":"神经网络模型","slug":"神经网络模型","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}]},{"title":"AlexNet","text":"内容以PPT形式展示第一张ppt 论文题目是基于深度卷积神经网络的图像网络分类 首先说明ImageNet和ILSVRC-2012是不同的两个概念，ImageNet项目是一个用于视觉对象识别软件研究的大型可视化数据库。有14,197,122 张图片, 21841 多个类别。 ILSVRC-2012是一个大型的挑战赛，从ImageNet中挑选1000类的1200000张作为训练集。 第二张ppt ILSVRC有一个分类任务，对于每张图像，算法将按置信度的降序生成最多 5 个对象类别的列表。将根据与图像的真实标签最匹配的标签来评估标签的质量。 这5个类别就是我们计算TOP-5error的数据 为什么： 这个想法是允许算法识别图像中的多个对象，并且如果识别出的对象之一确实存在，但不包含在基本事实中，则不会受到惩罚。 如果这5个类别我们有一个分类正确了，就不会惩罚我们的模型，继续优化。 第三张ppt 1CNN︰训练一个AlexNet 5CNNs:训练五个AlexNet取平均值 1CNN*在最后一个池化层之后，额外添加第六个卷积层,并使用lmageNet 2011(秋）数据集上预训练 7CNNs*两个预训练模型微调，与5CNNs取平均值 第四张ppt 包含5个卷积层（某些卷积层后面带有池化层）和3个全连接层，网络的输入为150,528，224×224×3 网络剩余层中的神经元数量由253,440–186,624–64,896–64,896–43,264–4096–4096–1000. 从图片当中，从原始的RGB，224 × 224，经过一个卷积，得到了一个55×55的特征图，经历一个Relu，经历池化，然后就变成了后面这个27，并且加有一个LRN这个操作 公式含义：输入特征图的大小 – kernel size(卷积核的大小) + 2p p是padding 也就是像素，有多少个pixel。除以stride 也就是步长 这里227与上面图的224是有所不同的，其实在早期的Alexnet网络结构图当中，它就是227，为什么227这里经过卷积是55，论文当中224卷积也是55呢。我们算一下，（227-11）/4=55 。而224 计算的话 224-11 213 除以4 是53余1 向下取整再加1是54，在pytorch中，实现卷积是向下取整的。加两个像素，224 -11 +2×2 / 4 在加1 ​ 然后我们得到了55×55，那有多少通道呢，就是有多少个卷积核就有多少个通道，也可以用公式这样算，K这里是池化窗的大小，算完经过第一个池化，特征图变为了27×27，通道数不变，再往下也是重复的卷积池化卷积池化，在全连接层前面画一条线，通常我们也会在这里切一刀，认为前面的这一系列操作就是对原始图像的特征提取，最后的这个就是特征，进入全连接层认为这是分类。 第五张ppt 第一列：名称 第二列 卷积核的数量 第三列padding 第四列：卷积核大小 第五列：步长 第六列：img size我们之前算的特征图的高和宽 ​ 公式解释：Fi 我们输入图片的通道数 Ks kernel size 卷积核的大小 Kn 卷积核的数量 也就是输出的通道数+ 偏置 ​ 我们可以发现总共有6000多万个参数，第一个FC层（全连接层）3700多万，大于百分之50，这也就是我们后面的神经网络不太爱使用这个FC层，因为实在是太占我们的内存了，占我们的参数。 第六张ppt[ 这个图意思是他们做了一个实验去比较使用relu和使用tanh当达到0.25这个error的时候的一个速度比较，relu可以加快神经网络的训练，从这个图上可以看出差不多6倍左右。 受到真实的神经元侧抑制的启发。 这个抑制信号我们需要注意一下，这个公式想实现的功能就是抑制。 等号左边就是经过LRN之后每个神经元的激活值，分子是原始神经元的一个激活值。所以呢我们可以看到原始神经元a，经过LRN后呢是神经元b。i代表当前的通道channel，x，y代表像素的位置。我们想如果分母越大，那么b就会越小，这样就反映出对神经元一个抑制的效果，k是一个超参数，常数，α和β都是常数，由原型中的alpha和belta指定、论文也有介绍，我们不关心这个，我们来看这个Σ，这个Σ值越大，抑制效果就越强。 a代表的是feature map里面的i对应像素的具体值，j是平方累加索引，我们来关注j的取值，j=max(0,i-n/2)，它是从0开始的，为了防止它超出别界，0，N-1是为了防止计算超过这个特征图，我们来看这个i-二分之n和i+二分之n，在图上i-二分之n代表往左考虑二分之n个通道，i+二分之n代表往右考虑二分之n个通道，也就是表示它有一个范围，这个范围就表示对应的我们真实神经元这个周围，这个周围是通过n来体现的。 第四个神经元的值非常大，我们假如说是100，第5个是5，那么我们第三个神经元计算的时候，我们看公式，这个分母就会被这个100所控制了，也就是被抑制了，由于这个神经元的值非常大，它就抑制了周围的神经元的输出。 我们再来看这个公式，它要看分母，分母当中它要考虑周围的一些神经元，那这些神经元当中如果有很大的值，他就会抑制，这就是由侧抑制启发得来的一个公式。 第七张ppt 我们常常会遇到数据不足的情况。比如，你遇到的一个任务，目前只有小几百的数据，然而，你知道目前现在流行的最先进的神经网络都是成千上万的图片数据。 Hinton说他的灵感之一来自于银行的防欺诈机制。用他自己的话来说：“我去银行办理业务。柜员不停地换人，于是我问其中一人这是为什么。他说他不知道，但他们经常换来换去。我猜想，银行工作人员要想成功欺诈银行，他们之间要互相合作才行。这让我意识到，在每个样本中随机删除不同的部分神经元，可以阻止它们的阴谋，因此可以降低过拟合。” 其核心思想是在层的输出值中引入噪声，打破不显著的偶然模式（Hinton 称之为阴谋）。如果没有噪声的话，网络将会记住这些偶然模式。 第八张ppt 上面是48个下面是48个，上面是GPU1，下面是GPU2,上面是不同的方向，不同的频率，下面呈现的是不同的色彩，更多的是颜色块，而且上下基本上是不重叠的，不同的GPU有不同的选择，这有助于我们理解卷积核学习了什么内容，我们一提到神经网络，通常会想到这是一个黑盒子，Alexnet一共有5个卷积层，为什么第一个这个卷积层能够可视化呢？ 两个原因：第一个是kernels size的原因 11×11 比较大，它可视化出来还能看出一些内容，121个像素，后面的卷积核5*×5 3×3 3×3只有9个像素，我们几乎看不出什么模式的，这也就是为什么没有可视化后面的卷积核，可视化出来也分析不出什么东西 第二个原因是关于特征，因为我们知道卷积神经网络越往后，特征越高级，越抽象的，我们人类的眼睛很难去识别它，那么我们第一个卷积层是最底层的最低级的特征，最接近人类观察到原始图片上的一些信息的，也就是这些颜色色彩纹理边缘，符合特征由低级到高级这么一个过程。 第九张ppt","path":"2021/08/02/AlexNet/","date":"08-02","excerpt":"","tags":[{"name":"神经网络模型","slug":"神经网络模型","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"}]},{"title":"Spring","text":"Spring基础 spring全家桶：Spring springmvc spring boot spring cloud spring: 解决企业开发的难度，减轻对项目模块之间的管理，类和类之间的管理，帮助开发人员创建对象，管理对象之间的关系 核心技术： IOC aop 实现模块之间，类之间的解耦合 依赖： class a中使用class b的属性或方法，叫做class a 依赖 class b maven 管理的是项目中需要的jar包 管理模块 管理资源（项目之前） Spring管理的是项目之中写的类，模块（项目之中） (1) 轻量 运行占用的资源少，运行效率高，不依赖其它jar(2) 针对接口编程，解耦合(3) AOP编程的指出(4) 方便集成各种优秀框架 容器：存的是java对象 mybatis– 访问数据库，对表中的数据执行增删改查 框架要完成一个功能，需要一定的步骤支持的。框架内部怎么做，原理是什么自定义框架 IOC是一种理论、概念、思想。 把对象的创建，赋值，管理工作都交给代码之外的容器实现，也就是对象的创建是由其他的外部资源来完成。 控制： 创建对象，对象的属性赋值，对象之间的关系管理反转： 把原来的开发人员管理，创建对象的权限转移给代码之外的容器实现。由容器代替开发人员管理对象。创建对象，给属性赋值正转： 由开发人员在代码中，使用new构造方法创建对象，开发人员主动管理对象。 Student student = new Student();//在代码中，创建对象 –正转 为什么使用IOC： 减少对代码的改动，也能实现不同的功能（实现解耦合） java中创建对象有哪些方式：1、构造方法 , new Student()2、反射3、序列化4、克隆5、ioc：容器创建对象6、动态代理 IOC不需要在你的程序里写代码就可以获取对象 以往学过的内容中，IOC思想存在、体现？ servlet 1、创建类继承HttpServlet 2、在web.xml 注册servlet, 使用myservlet 3、没有创建servlet对象，没有MyServlet myservlet = new MyServlet() 4、Servlet 是Tomcat服务器它为你创建的 Tomcat也称为容器，里面存放的有servlet对象，Listener，Filter 等 IOC的技术实现： DI 是IOC的技术实现， DI ：依赖注入 ， 我们只需要在程序中提供要使用的对象名称就可以，至于对象如何在容器中创建，赋值，查找都由容器内部实现。 spring是使用的di实现了IOC的功能，spring底层创建对象，使用的是反射机制 总结：spring是一个容器，管理对象，给属性赋值，底层是反射创建对象","path":"2021/04/06/Spring/","date":"04-06","excerpt":"","tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}]},{"title":"机器学习第二章","text":"第二章模型评估与选择 一种训练集一种算法2.1经验误差与过拟合m 样本数量 Y 样本正确的结果 错误率 精度 误差 2.2评估方法训练集验证集与测试集测试集的保留方法：留出法 37分 28分 交叉验证法 K折交叉验证一个训练集 一个测试集 对应结果 然后取平均 来作为这个模型的衡量标准 自助法 自助采样的定义：给定包含 m 个样本的数据集 D，我们对它进行采样产生的数据集 D’ ：每次随机从 D 中挑选一个样本，将其拷贝放入 D’ 中，然后再将该样本放回初始数据集 D 中，使得该样本在下次采样时仍有可能被采到；重复这个过程 m 次，我们就得到了包含 m 个样本的数据集 D’，这就是自助采样的结果。 在初始数据集D中， m 次采样中始终不被采到的概率是 36.8% ，将 D’ 作为训练集，将 D \\ D’用作测试集。 优点：自助法在数据集较小、难以有效划分训练/测试集时很有用缺点：由于产生的数据集改变了初始数据集的分布，会引入估计偏差，因此初始数据量足够时，留出法和交叉验证发更常用一些。","path":"2021/04/05/机器学习第二章/","date":"04-05","excerpt":"第二章模型评估与选择","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"机器学习第一章","text":"第一章 1.1基本术语有了数据 通过某种学习算法 得到模型 有监督（分类和回归）无监督（聚类） 进行预测 测试 测试样本 泛化能力（它能预测它没见过的数据的能力） 1.2假设空间科学的推理手段 归纳 演绎 … 1.3归纳偏好同一个数据集训练出了不同的模型，如何选择模型 原则 奥卡姆剃刀（选择最简单的那个） 1.4发展历程一种程序，有自我改善的能力，人为干预越少越好","path":"2021/04/05/机器学习第一章/","date":"04-05","excerpt":"第一章","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"调研报告","text":"关于图像识别、CV、视频理解的调研报告 人工智能（即AI）是一个很大的体系，有远不止机器学习这个理论的理论体系，有远不止计算机视觉这个应用的应用领域。如今依然是弱AI百花齐放的时代，一句话解释：阿尔法狗是不认识狗的。当强AI的时代来临，AI将变成人类这种通才，但是又可以兼具阿尔法狗的围棋能力，又兼具深蓝的博学多才。我们可以把人工智能这个庞然大物看作一个超级生命，下面来看一下它的生存能力图： Computer Vision（计算机视觉）和 Natural Language Processing （自然语言处理）一直是两个独立的研究方向。计算机视觉是一门研究如何使机器“看”的科学，而自然语言处理是人工智能和语言学领域的分支学科，主要探索的是如何使机器“读”和“写”的科学。它们相通的地方是，都需要用到很多机器学习，模式识别等技术，同时，它们也都受益于近几年的深度神经网络的进步，可以说这两个领域目前的 state-of-art，都是基于神经网络的，而且很多任务，比如 CV 里的物体识别检测，NLP 里的机器翻译，都已经达到了可以实用的程度。于是从 2015 年开始，有一个趋势就是将视觉与语言进行一定程度的结合，从而产生出一些新的应用与挑战。比如 image captioning，visual question answering（视觉问答） 等比较经典的 vision-and-language 任务。在经典的 vision-language 任务上，比如 image captioning 和 VQA（Visual Question Answering），能够增长的空间已经很小，已经过了暴力的通过数据去学习的阶段。真正的挑战其实是一些细分的领域，比如多样性、可控性、推理以及如何将 vision-language 应用在真实的场景当中。结合计算机视觉、机器人领域5大顶会（CVPR/ICCV/IROS/ICRA/ECCV），以及产业界的需求，现有3个当下热门及前沿的研究领域：1、三维视觉三维视觉是传统的研究领域，但最近5年内得到快速发展。三维视觉主要研究内容有：三维感知（点云获取及处理）、位姿估计（视觉SLAM）、三维重建（大规模场景的三维重建、动态三维重建）、三维理解（三维物体的识别、检测及分割等）。2、视频理解随着新型网络媒体的出现，以及5G时代的到来，视频呈现爆炸式增长，已成为移动互联网最主要的内容形式。面对于海量的视频信息，仅靠人工处理是无法完成的，因此实现视频的智能化理解则成为了亟待解决的问题。自2012年，深度学习在图像理解的问题上取得了较大的突破，但视觉理解比图像的目标检测识别要复杂的多。这是因为视频常有许多动作，动作往往是一个复杂概念的集合，可以是简单的行为，但也可能是带有复杂的情绪、意图。举个简单的例子，对一段视频分类，与对一幅图像分类，哪个更容易一些？从最近几年知名的计算机视觉竞赛，也可以看出，图像层面的竞赛在减少，视频层面的竞赛在增加。3、多模态融合多模态融合的知识获取是指从文本、图片、视频、音频等不同模态数据中交叉融合获取知识的过程。随着计算机视觉越来越成熟之后，有一些计算机视觉解决不了的问题慢慢就会更多地依赖于多个传感器之间的相互保护和融合。 关于CV和实际工作需求方面的调研：1、本人机器人&amp;无人驾驶研发从业两年，个人感觉这个CV行业供求关系是不平衡的。其次发文章不代表你的工程和产品能力好，不发文章也不能证明你的工程能力差。所以我的建议是在学习期间先把技能掌握好，比如CV的基本算法。从基本的图像处理开始，二值化–光留—霍夫变换识别，roi–各种算子直到CNN网络 —-2018年2、开始一直执迷于算法工作，其实我觉得我们这个做算法做好的都是一些创业型公司。 后来又担心去了公司做“调参侠”，最后的最后放弃了算法，改做研发了。我知道别人也有这样情况的，跟我想的差不多，最后去做了后台研发。这样的话不应该在这说，可能会给大家带不好的头【但其实我自己也不清楚去公司是不是去调参，我也只是看网上有人说，又看了某些人的微博动态罢了】 但是一直又放不下，感觉学的还不错，但是要全部丢掉了，舍不得。 最重要的是很多人说人工智能是未来的趋势，可能我们会被迷惑，也不能这样说，好像也确实是趋势，反正我是蛮纠结的，目前的定位不是算法。希望你也可以想清楚吧。 —2016年3、面试造航母, 工作拧螺丝。4、个人觉得人才并没有过剩，甚至还短缺。若干人以为学过cs231n等一些课，然后会import一些库或者git clone，就声称自己熟悉相关领域了。很多人对自己所做的任务也没有很好地调研，比如不知道目标检测应该分成one stage和two stage的方法，具体这些方法有什么异同，同类方法有什么不同之处。个人觉得还是应该静下心来，读点论文，至少知道领域上下几十年，然后写点代码，做点项目，成为一个有用的人吧。另外目前CV领域还有很多问题可以去解决，比如三维视觉，虽然这个领域还会牵扯到图形学了，但是毕竟这个世界还是三维的。还有比如visual reasoning，做一些带推理的VQA或者caption。再比如Person Re-ID、Activity Recognition等等。总之，人才大多情况下不会过剩，有些只是虚假繁荣吧，要想立足还是必须要有真才实学的，不然多读读书、写写代码。 —2019年 5、(1)先说最熟悉的CV， CV大公司和小公司差距非常大，基本上被旷视，云从，百度等垄断。他们不光垄断了市场，垄断了技术，甚至还垄断了高校，云从+南洋理工，旷视+西交，南大，港科大，上科大，百度更不用说，吴恩达等都曾在职过。巨头拥有的海量数据和资本支撑再配合上高校恐怖的科研能力(一个实验室一堆免费的研究生搬砖)。有几家公司能抗衡?更别提现在就连高校都快被抢完了，我曾在职的公司就勉强争取到了上海某985高校的合作。还有大量没钱没资源的公司根本拿不到合作。这些都导致了CV领域被巨头占领，普通公司生存环境越来越恶劣，大部分已经跑到工业领域降维打击去了。企业的情况基本就是研发岗位的现状，想进小厂，有个硕士文凭，做过视觉相关就能进，但是，要想搞出像样的算法，非大厂不可，而大厂的算法都有超级大牛们带头，应届生去也是搬砖，不过提升应该很快，搬几年砖空降某些原来做视频方面的厂搞算法，基本就是带头大哥。(2)音频，我曾入职一家专门做音频和nlp的创业公司，大牛非常多，硕士，博士博后一大把。问题是没数据，没落地方案。国内专业做音频的也就讯飞和一堆xx精灵，xx同学，市场不大，暂时还在开拓应用场景，做的人也比较少。基本都要结合nlp做语音识别（感觉还不如搞nlp)(3)Nlp，范围挺大，手机助手，xx精灵，xx搜索，xx翻译，知识图谱等等都要用。算是Al比较高逼格的领域，也是近期热门，顶会paper也很多，取得突破的话应该能使人工智能往前迈进一大步。喜欢研究的话可以深耕这个领域。(4)推荐系统，这个方向是我现在选定的方向，就业面很广，kaggle上的比赛基本都能打，广告推荐，用户画像基本各个互联网公司都要用。门槛比较低，毕竟机器学习入门就是线性回归。(5)金融，了解不多，据业内前辈说是一个比较独立的领域，金融互联网行业是现在最赚钱的行业，想来以后待遇不会差。 —2019年 关于个人想研究的方向（学习规划）： 首先，个人看来要先学习的内容是：课程方面：1、数据分析机器学习的主要前提是数据分析，数据分析是完成工作所需的第一项技能。 2、Coursera（大型公开在线课程项目）上一门免费的机器学习课程 一边跟着吴恩达的课，一边看李沐大佬的动手学深度学习。 3、神经网络、卷积神经网络卷积神经网络，也就是convolutional neural networks （简称CNN），现在已经被用来应用于各个领域，物体分割、风格转换、自动上色，但是，CNN真正能做的，只是起到一个特征提取器的作用！所以这些应用，都是建立在CNN对图像进行特征提取的基础上进行的。 卷积神经网络 (LeCun 等, 1998) 作为深度学习的代表性网络，在图像识别、目标检测等方面取得了巨大的成功。卷积神经网络通过使用可训练的滤波器和特征池化操作的层次结构，自动学习视觉目标识别任务所需的复杂特征，从而实现比手工制作特征卓越的性能。直接将原始图像输入网络便可得到特征，传统算法往往需要复杂的特征设计以及统计策略，而现在只需要设计网络结构和选择训练超参数。计算机视觉领域，研究深度学习已经成为这几年的主流。 –摘自《基于深度学习的视频识别研究》 第一学期打好基础，“小布慢跑”，学一个算法要学透（公式推导+代码编写）。学好理论基础,磨刀不误砍柴功。 工具方面：1、MatlabMathWork公司的Matlab软件可以说是算法研究的利器，它的强大之处在于其方便快捷的矩阵运算能力和图形仿真能力。做图像处理方面的研究，Matlab是必须掌握的，而且是熟练掌握。当你有一些想法需要验证时，最好明智的先用matlab编写出来测试。如果你上来就用看似高大上的C++来实验，不仅错误BUG一大堆，到头来可能效果还不佳，就算效果好，时间也会耽搁不少，毕竟算法开发还是要快的，这样才能赶在别人之前发论文。总之，只要是接触图像算法，终究逃不过Matlab。 2、OpenCVOpencv是Intel公司开发的C++图像处理工具包，形象的理解为就是C++版的Matlab。当初Intel公司开发这个工具包的初衷也是方便大家共享，希望大家能够在一个共同架构的基础上共同建造摩天大楼，而不是各自在自己的地基上盖平房。与Matlab不同，Opencv是面向开发的，稳定性好，异常处理机制周全，但有一点需要注意，由于Opencv是开源的，那么如果你在项目中直接调用了它的API，那就意味着你的项目也必须开源。因此在真正的产品开发过程中，往往需要从Opencv库里面挖代码，而不是直接调用。3、PythonPython在图像处理算法方面除了其自身简洁的编程优势外，还得益于两个重要的Python类库——Numpy和Theano。Numpy是Python的线性代数库，对于矩阵运算能提供很好的支持，并且能够在此基础上进行很多机器学习相关算法的开发仿真。 最后是关于研究方向：图像分割 图像分割分为语义分割和实例分割两种方式，语义分割是根据图像的语义信息来进行分割，在无人驾驶系统中主要用于可通行区域检测、车道线检测等。实例分割是在语义分割的基础上提出来的新概念，不仅可以预测像素所属类别，还能够区别同一类别的不同实例，主要用于车辆、行人等可数目标的识别与距离判断语义分割是指像素级的图像理解，即对图像中的每个像素标注所属的类别。国内外研究现状：图像分割是计算机视觉中的一种重要的图像预处理方法，是根据图像的灰度变化、空间纹理、几何形状等特征把图像划分成若干个互不相交的区域，使得这些特征在同一区域内具有一致性或相似性，而不同区域之间具有明显的不同点。传统的图像分割方法多数只利用了图像的低层特征，较为经典的算法有基于阈值的图像分割、基于边缘检测的图像分割以及基于图论的图像分割等。近年来，随着深度学习的发展，研究重点发展为图像的高层特征，先后提出了语义分割和实例分割的概念，并将先验知识引入到了图像分割算法中，推出了新的算法理念。语义分割对于无人驾驶感知系统的场景解析具有重要的作用，目前，常用于无人驾驶的语义分割网络主要有：ｖｉｊａｙ等人［３２］提出编码器－解码器网络结构，成为经典的图像分割网络结构之一，常用于可通行区域的分割；Ｌｉｎ等人［３３１提出一种多阶段的提炼网络，采用长距离残差连接，能够有效的将下采样中缺失的信息融合进来，提高了对于道路上的小目标的分割精度；Ｚｈａｏ等人％］提出一种金字塔场景解析模块，能够聚合不同区域的上下文信息，在目前的无人驾驶的图像分割算法中被广泛应用；Ｈａｎｇ等人［３５］提出了一种上下文编码网络，用于捕获场景的上下文语义并选择性的突出与类别相关的特征图，在复杂的道路场景中的分割结果具有明显的改善。上述方法在空间维度和特征通道两个方面均提出了很多新的提升图像分割精度的方法。实例分割是在语义分割的基础上，不仅可以预测像素所属类别，还能够区别同一类别的不同实例。用于无人驾驶技术的实例分割网络主要有：Ｂａｅｋ等人提出一种基于环视监控的无人驾驶场景解析网络，实现了在环视图像中进行车辆的实例分割，对于无人驾驶的感知系统具有重要的意义；Ｋｉｒｉｌｌｏｖ等人提出一种全景分割方法，通过将扩张卷积增强的语义分割网络与具有特征金字塔骨架的实例分割网络相结合的方法，用于无人驾驶感知系统，对摄像头范围内的场景进行全方位的解析；Neven等人提出一种端到端的车道线实例分割算法，分为分割和聚类两个部分，在应用中不受所需检测车道数的限制，同时还能应对车辆变道，为无人驾驶系统的车道线识别开辟了新的研究方向。基于深度卷积网的图像分割随着深度学习的发展，基于深度卷积网的图像分割方法在一定程度上取代了传统的方法成为目前主流的图像分割方法。将深度卷积网应用到图像分割领域最重要的突破是实现了网络模型的全卷积化，从而将图像分割任务实现端到端得训练。随后提出了编码器－解码器的网络结构，以及空洞卷积、特征聚合等多种优化图像分割的方法，对于图像分割技术的发展均具有重要的意义。—摘自《基于深度学习的无人驾驶图像分割算法研究》","path":"2020/10/31/调研报告/","date":"10-31","excerpt":"关于图像识别、CV、视频理解的调研报告","tags":[]},{"title":"人工智能导论笔记","text":"人工智能导论 第一章人工智能：藏于冰山下的万亿级巨大市场1、 AlphaGo VS 李世石2、人工智能（AI）：是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术应用系统的一门科学。研究范围包括：语言识别、图像识别、专家系统和我们最熟悉的机器人等。3、人工智能是对人的意识、思维的模拟。看起来好像拥有自己的思维。4、人工智能可以被分为5个等级1、无法与人类交互的人工智能 交互意为交流互动，我们无法窥探它们的思维2、可以与人类交互的人工智能 智能家电，它们无法自动升级，更不会主动去学习新的技能 3、可以通过外界设备进行升级的人工智能 手机，计算机，可以被动升级 4、可以通过云端共享信息的人工智能 搜索引擎 5、能够自主学习，创新的人工智能 人类可以看成是人工智能的圆满境界。总结为：能够与人类交互信息，可以通过各种信息载体甚至在云端中搜集信息，并且可以创新、创造、形成新的知识的人工智能。5、汽车与机器人2014年，谷歌研发出无人驾驶汽车原型6、互联网的搜索引擎也属于人工智能，它依托的就是一种名为“机器学习”的人工智能技术。机器学习以及超高速的运算能力使人工智能完成者一件又一件人工不可能完成的任务7、百度的先见之明 用于百度外卖。 第二章人工智能1.0时代：图灵的计算王国1、阿兰-图灵：计算机之父和人工智能之父2、1956年的达特茅斯会议，提出了人工智能的概念，这个概念包含了用机器人承载人类思维的设想。3、早期的机器人就是根据搜索树来制定行动规划的。4、这个研究将所有的“前提条件”“行动”“结果”这三个机器人运作的要素组合在了一起，它就是STRIPS 第三章人工智能2.0时代：知识 让计算机更聪明1、人机对话：Eliza2、本体研究：如何正确描述知识3、本体论本体的研究是一件非常困难的事情重量级本体论：研究重点应该是怎么描述知识，并根据这个方向进行试验。轻量级本体论：应该将数据输入计算机后，让计算机找出这些数据之间的关联。4、机器翻译：难在何处？1、如果要让机器翻译完全达到人类的水平，就必须克服自然语言中有关歧义的问题，这是自然语言中普遍存在的问题，歧义就是多义性。2、跨语种翻译很简单的一件事或一句话，如果没有常识作为基础，那么翻译、理解起来必然非常困难，这也是机器翻译的困难之一。5、1981年，日本宣布开始研制第五代计算机。6、最大的问题：如果人工智能的所有知识都需要人类输入，那么需要输入的内容将是无穷无尽的 第四章人工智能3.0时代：悄然兴起的机器学习1、机器学习：一种能够通过经验自动改进计算机算法的研究2、机器学习是一门研究机器获取新知识和新技能，并识别现有知识的学问3、机器学习分为有监督学习和无监督学习有监督学习：就是分类邻近算法就是理论比较成熟的有监督学习的应用如果一个样本在特定的空间内有K个最相似的样本，并且这些样本大多数属于某一类别，那么这个样本就属于这个类别，因为在判断之前要确认所选择的邻近样本都是正确的分类对象，这就是所谓的监督。无监督学习：更有探索价值，我们并没有放置任何可以参考的样本或者已经分类的参考目标，机器需要直接对已有数据建立模型。4、聚类分析是无监督学习的典型案例。目的是在相似的基础上收集数据进行分类。很多领域都会使用到聚类分析5、分类方法有很多种：1、决策树2、正则化法3、朴素贝叶斯算法4、人工神经网络：6、人工神经网络更像是真正意义上的人工智能的计算方式，因为它是通过模仿人类的脑神经回路进行分类的7、人工神经网络： 权重8、机器学习的难点：特征工程继自然语言处理之后，如何让计算机自己去选择合适的特征量成为了人工智能发展需要克服的又一道难题 第五章人工智能4.0时代：深度学习打破沉寂1、2006年，提出了深度学习的概念。它是机器学习这门学科的一个分支，属于无监督学习卷积神经网络。2、深度学习建立在模拟人脑进行分析学习的神经网络基础上，深度学习的重点在于深度，它比机器学习更看重纵向传输，也就是在某一个节点进行深度计算。3、人工神经网络无疑是机器学习十分高效的算法之一。4、自动编码器的功能就是为计算机学习的过程提供一个学习正解的机会。5、深度学习中，多层级架构是一个需要重点攻克的领域。6、情绪识别7、人工智能软实力基础：1、深度学习2、自动编码器3、鲁棒性8、鲁棒性决定了计算机能否在危险环境或异常情况下继续存活下去作为人工智能领域的前沿课题，鲁棒性研究还需要更好地向非线性系统扩展 第六章人工智能对人类社会的影响1、 语音助手：Siri2、 智能家居中国机器人产业链尚不完善，国内没有能够提供规模化且性能可靠地减速机等核心部件企业，本体核心技术被外资垄断失业、就业第七章信息奇点：让人类永生还是做人类的主人1、不能模仿人类思维的人工智能称为“弱人工智能”，反之为“强人工智能2、人工智能须造福大众。”","path":"2020/10/18/人工智能导论笔记/","date":"10-18","excerpt":"人工智能导论","tags":[]},{"title":"数据结构之线性表","text":"数据结构之线性表 3个项目：1、 电话簿2、 迷宫3、 自助交易系统 线性表：一对一的关系 7个内容 1、 定义：n个同类型数据元素的有限序列特点：ai的数据类型相同 位序i从1开始 前驱与后继 2、 抽象数据类型数据对象 数据关系 基本操作 基本操作：1、结构初始化操作2、结构销毁操作3、引用型操作4、加工型操作 具体问题具体分析 为什么要研究线性表、树、图这三种数据结构？如何对这些结构进行研究？ 合并线性表：合并两个图书馆的所有书籍思想：假设有两个线性表A和B，从B线性表依次读入每一本书的信息，然后查询A是否存在这本书，如果是，则数量增加；否则插入到A线性表中，直到B线性表的所有数据都处理完细化：逐一：从第一个到最后一个，计数型循环，前提是知道元素个数 List_Size 如何取出第i个数据元素bi? List_Retrieve(Lb,I,&amp;elem) 如何判断bi是否已在A中 List_Locate(La,elem,&amp;j) 返回success or false 如何不在A中，怎么样将bi插入呢 List_Insert(La,1,elem) 算法：Status List_Union(SqListPtr La,SqListptr Lb){ ElemType elem;//存放从Lb中取出的元素 Status status; int i,j,len=List_Size(Lb);//len存放Lb的元素个数 for(i=1;i&lt;=len;i++) { List_Retrive(Lb,i,&amp;elem);//取出Lb中第i个数据元素 status=List_Locate(La,elem,&amp;j);//判断是否在A中 if(status!=success) { status=List_Insert(La,1,elem);//插入 if(status!=success) break; //失败则退出 } else List_Add(La,j,1)//La的第j个数据加1 } return status;} 分析：最好情形分析：B为A前面的部分元素最坏情形分析：B∩A为空选数据元素少的作为Lb 这样n越小，性能越好 时间复杂度O（n2） 合并有序表时间复杂度O（n） 分析 ：依次扫描 插入 Status List_Merge(SqListPtr La,SqListptr Lb,SqListptr Lc){ ElemType elem1,elem2; status=List_Init(Lc); int i=1,j=1,k=1//i,j,k分别用于指示La，Lb，Lc中当前元素 int n=List_Size(La),m=List_Size(Lb); while(i&lt;=n&amp;&amp;j&lt;=m) { List_Retrive(La,i,&amp;elem1);//取出La中第i个数据元素 List_Retrive(Lb,i,&amp;elem2);//通过这个函数获取Lb中的第i个元素，存放到elem2当中 if(elem1&lt;elem2) { status=List_Insert(Lc,k,elem1);//如果elem1小,就把它插入到Lc当中 i=i+1; } else { status=List_Insert(Lc,k,elem2); j=j+1; } k=k+1;//无论插入那个，k都要➕1 } while(i&lt;=n)//表La没处理完 { List_Retrieve(La,i,&amp;elem1); status=List_Insert(Lc,k,elem1); i=i+1; k=k+1; } while(j&lt;=m)//表Lb没处理完 { List_Retrieve(Lb,i,&amp;elem2); status=List_Insert(Lc,k,elem2); j=j+1; k=k+1; } return status;} 这里插入一个问题单链表插入一个元素Status ListInsert L(LinkList &amp;L,int i,ElemType e){//在带头结点的单链线性表L中第i个位置之前插入元素ep=L; j=0;while(p&amp;&amp;j&lt;i-1)//寻找第I-1个节点{ p=p-&gt;next; ++j;}if（!p||j&gt;i-1）//i小于1或大于表长return ERROR;s=(LinkList)malloc(sizeof(LNode));//生成新节点s-&gt;data=e; //插入L中s-&gt;next=p-&gt;next;p-&gt;next=s;return OK;} 线性表的顺序存储结构 用一组地址连续的存储单元依次存放线性表中的数据元素 前驱后继关系非常容易找 可以随机存取数据：无论要找什么位置的数据，访问的速度都非常快。常数时间就可以访问顺序存储结构的实现：用结构来整合：空间如何分配？静态：数组 动态：指针typedef struct SqList{ ElemType *elem; int length; int list_size;}SqList,*ptr;typedef Ptr SqlListPtr; ElemType 这个数据类型 具体问题具体分析 整数int 字符型char 1、 基本操作1、初始化 创建线性表c语言中没有baistatus这个关键字。但一般写程序du时，会定义这样的一个类型，zhi用来表示成功或dao失败状态。如：0表示成功，-1表示失败这样status就可以定义成int类型，如：typedef int status ;然后用它去定义变量或函数返回值类型， Status List_Init(SqListPtr L){ Status s=success; L-&gt;list_size=LIST_INIT_SIZE; L-&gt;length=0; L-&gt;elem=(ElemType *)malloc(sizeof(ElemType)*L-&gt;list_size); if(L-&gt;elem==null) s=fatal; return s;}时间复杂度 O(1) 2、查找 按位置查找Status List_Retrival(SqListPtr L,int pos,ElemType *elem)//在线性表L当中 找pos这个元素 如果找到放到elem指针当中 Status 的返回值就是查找是不是成功的状态信息{ Status s=range_error;//初始时给一个查找失败 越界错误 if(L){//意为线性表存在的情况下 if((pos-1)&gt;=0&amp;&amp;(pos-1)length)//我们的pos 是1到n C语言第一个位置是0 { *elem=L-&gt;elem[pos-1];//把elem pos-1的值赋给指针 s=success; } } else s=fatal; return s;} 时间复杂度也是常数的时间复杂度 按值查找 Status List_Retrival(SqListPtr L,ElemType elem,int *pos)//找elem在线性表中的位置，放到pos当中{ Status s=range_error; if(L){ for(int i=0;ilength;++i) { if(L-&gt;elem[i]==elem){ *pos=i+1; s=success; break; } } } else s=fatal; return s;}时间复杂度O（n）线性表的链式存储结构单链表：用一组地址任意的存储单元存放线性表中的数据元素数据域和指针域 头指针与空指针空表时头结点的指针域为空","path":"2020/10/06/数据结构之线性表/","date":"10-06","excerpt":"数据结构之线性表","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构笔记（开头部分）","text":"数据结构笔记（开头部分） 程序=算法+数据结构 算法=逻辑+控制 迷宫：定义一个二维数组maze[m][n];maze[m][n]=0 表示不通 maze[m][n]=1表示通路 角点、边点与中点探测方法一致性处理 —迷宫地图简化： 在原来的迷宫四周围都扩展一个点，即增加两行和两列。值全部为0 表示不通 死路回退：回退到上一个具有多条路径的地方选择下一条路径探测 —栈 兜死圈子：1、设置一个标志数组mark[m][n], 1、 迷宫地图：如何表示给定的空间和可行的路径？如何表示入口和出口 ———数据结构选择与设计2、 寻路过程：当有多条可行的路径时如何选择？当某条路径在某一点再没有可行之路时如何处理某点重复经过吗（避免兜圈子）？ ————算法设计 结论：二维数组maze[m+2][n+2]来表示迷宫，解决迷宫地图的存储；一维数组DeltaXY[8]来记载8个探索方向的坐标增量 数据结构两大用途：1、 存放要处理的数据2、 实现算法策略 数据结构的设计能够决定算法效率的好和坏 数据之间是有联系的这些联系影响算法的选择和效率数据结构 研究数据之间的联系及选择与设计高效率的算法 线性结构 树形结构 图形结构 数据：所有能被计算机识别的符合集合数据元素：是数据（集合）中的一个“个体” 是数据结构中讨论的基本单位数据项：数据结构中讨论的最小单位 数据元素可以是数据项的集合数据对象：是具有相同性质的数据元素的集合，是数据的一个子集包含关系： 数据结构：带结构的数据元素的集合数据元素和其相互关系称为数据结构 数据结构由一个四元组来表示：Data_Structure=(D,L,S,O) 数据元素、数据元素之间的逻辑关系、逻辑关系在计算机中的存储表示、以及规定的操作者四部分 逻辑结构：线性 树形 图形 集合（极为松散的结构） 非线性结构 逻辑结构：算法分析与设计 存储结构：算法实现 两类：顺序存储结构：数组 特点：存取数据元素速度非常快，每个数据访问速度相同，也称为随机存取结构链式存储结构：指针类型或索引结构来实现 特点： 数据结构的操作：查找 插入 删除 遍历 排序 数据类型： 抽象数据类型：ADT一般包含数据元素、数据元素之间关系及操作三要素（D、R、O） 其中：D是数据元素集 R是D上的关系集合 O是对D的基本操作集 不需要考虑存储结构 特点：抽象性和扩展性 数据结构包括了抽象数据类型，抽象数据类型少了存储结构 意义 ： 要求： 算法：可以0输入 但至少有一个输出 确定性 有限性算法是解决问题的逻辑过程。 现实世界问题 —-人 —– 算法（通过逻辑思维）—–( 程序性实现)–计算机 方法学 计算机问题求解5步骤1、 问题的理解2、 数据结构设计3、 算法设计4、 算法分析5、 程序实现 好的算法：正确性 可读性 健壮性 高效性 算法性能比较方法：1、 编程后测试运行时间运行时间比较算法效率是很难做到准确的，只能做定性分析，要定量分析得靠估计出可能的运行时间2、 编程前分析可能的运行时间 算法复杂度分析：时间与空间复杂度和3个量有关：规模 输入 函数C=F (N, I, A)T=T (N, I) 一个算法用程序设计语言表示后，算法就是由一组语句构成，算法的执行效率就由各语句的执行的次数所决定 计算步：一个算法花费的时间与算法中语句的执行次数成正比例，哪个算法中语句执行次数多，它花费的时间就多，T（n）表示规模T（n）=O（f(n)）求 计算步 就是分析语句的执行次数 并不考虑输入对算法带来的影响 规定输入：最坏情况下的时间复杂性最好情况下的时间复杂性平均情况下的时间复杂性 渐进表达： 对于一个输入为n的问题，给出两个算法A和B A运行100n步 B运行nlogn步哪个好？ 若 A运行n²+100n步，B运行2n²步哪个好？ 很多时候我们认为n和n2之间差异很大，而n和5n之间的差异是微小的可以忽略不记的用渐进表达式可以解决这个问题，就算都是多项式，也需表现差异 复杂性渐进性态：当N单调增加趋于∞时有（T（N）-T（N1））/T(N) —&gt;0称T（N1）是T（N）的渐进性态直观上T（N1）是T（N）中略去低阶项所留下的主项 分析算法复杂性的目的在于比较求解同一问题的两个不同算法的效率可以采用渐进复杂性分析 算法复杂度分析：时间与空间复杂度和3个量有关：规模 输入 函数C=F (N, I, A)T=T (N, I) 一个算法用程序设计语言表示后，算法就是由一组语句构成，算法的执行效率就由各语句的执行的次数所决定 计算步：一个算法花费的时间与算法中语句的执行次数成正比例，哪个算法中语句执行次数多，它花费的时间就多，T（n）表示规模T（n）=O（f(n)）求 计算步 就是分析语句的执行次数 并不考虑输入对算法带来的影响 规定输入：最坏情况下的时间复杂性最好情况下的时间复杂性平均情况下的时间复杂性 渐进表达： 对于一个输入为n的问题，给出两个算法A和B A运行100n步 B运行nlogn步哪个好？ 若 A运行n²+100n步，B运行2n²步哪个好？ 很多时候我们认为n和n2之间差异很大，而n和5n之间的差异是微小的可以忽略不记的用渐进表达式可以解决这个问题，就算都是多项式，也需表现差异 复杂性渐进性态：当N单调增加趋于∞时有（T（N）-T（N1））/T(N) —&gt;0称T（N1）是T（N）的渐进性态直观上T（N1）是T（N）中省略低阶项所留下的主项 分析算法复杂性的目的在于比较求解同一问题的两个不同算法的效率可以采用渐进复杂性分析 渐进分析的符号：渐进上界O 下界Ω 2n&gt;n2&gt; n log n &gt; n&gt; log n 常用：1、多项式： 。。。。。nd 最高次2、对数：对数的底跟复杂度关系不大 O(loga n)= O(logb n)3、对数复杂度一定小于次方复杂度4、次方复杂度一定低于指数复杂度 K元素独立集问题 给出一个图，是否存在一个具有K个顶点的子图，该子图中任意两点间无边存在 穷举搜索产生运行时间的算法 习题问题：栈和队列术语都和数据的存储结构无关","path":"2020/10/05/数据结构笔记（开头部分）/","date":"10-05","excerpt":"数据结构笔记（开头部分）","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据挖掘笔记","text":"day 1:2020 10月4日 修改了一些博客yilia主题的内容 人物头像 链接 名称… 上传有点慢 一些些–","path":"2020/10/04/数据挖掘笔记/","date":"10-04","excerpt":"","tags":[]}],"categories":[],"tags":[{"name":"Gitalk","slug":"Gitalk","permalink":"http://example.com/tags/Gitalk/"},{"name":"图像分割","slug":"图像分割","permalink":"http://example.com/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"},{"name":"备案","slug":"备案","permalink":"http://example.com/tags/%E5%A4%87%E6%A1%88/"},{"name":"深度相机","slug":"深度相机","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E7%9B%B8%E6%9C%BA/"},{"name":"RGB-D","slug":"RGB-D","permalink":"http://example.com/tags/RGB-D/"},{"name":"三维重建","slug":"三维重建","permalink":"http://example.com/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"},{"name":"神经网络","slug":"神经网络","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"游戏制作","slug":"游戏制作","permalink":"http://example.com/tags/%E6%B8%B8%E6%88%8F%E5%88%B6%E4%BD%9C/"},{"name":"引擎","slug":"引擎","permalink":"http://example.com/tags/%E5%BC%95%E6%93%8E/"},{"name":"三维点云","slug":"三维点云","permalink":"http://example.com/tags/%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/tags/hexo/"},{"name":"神经网络模型","slug":"神经网络模型","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"},{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"},{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数据结构","slug":"数据结构","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]}